{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bayesian Filters","text":"<p>Kalman filtering and optimal estimation library</p> <p>This library provides Kalman filtering and various related optimal and non-optimal filtering software written in Python. It contains:</p> <ul> <li>Kalman filters: Standard, Extended, Unscented, Ensemble, Information, Square Root</li> <li>Smoothers: Kalman smoothers, Fixed Lag smoothers</li> <li>Other filters: H-Infinity, Fading memory, g-h filters, Least Squares</li> <li>Bayesian methods: Discrete Bayes, Monte Carlo</li> <li>Multi-model: IMM Estimator, MMAE Filter Bank</li> </ul>"},{"location":"#quick-install","title":"Quick Install","text":"<pre><code>pip install bayesian-filters\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import numpy as np\nfrom bayesian_filters.kalman import KalmanFilter\nfrom bayesian_filters.common import Q_discrete_white_noise\n\n# Create filter\nmy_filter = KalmanFilter(dim_x=2, dim_z=1)\n\n# Initialize matrices\nmy_filter.x = np.array([[2.], [0.]])  # initial state (position and velocity)\nmy_filter.F = np.array([[1., 1.], [0., 1.]])  # state transition matrix\nmy_filter.H = np.array([[1., 0.]])  # measurement function\nmy_filter.P *= 1000.  # covariance matrix\nmy_filter.R = 5  # measurement uncertainty\nmy_filter.Q = Q_discrete_white_noise(dim=2, dt=0.1, var=0.1)  # process uncertainty\n\n# Run filter\nwhile True:\n    my_filter.predict()\n    my_filter.update(get_measurement())\n\n    # Use the filtered output\n    x = my_filter.x\n    do_something_with(x)\n</code></pre>"},{"location":"#features","title":"Features","text":""},{"location":"#comprehensive-filter-collection","title":"Comprehensive Filter Collection","text":"<p>This library implements a wide range of filtering algorithms:</p> <ul> <li>Standard Kalman Filter: The classic discrete Kalman filter</li> <li>Extended Kalman Filter (EKF): For nonlinear systems using linearization</li> <li>Unscented Kalman Filter (UKF): For nonlinear systems using unscented transform</li> <li>Ensemble Kalman Filter (EnKF): Monte Carlo-based approach</li> <li>Square Root Filter: Numerically stable variant</li> <li>Information Filter: Inverse covariance form of Kalman filter</li> </ul>"},{"location":"#clear-pedagogical-code","title":"Clear, Pedagogical Code","text":"<p>The code is written to match equations from textbooks on a 1-to-1 basis, making it easy to learn and understand. This library was developed in conjunction with the book Kalman and Bayesian Filters in Python.</p>"},{"location":"#well-tested","title":"Well Tested","text":"<p>All filters include comprehensive test suites and are used in production systems.</p>"},{"location":"#about-this-fork","title":"About This Fork","text":"<p>This is a fork of the original FilterPy library by Roger Labbe. The main changes:</p> <ul> <li>Renamed from <code>filterpy</code> to <code>bayesian_filters</code> for PyPI publication</li> <li>Modern packaging with <code>uv</code> and <code>pyproject.toml</code></li> <li>Updated documentation with MkDocs Material theme</li> <li>Automated releases and GitHub Pages deployment</li> </ul> <p>Original project credit goes to Roger Labbe.</p>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"#documentation-sections","title":"Documentation Sections","text":"<ul> <li>Getting Started: Installation and quick start guide</li> <li>Filters: Detailed documentation on each filter type</li> <li>Algorithms: Other estimation algorithms</li> <li>API Reference: Complete API documentation</li> <li>Examples: Working examples and tutorials</li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>Book: Kalman and Bayesian Filters in Python</li> <li>GitHub: https://github.com/GeorgePearse/filterpy</li> <li>PyPI: https://pypi.org/project/bayesian-filters/</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>MkDocs-based documentation system</li> <li>Pre-commit hooks configuration</li> <li>GitHub Actions workflows for testing and documentation</li> <li>README in Markdown format</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Migrated from Sphinx to MkDocs for documentation</li> <li>Renamed package from <code>filterpy</code> to <code>bayesian_filters</code></li> <li>Updated to use <code>pyproject.toml</code> and hatchling build system</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>NumPy 2.0 compatibility issues</li> <li>Various linting and type checking issues</li> <li>Bug in <code>helpers.py</code> with undefined variable</li> </ul>"},{"location":"changelog/#legacy-changelog","title":"Legacy Changelog","text":"<p>For changes prior to the fork, see the original changelog.txt in the upstream repository.</p>"},{"location":"changelog/#original-filterpy-releases","title":"Original FilterPy Releases","text":"<p>The original FilterPy library by Roger R. Labbe Jr can be found at: https://github.com/rlabbe/filterpy</p> <p>This fork maintains compatibility while adding modernizations and improvements.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for considering contributing to Bayesian Filters!</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Fork the repository on GitHub</li> <li> <p>Clone your fork locally:    <pre><code>git clone https://github.com/YOUR_USERNAME/bayesian_filters.git\ncd bayesian_filters\n</code></pre></p> </li> <li> <p>Install development dependencies using uv:    <pre><code>uv sync --extra dev\n</code></pre></p> </li> <li> <p>Install pre-commit hooks:    <pre><code>uv run pre-commit install\n</code></pre></p> </li> </ol>"},{"location":"contributing/#making-changes","title":"Making Changes","text":"<ol> <li> <p>Create a new branch for your feature:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes and add tests</p> </li> <li> <p>Run the test suite:    <pre><code>uv run pytest\n</code></pre></p> </li> <li> <p>Run pre-commit checks:    <pre><code>uv run pre-commit run --all-files\n</code></pre></p> </li> <li> <p>Commit your changes:    <pre><code>git add .\ngit commit -m \"Description of your changes\"\n</code></pre></p> </li> </ol>"},{"location":"contributing/#submitting-a-pull-request","title":"Submitting a Pull Request","text":"<ol> <li> <p>Push your branch to GitHub:    <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Create a Pull Request on GitHub</p> </li> <li>Ensure all CI checks pass</li> <li>Wait for review</li> </ol>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 guidelines</li> <li>Use type hints where appropriate</li> <li>Write comprehensive docstrings (NumPy style)</li> <li>Keep code clear and readable over clever optimizations</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Add tests for new functionality</li> <li>Ensure all tests pass before submitting</li> <li>Aim for high test coverage</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update documentation for any changed functionality</li> <li>Add docstrings to new functions/classes</li> <li>Update relevant markdown files in <code>docs-mkdocs/</code></li> </ul>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>Feel free to open an issue on GitHub if you have questions about contributing!</p>"},{"location":"examples/","title":"Examples","text":"<p>This library includes several examples demonstrating various filtering techniques.</p>"},{"location":"examples/#example-code","title":"Example Code","text":"<p>Example code can be found in the <code>bayesian_filters/examples/</code> directory of the repository:</p> <ul> <li>RadarUKF.py - Unscented Kalman Filter tracking example</li> <li>bearing_only.py - Bearing-only tracking example</li> <li>GetRadar.py - Radar simulation utilities</li> </ul>"},{"location":"examples/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>For comprehensive examples and tutorials, see the companion book:</p> <p>Kalman and Bayesian Filters in Python</p> <p>This free book contains extensive examples using FilterPy and is written as Jupyter notebooks, making it easy to experiment with the code interactively.</p>"},{"location":"examples/#running-examples","title":"Running Examples","text":"<p>To run the examples:</p> <pre><code>git clone https://github.com/GeorgePearse/bayesian_filters\ncd bayesian_filters/bayesian_filters/examples\npython RadarUKF.py\n</code></pre>"},{"location":"examples/#contributing-examples","title":"Contributing Examples","text":"<p>If you have created interesting examples using this library, please consider contributing them! See the Contributing guide for more information.</p>"},{"location":"algorithms/discrete-bayes/","title":"Discrete Bayes","text":""},{"location":"algorithms/discrete-bayes/#discrete-bayes","title":"Discrete Bayes","text":"<p>The discrete Bayes filter provides functions for Bayesian filtering with discrete probability distributions.</p>"},{"location":"algorithms/discrete-bayes/#bayesian_filters.discrete_bayes.discrete_bayes.normalize","title":"<code>normalize(pdf)</code>","text":"<p>Normalize distribution <code>pdf</code> in-place so it sums to 1.0.</p> <p>Returns pdf for convienence, so you can write things like:</p> <p>kernel = normalize(randn(7))</p> <p>Parameters:</p> Name Type Description Default <code>pdf</code> <code>ndarray</code> <p>discrete distribution that needs to be converted to a pdf. Converted in-place, i.e., this is modified.</p> required <p>Returns:</p> Name Type Description <code>pdf</code> <code>ndarray</code> <p>The converted pdf.</p> Source code in <code>bayesian_filters/discrete_bayes/discrete_bayes.py</code> <pre><code>def normalize(pdf):\n    \"\"\"Normalize distribution `pdf` in-place so it sums to 1.0.\n\n    Returns pdf for convienence, so you can write things like:\n\n    &gt;&gt;&gt; kernel = normalize(randn(7))\n\n    Parameters\n    ----------\n\n    pdf : ndarray\n        discrete distribution that needs to be converted to a pdf. Converted\n        in-place, i.e., this is modified.\n\n    Returns\n    -------\n\n    pdf : ndarray\n        The converted pdf.\n    \"\"\"\n\n    pdf /= sum(np.asarray(pdf, dtype=float))\n    return pdf\n</code></pre>"},{"location":"algorithms/discrete-bayes/#bayesian_filters.discrete_bayes.discrete_bayes.update","title":"<code>update(likelihood, prior)</code>","text":"<p>Computes the posterior of a discrete random variable given a discrete likelihood and prior. In a typical application the likelihood will be the likelihood of a measurement matching your current environment, and the prior comes from discrete_bayes.predict().</p> <p>Parameters:</p> Name Type Description Default <code>likelihood</code> <code>ndarray, dtype=flaot</code> <p>array of likelihood values</p> required <code>prior</code> <code>ndarray, dtype=flaot</code> <p>prior pdf.</p> required <p>Returns:</p> Name Type Description <code>posterior</code> <code>ndarray, dtype=float</code> <p>Returns array representing the posterior.</p> <p>Examples:</p> <p>.. code-block:: Python</p> <pre><code># self driving car. Sensor returns values that can be equated to positions\n# on the road. A real likelihood compuation would be much more complicated\n# than this example.\n\nlikelihood = np.ones(len(road))\nlikelihood[road==z] *= scale_factor\n\nprior = predict(posterior, velocity, kernel)\nposterior = update(likelihood, prior)\n</code></pre> Source code in <code>bayesian_filters/discrete_bayes/discrete_bayes.py</code> <pre><code>def update(likelihood, prior):\n    \"\"\"Computes the posterior of a discrete random variable given a\n    discrete likelihood and prior. In a typical application the likelihood\n    will be the likelihood of a measurement matching your current environment,\n    and the prior comes from discrete_bayes.predict().\n\n    Parameters\n    ----------\n\n    likelihood : ndarray, dtype=flaot\n         array of likelihood values\n\n    prior : ndarray, dtype=flaot\n        prior pdf.\n\n    Returns\n    -------\n\n    posterior : ndarray, dtype=float\n        Returns array representing the posterior.\n\n\n    Examples\n    --------\n    .. code-block:: Python\n\n        # self driving car. Sensor returns values that can be equated to positions\n        # on the road. A real likelihood compuation would be much more complicated\n        # than this example.\n\n        likelihood = np.ones(len(road))\n        likelihood[road==z] *= scale_factor\n\n        prior = predict(posterior, velocity, kernel)\n        posterior = update(likelihood, prior)\n    \"\"\"\n\n    posterior = prior * likelihood\n    return normalize(posterior)\n</code></pre>"},{"location":"algorithms/discrete-bayes/#bayesian_filters.discrete_bayes.discrete_bayes.predict","title":"<code>predict(pdf, offset, kernel, mode='wrap', cval=0.0)</code>","text":"<p>Performs the discrete Bayes filter prediction step, generating the prior.</p> <p><code>pdf</code> is a discrete probability distribution expressing our initial belief.</p> <p><code>offset</code> is an integer specifying how much we want to move to the right (negative values means move to the left)</p> <p>We assume there is some noise in that offset, which we express in <code>kernel</code>. For example, if offset=3 and kernel=[.1, .7., .2], that means we think there is a 70% chance of moving right by 3, a 10% chance of moving 2 spaces, and a 20% chance of moving by 4.</p> <p>It returns the resulting distribution.</p> <p>If <code>mode='wrap'</code>, then the probability distribution is wrapped around the array.</p> <p>If <code>mode='constant'</code>, or any other value the pdf is shifted, with <code>cval</code> used to fill in missing elements.</p> <p>Examples:</p> <p>.. code-block:: Python</p> <pre><code>belief = [.05, .05, .05, .05, .55, .05, .05, .05, .05, .05]\nprior = predict(belief, offset=2, kernel=[.1, .8, .1])\n</code></pre> Source code in <code>bayesian_filters/discrete_bayes/discrete_bayes.py</code> <pre><code>def predict(pdf, offset, kernel, mode=\"wrap\", cval=0.0):\n    \"\"\"Performs the discrete Bayes filter prediction step, generating\n    the prior.\n\n    `pdf` is a discrete probability distribution expressing our initial\n    belief.\n\n    `offset` is an integer specifying how much we want to move to the right\n    (negative values means move to the left)\n\n    We assume there is some noise in that offset, which we express in `kernel`.\n    For example, if offset=3 and kernel=[.1, .7., .2], that means we think\n    there is a 70% chance of moving right by 3, a 10% chance of moving 2\n    spaces, and a 20% chance of moving by 4.\n\n    It returns the resulting distribution.\n\n    If `mode='wrap'`, then the probability distribution is wrapped around\n    the array.\n\n    If `mode='constant'`, or any other value the pdf is shifted, with `cval`\n    used to fill in missing elements.\n\n    Examples\n    --------\n    .. code-block:: Python\n\n        belief = [.05, .05, .05, .05, .55, .05, .05, .05, .05, .05]\n        prior = predict(belief, offset=2, kernel=[.1, .8, .1])\n    \"\"\"\n\n    if mode == \"wrap\":\n        return convolve(np.roll(pdf, offset), kernel, mode=\"wrap\")\n\n    return convolve(shift(pdf, offset, cval=cval), kernel, cval=cval, mode=\"constant\")\n</code></pre>"},{"location":"algorithms/gh-filter/","title":"GH Filter","text":""},{"location":"algorithms/gh-filter/#ghfilter","title":"GHFilter","text":"<p>Implements the g-h filter.</p>"},{"location":"algorithms/gh-filter/#bayesian_filters.gh.gh_filter.GHFilter","title":"<code>GHFilter</code>","text":"<p>               Bases: <code>object</code></p> <p>Implements the g-h filter. The topic is too large to cover in this comment. See my book \"Kalman and Bayesian Filters in Python\" [1] or Eli Brookner's \"Tracking and Kalman Filters Made Easy\" [2].</p> <p>A few basic examples are below, and the tests in ./gh_tests.py may give you more ideas on use.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>1D np.array or scalar</code> <p>Initial value for the filter state. Each value can be a scalar or a np.array.</p> <p>You can use a scalar for x0. If order &gt; 0, then 0.0 is assumed for the higher order terms.</p> <p>x[0] is the value being tracked x[1] is the first derivative (for order 1 and 2 filters) x[2] is the second derivative (for order 2 filters)</p> required <code>dx</code> <code>1D np.array or scalar</code> <p>Initial value for the derivative of the filter state.</p> required <code>dt</code> <code>scalar</code> <p>time step</p> required <code>g</code> <code>float</code> <p>filter g gain parameter.</p> required <code>h</code> <code>float</code> <p>filter h gain parameter.</p> required <p>Attributes:</p> Name Type Description <code>x</code> <code>1D np.array or scalar</code> <p>filter state</p> <code>dx</code> <code>1D np.array or scalar</code> <p>derivative of the filter state.</p> <code>x_prediction</code> <code>1D np.array or scalar</code> <p>predicted filter state</p> <code>dx_prediction</code> <code>1D np.array or scalar</code> <p>predicted derivative of the filter state.</p> <code>dt</code> <code>scalar</code> <p>time step</p> <code>g</code> <code>float</code> <p>filter g gain parameter.</p> <code>h</code> <code>float</code> <p>filter h gain parameter.</p> <code>y</code> <code>np.array, or scalar</code> <p>residual (difference between measurement and prior)</p> <code>z</code> <code>np.array, or scalar</code> <p>measurement passed into update()</p> <p>Examples:</p> <p>Create a basic filter for a scalar value with g=.8, h=.2. Initialize to 0, with a derivative(velocity) of 0.</p> <pre><code>&gt;&gt;&gt; from bayesian_filters.gh import GHFilter\n&gt;&gt;&gt; f = GHFilter (x=0., dx=0., dt=1., g=.8, h=.2)\n</code></pre> <p>Incorporate the measurement of 1</p> <pre><code>&gt;&gt;&gt; f.update(z=1)\n(0.8, 0.2)\n</code></pre> <p>Incorporate a measurement of 2 with g=1 and h=0.01</p> <pre><code>&gt;&gt;&gt; f.update(z=2, g=1, h=0.01)\n(2.0, 0.21000000000000002)\n</code></pre> <p>Create a filter with two independent variables.</p> <pre><code>&gt;&gt;&gt; from numpy import array\n&gt;&gt;&gt; f = GHFilter (x=array([0,1]), dx=array([0,0]), dt=1, g=.8, h=.02)\n</code></pre> <p>and update with the measurements (2,4)</p> <pre><code>&gt;&gt;&gt; f.update(array([2,4])\n(array([ 1.6,  3.4]), array([ 0.04,  0.06]))\n</code></pre> References <p>[1] Labbe, \"Kalman and Bayesian Filters in Python\" http://rlabbe.github.io/Kalman-and-Bayesian-Filters-in-Python</p> <p>[2] Brookner, \"Tracking and Kalman Filters Made Easy\". John Wiley and Sons, 1998.</p> Source code in <code>bayesian_filters/gh/gh_filter.py</code> <pre><code>class GHFilter(object):\n    \"\"\"\n    Implements the g-h filter. The topic is too large to cover in\n    this comment. See my book \"Kalman and Bayesian Filters in Python\" [1]\n    or Eli Brookner's \"Tracking and Kalman Filters Made Easy\" [2].\n\n    A few basic examples are below, and the tests in ./gh_tests.py may\n    give you more ideas on use.\n\n\n    Parameters\n    ----------\n\n    x : 1D np.array or scalar\n        Initial value for the filter state. Each value can be a scalar\n        or a np.array.\n\n        You can use a scalar for x0. If order &gt; 0, then 0.0 is assumed\n        for the higher order terms.\n\n        x[0] is the value being tracked\n        x[1] is the first derivative (for order 1 and 2 filters)\n        x[2] is the second derivative (for order 2 filters)\n\n    dx : 1D np.array or scalar\n        Initial value for the derivative of the filter state.\n\n    dt : scalar\n        time step\n\n    g : float\n        filter g gain parameter.\n\n    h : float\n        filter h gain parameter.\n\n\n    Attributes\n    ----------\n    x : 1D np.array or scalar\n        filter state\n\n    dx : 1D np.array or scalar\n        derivative of the filter state.\n\n    x_prediction : 1D np.array or scalar\n        predicted filter state\n\n    dx_prediction : 1D np.array or scalar\n        predicted derivative of the filter state.\n\n    dt : scalar\n        time step\n\n    g : float\n        filter g gain parameter.\n\n    h : float\n        filter h gain parameter.\n\n    y : np.array, or scalar\n        residual (difference between measurement and prior)\n\n    z : np.array, or scalar\n        measurement passed into update()\n\n    Examples\n    --------\n\n    Create a basic filter for a scalar value with g=.8, h=.2.\n    Initialize to 0, with a derivative(velocity) of 0.\n\n    &gt;&gt;&gt; from bayesian_filters.gh import GHFilter\n    &gt;&gt;&gt; f = GHFilter (x=0., dx=0., dt=1., g=.8, h=.2)\n\n    Incorporate the measurement of 1\n\n    &gt;&gt;&gt; f.update(z=1)\n    (0.8, 0.2)\n\n    Incorporate a measurement of 2 with g=1 and h=0.01\n\n    &gt;&gt;&gt; f.update(z=2, g=1, h=0.01)\n    (2.0, 0.21000000000000002)\n\n    Create a filter with two independent variables.\n\n    &gt;&gt;&gt; from numpy import array\n    &gt;&gt;&gt; f = GHFilter (x=array([0,1]), dx=array([0,0]), dt=1, g=.8, h=.02)\n\n    and update with the measurements (2,4)\n\n    &gt;&gt;&gt; f.update(array([2,4])\n    (array([ 1.6,  3.4]), array([ 0.04,  0.06]))\n\n\n    References\n    ----------\n\n    [1] Labbe, \"Kalman and Bayesian Filters in Python\"\n    http://rlabbe.github.io/Kalman-and-Bayesian-Filters-in-Python\n\n    [2] Brookner, \"Tracking and Kalman Filters Made Easy\". John Wiley and\n    Sons, 1998.\n\n    \"\"\"\n\n    def __init__(self, x, dx, dt, g, h):\n        self.x = x\n        self.dx = dx\n        self.dt = dt\n        self.g = g\n        self.h = h\n        self.dx_prediction = self.dx\n        self.x_prediction = self.x\n\n        if np.ndim(x) == 0:\n            self.y = 0.0  # residual\n            self.z = 0.0\n        else:\n            self.y = np.zeros(len(x))\n            self.z = np.zeros(len(x))\n\n    def update(self, z, g=None, h=None):\n        \"\"\"\n        performs the g-h filter predict and update step on the\n        measurement z. Modifies the member variables listed below,\n        and returns the state of x and dx as a tuple as a convienence.\n\n        **Modified Members**\n\n        x\n            filtered state variable\n\n        dx\n            derivative (velocity) of x\n\n        residual\n            difference between the measurement and the prediction for x\n\n        x_prediction\n            predicted value of x before incorporating the measurement z.\n\n        dx_prediction\n            predicted value of the derivative of x before incorporating the\n            measurement z.\n\n        Parameters\n        ----------\n\n        z : any\n            the measurement\n        g : scalar (optional)\n            Override the fixed self.g value for this update\n        h : scalar (optional)\n            Override the fixed self.h value for this update\n\n        Returns\n        -------\n\n        x filter output for x\n        dx filter output for dx (derivative of x\n        \"\"\"\n\n        if g is None:\n            g = self.g\n        if h is None:\n            h = self.h\n\n        # prediction step\n        self.dx_prediction = self.dx\n        self.x_prediction = self.x + (self.dx * self.dt)\n\n        # update step\n        self.y = z - self.x_prediction\n        self.dx = self.dx_prediction + h * self.y / self.dt\n        self.x = self.x_prediction + g * self.y\n\n        return (self.x, self.dx)\n\n    def batch_filter(self, data, save_predictions=False, saver=None):\n        \"\"\"\n        Given a sequenced list of data, performs g-h filter\n        with a fixed g and h. See update() if you need to vary g and/or h.\n\n        Uses self.x and self.dx to initialize the filter, but DOES NOT\n        alter self.x and self.dx during execution, allowing you to use this\n        class multiple times without reseting self.x and self.dx. I'm not sure\n        how often you would need to do that, but the capability is there.\n        More exactly, none of the class member variables are modified\n        by this function, in distinct contrast to update(), which changes\n        most of them.\n\n        Parameters\n        ----------\n\n        data : list like\n            contains the data to be filtered.\n\n        save_predictions : boolean\n            the predictions will be saved and returned if this is true\n\n        saver : filterpy.common.Saver, optional\n            filterpy.common.Saver object. If provided, saver.save() will be\n            called after every epoch\n\n\n        Returns\n        -------\n\n        results : np.array shape (n+1, 2), where n=len(data)\n            contains the results of the filter, where\n            results[i,0] is x , and\n            results[i,1] is dx (derivative of x)\n            First entry is the initial values of x and dx as set by __init__.\n\n        predictions : np.array shape(n), optional\n            the predictions for each step in the filter. Only retured if\n            save_predictions == True\n        \"\"\"\n\n        x = self.x\n        dx = self.dx\n        n = len(data)\n\n        results = np.zeros((n + 1, 2))\n        results[0, 0] = x\n        results[0, 1] = dx\n\n        if save_predictions:\n            predictions = np.zeros(n)\n\n        # optimization to avoid n computations of h / dt\n        h_dt = self.h / self.dt\n\n        for i, z in enumerate(data):\n            # prediction step\n            x_est = x + (dx * self.dt)\n\n            # update step\n            residual = z - x_est\n            dx = dx + h_dt * residual  # i.e. dx = dx + h * residual / dt\n            x = x_est + self.g * residual\n\n            results[i + 1, 0] = x\n            results[i + 1, 1] = dx\n            if save_predictions:\n                predictions[i] = x_est\n\n            if saver is not None:\n                saver.save()\n\n        if save_predictions:\n            return results, predictions\n\n        return results\n\n    def VRF_prediction(self):\n        r\"\"\"\n        Returns the Variance Reduction Factor of the prediction\n        step of the filter. The VRF is the\n        normalized variance for the filter, as given in the equation below.\n\n        .. math::\n            VRF(\\hat{x}_{n+1,n}) = \\\\frac{VAR(\\hat{x}_{n+1,n})}{\\sigma^2_x}\n\n        References\n        ----------\n\n        Asquith, \"Weight Selection in First Order Linear Filters\"\n        Report No RG-TR-69-12, U.S. Army Missle Command. Redstone Arsenal, Al.\n        November 24, 1970.\n        \"\"\"\n\n        g = self.g\n        h = self.h\n\n        return (2 * g**2 + 2 * h + g * h) / (g * (4 - 2 * g - h))\n\n    def VRF(self):\n        r\"\"\"\n        Returns the Variance Reduction Factor (VRF) of the state variable\n        of the filter (x) and its derivatives (dx, ddx). The VRF is the\n        normalized variance for the filter, as given in the equations below.\n\n        .. math::\n            VRF(\\hat{x}_{n,n}) = \\\\frac{VAR(\\hat{x}_{n,n})}{\\sigma^2_x}\n\n            VRF(\\hat{\\dot{x}}_{n,n}) = \\\\frac{VAR(\\hat{\\dot{x}}_{n,n})}{\\sigma^2_x}\n\n            VRF(\\hat{\\ddot{x}}_{n,n}) = \\\\frac{VAR(\\hat{\\ddot{x}}_{n,n})}{\\sigma^2_x}\n\n        Returns\n        -------\n\n        vrf_x   VRF of x state variable\n        vrf_dx  VRF of the dx state variable (derivative of x)\n        \"\"\"\n\n        g = self.g\n        h = self.h\n\n        den = g * (4 - 2 * g - h)\n\n        vx = (2 * g**2 + 2 * h - 3 * g * h) / den\n        vdx = 2 * h**2 / (self.dt**2 * den)\n\n        return (vx, vdx)\n\n    def __repr__(self):\n        return \"\\n\".join(\n            [\n                \"GHFilter object\",\n                pretty_str(\"dt\", self.dt),\n                pretty_str(\"g\", self.g),\n                pretty_str(\"h\", self.h),\n                pretty_str(\"x\", self.x),\n                pretty_str(\"dx\", self.dx),\n                pretty_str(\"x_prediction\", self.x_prediction),\n                pretty_str(\"dx_prediction\", self.dx_prediction),\n                pretty_str(\"y\", self.y),\n                pretty_str(\"z\", self.z),\n            ]\n        )\n</code></pre>"},{"location":"algorithms/gh-filter/#bayesian_filters.gh.gh_filter.GHFilter.VRF","title":"<code>VRF()</code>","text":"<p>Returns the Variance Reduction Factor (VRF) of the state variable of the filter (x) and its derivatives (dx, ddx). The VRF is the normalized variance for the filter, as given in the equations below.</p> <p>.. math::     VRF(\\hat{x}{n,n}) = \\frac{VAR(\\hat{x}})}{\\sigma^2_x</p> <pre><code>VRF(\\hat{\\dot{x}}_{n,n}) = \\\\frac{VAR(\\hat{\\dot{x}}_{n,n})}{\\sigma^2_x}\n\nVRF(\\hat{\\ddot{x}}_{n,n}) = \\\\frac{VAR(\\hat{\\ddot{x}}_{n,n})}{\\sigma^2_x}\n</code></pre> <p>Returns:</p> Type Description <code>vrf_x   VRF of x state variable</code> <code>vrf_dx  VRF of the dx state variable (derivative of x)</code> Source code in <code>bayesian_filters/gh/gh_filter.py</code> <pre><code>def VRF(self):\n    r\"\"\"\n    Returns the Variance Reduction Factor (VRF) of the state variable\n    of the filter (x) and its derivatives (dx, ddx). The VRF is the\n    normalized variance for the filter, as given in the equations below.\n\n    .. math::\n        VRF(\\hat{x}_{n,n}) = \\\\frac{VAR(\\hat{x}_{n,n})}{\\sigma^2_x}\n\n        VRF(\\hat{\\dot{x}}_{n,n}) = \\\\frac{VAR(\\hat{\\dot{x}}_{n,n})}{\\sigma^2_x}\n\n        VRF(\\hat{\\ddot{x}}_{n,n}) = \\\\frac{VAR(\\hat{\\ddot{x}}_{n,n})}{\\sigma^2_x}\n\n    Returns\n    -------\n\n    vrf_x   VRF of x state variable\n    vrf_dx  VRF of the dx state variable (derivative of x)\n    \"\"\"\n\n    g = self.g\n    h = self.h\n\n    den = g * (4 - 2 * g - h)\n\n    vx = (2 * g**2 + 2 * h - 3 * g * h) / den\n    vdx = 2 * h**2 / (self.dt**2 * den)\n\n    return (vx, vdx)\n</code></pre>"},{"location":"algorithms/gh-filter/#bayesian_filters.gh.gh_filter.GHFilter.VRF_prediction","title":"<code>VRF_prediction()</code>","text":"<p>Returns the Variance Reduction Factor of the prediction step of the filter. The VRF is the normalized variance for the filter, as given in the equation below.</p> <p>.. math::     VRF(\\hat{x}{n+1,n}) = \\frac{VAR(\\hat{x}})}{\\sigma^2_x</p> References <p>Asquith, \"Weight Selection in First Order Linear Filters\" Report No RG-TR-69-12, U.S. Army Missle Command. Redstone Arsenal, Al. November 24, 1970.</p> Source code in <code>bayesian_filters/gh/gh_filter.py</code> <pre><code>def VRF_prediction(self):\n    r\"\"\"\n    Returns the Variance Reduction Factor of the prediction\n    step of the filter. The VRF is the\n    normalized variance for the filter, as given in the equation below.\n\n    .. math::\n        VRF(\\hat{x}_{n+1,n}) = \\\\frac{VAR(\\hat{x}_{n+1,n})}{\\sigma^2_x}\n\n    References\n    ----------\n\n    Asquith, \"Weight Selection in First Order Linear Filters\"\n    Report No RG-TR-69-12, U.S. Army Missle Command. Redstone Arsenal, Al.\n    November 24, 1970.\n    \"\"\"\n\n    g = self.g\n    h = self.h\n\n    return (2 * g**2 + 2 * h + g * h) / (g * (4 - 2 * g - h))\n</code></pre>"},{"location":"algorithms/gh-filter/#bayesian_filters.gh.gh_filter.GHFilter.batch_filter","title":"<code>batch_filter(data, save_predictions=False, saver=None)</code>","text":"<p>Given a sequenced list of data, performs g-h filter with a fixed g and h. See update() if you need to vary g and/or h.</p> <p>Uses self.x and self.dx to initialize the filter, but DOES NOT alter self.x and self.dx during execution, allowing you to use this class multiple times without reseting self.x and self.dx. I'm not sure how often you would need to do that, but the capability is there. More exactly, none of the class member variables are modified by this function, in distinct contrast to update(), which changes most of them.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list like</code> <p>contains the data to be filtered.</p> required <code>save_predictions</code> <code>boolean</code> <p>the predictions will be saved and returned if this is true</p> <code>False</code> <code>saver</code> <code>Saver</code> <p>filterpy.common.Saver object. If provided, saver.save() will be called after every epoch</p> <code>None</code> <p>Returns:</p> Name Type Description <code>results</code> <code>np.array shape (n+1, 2), where n=len(data)</code> <p>contains the results of the filter, where results[i,0] is x , and results[i,1] is dx (derivative of x) First entry is the initial values of x and dx as set by init.</p> <code>predictions</code> <code>np.array shape(n), optional</code> <p>the predictions for each step in the filter. Only retured if save_predictions == True</p> Source code in <code>bayesian_filters/gh/gh_filter.py</code> <pre><code>def batch_filter(self, data, save_predictions=False, saver=None):\n    \"\"\"\n    Given a sequenced list of data, performs g-h filter\n    with a fixed g and h. See update() if you need to vary g and/or h.\n\n    Uses self.x and self.dx to initialize the filter, but DOES NOT\n    alter self.x and self.dx during execution, allowing you to use this\n    class multiple times without reseting self.x and self.dx. I'm not sure\n    how often you would need to do that, but the capability is there.\n    More exactly, none of the class member variables are modified\n    by this function, in distinct contrast to update(), which changes\n    most of them.\n\n    Parameters\n    ----------\n\n    data : list like\n        contains the data to be filtered.\n\n    save_predictions : boolean\n        the predictions will be saved and returned if this is true\n\n    saver : filterpy.common.Saver, optional\n        filterpy.common.Saver object. If provided, saver.save() will be\n        called after every epoch\n\n\n    Returns\n    -------\n\n    results : np.array shape (n+1, 2), where n=len(data)\n        contains the results of the filter, where\n        results[i,0] is x , and\n        results[i,1] is dx (derivative of x)\n        First entry is the initial values of x and dx as set by __init__.\n\n    predictions : np.array shape(n), optional\n        the predictions for each step in the filter. Only retured if\n        save_predictions == True\n    \"\"\"\n\n    x = self.x\n    dx = self.dx\n    n = len(data)\n\n    results = np.zeros((n + 1, 2))\n    results[0, 0] = x\n    results[0, 1] = dx\n\n    if save_predictions:\n        predictions = np.zeros(n)\n\n    # optimization to avoid n computations of h / dt\n    h_dt = self.h / self.dt\n\n    for i, z in enumerate(data):\n        # prediction step\n        x_est = x + (dx * self.dt)\n\n        # update step\n        residual = z - x_est\n        dx = dx + h_dt * residual  # i.e. dx = dx + h * residual / dt\n        x = x_est + self.g * residual\n\n        results[i + 1, 0] = x\n        results[i + 1, 1] = dx\n        if save_predictions:\n            predictions[i] = x_est\n\n        if saver is not None:\n            saver.save()\n\n    if save_predictions:\n        return results, predictions\n\n    return results\n</code></pre>"},{"location":"algorithms/gh-filter/#bayesian_filters.gh.gh_filter.GHFilter.update","title":"<code>update(z, g=None, h=None)</code>","text":"<p>performs the g-h filter predict and update step on the measurement z. Modifies the member variables listed below, and returns the state of x and dx as a tuple as a convienence.</p> <p>Modified Members</p> <p>x     filtered state variable</p> <p>dx     derivative (velocity) of x</p> <p>residual     difference between the measurement and the prediction for x</p> <p>x_prediction     predicted value of x before incorporating the measurement z.</p> <p>dx_prediction     predicted value of the derivative of x before incorporating the     measurement z.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>any</code> <p>the measurement</p> required <code>g</code> <code>scalar(optional)</code> <p>Override the fixed self.g value for this update</p> <code>None</code> <code>h</code> <code>scalar(optional)</code> <p>Override the fixed self.h value for this update</p> <code>None</code> <p>Returns:</p> Type Description <code>x filter output for x</code> <code>dx filter output for dx (derivative of x</code> Source code in <code>bayesian_filters/gh/gh_filter.py</code> <pre><code>def update(self, z, g=None, h=None):\n    \"\"\"\n    performs the g-h filter predict and update step on the\n    measurement z. Modifies the member variables listed below,\n    and returns the state of x and dx as a tuple as a convienence.\n\n    **Modified Members**\n\n    x\n        filtered state variable\n\n    dx\n        derivative (velocity) of x\n\n    residual\n        difference between the measurement and the prediction for x\n\n    x_prediction\n        predicted value of x before incorporating the measurement z.\n\n    dx_prediction\n        predicted value of the derivative of x before incorporating the\n        measurement z.\n\n    Parameters\n    ----------\n\n    z : any\n        the measurement\n    g : scalar (optional)\n        Override the fixed self.g value for this update\n    h : scalar (optional)\n        Override the fixed self.h value for this update\n\n    Returns\n    -------\n\n    x filter output for x\n    dx filter output for dx (derivative of x\n    \"\"\"\n\n    if g is None:\n        g = self.g\n    if h is None:\n        h = self.h\n\n    # prediction step\n    self.dx_prediction = self.dx\n    self.x_prediction = self.x + (self.dx * self.dt)\n\n    # update step\n    self.y = z - self.x_prediction\n    self.dx = self.dx_prediction + h * self.y / self.dt\n    self.x = self.x_prediction + g * self.y\n\n    return (self.x, self.dx)\n</code></pre>"},{"location":"algorithms/ghk-filter/","title":"Ghk filter","text":""},{"location":"algorithms/ghk-filter/#ghkfilter","title":"GHKFilter","text":"<p>Implements the g-h-k filter for tracking acceleration.</p>"},{"location":"algorithms/ghk-filter/#bayesian_filters.gh.gh_filter.GHKFilter","title":"<code>GHKFilter</code>","text":"<p>               Bases: <code>object</code></p> <p>Implements the g-h-k filter.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>1D np.array or scalar</code> <p>Initial value for the filter state. Each value can be a scalar or a np.array.</p> <p>You can use a scalar for x0. If order &gt; 0, then 0.0 is assumed for the higher order terms.</p> <p>x[0] is the value being tracked x[1] is the first derivative (for order 1 and 2 filters) x[2] is the second derivative (for order 2 filters)</p> required <code>dx</code> <code>1D np.array or scalar</code> <p>Initial value for the derivative of the filter state.</p> required <code>ddx</code> <code>1D np.array or scalar</code> <p>Initial value for the second derivative of the filter state.</p> required <code>dt</code> <code>scalar</code> <p>time step</p> required <code>g</code> <code>float</code> <p>filter g gain parameter.</p> required <code>h</code> <code>float</code> <p>filter h gain parameter.</p> required <code>k</code> <code>float</code> <p>filter k gain parameter.</p> required <p>Attributes:</p> Name Type Description <code>x</code> <code>1D np.array or scalar</code> <p>filter state</p> <code>dx</code> <code>1D np.array or scalar</code> <p>derivative of the filter state.</p> <code>ddx</code> <code>1D np.array or scalar</code> <p>second derivative of the filter state.</p> <code>x_prediction</code> <code>1D np.array or scalar</code> <p>predicted filter state</p> <code>dx_prediction</code> <code>1D np.array or scalar</code> <p>predicted derivative of the filter state.</p> <code>ddx_prediction</code> <code>1D np.array or scalar</code> <p>second predicted derivative of the filter state.</p> <code>dt</code> <code>scalar</code> <p>time step</p> <code>g</code> <code>float</code> <p>filter g gain parameter.</p> <code>h</code> <code>float</code> <p>filter h gain parameter.</p> <code>k</code> <code>float</code> <p>filter k gain parameter.</p> <code>y</code> <code>np.array, or scalar</code> <p>residual (difference between measurement and prior)</p> <code>z</code> <code>np.array, or scalar</code> <p>measurement passed into update()</p> References <p>Brookner, \"Tracking and Kalman Filters Made Easy\". John Wiley and Sons, 1998.</p> Source code in <code>bayesian_filters/gh/gh_filter.py</code> <pre><code>class GHKFilter(object):\n    \"\"\"\n    Implements the g-h-k filter.\n\n    Parameters\n    ----------\n\n    x : 1D np.array or scalar\n        Initial value for the filter state. Each value can be a scalar\n        or a np.array.\n\n        You can use a scalar for x0. If order &gt; 0, then 0.0 is assumed\n        for the higher order terms.\n\n        x[0] is the value being tracked\n        x[1] is the first derivative (for order 1 and 2 filters)\n        x[2] is the second derivative (for order 2 filters)\n\n    dx : 1D np.array or scalar\n        Initial value for the derivative of the filter state.\n\n    ddx : 1D np.array or scalar\n        Initial value for the second derivative of the filter state.\n\n    dt : scalar\n        time step\n\n    g : float\n        filter g gain parameter.\n\n    h : float\n        filter h gain parameter.\n\n    k : float\n        filter k gain parameter.\n\n\n\n    Attributes\n    ----------\n    x : 1D np.array or scalar\n        filter state\n\n    dx : 1D np.array or scalar\n        derivative of the filter state.\n\n    ddx : 1D np.array or scalar\n        second derivative of the filter state.\n\n    x_prediction : 1D np.array or scalar\n        predicted filter state\n\n    dx_prediction : 1D np.array or scalar\n        predicted derivative of the filter state.\n\n    ddx_prediction : 1D np.array or scalar\n        second predicted derivative of the filter state.\n\n    dt : scalar\n        time step\n\n    g : float\n        filter g gain parameter.\n\n    h : float\n        filter h gain parameter.\n\n    k : float\n        filter k gain parameter.\n\n    y : np.array, or scalar\n        residual (difference between measurement and prior)\n\n    z : np.array, or scalar\n        measurement passed into update()\n\n    References\n    ----------\n\n    Brookner, \"Tracking and Kalman Filters Made Easy\". John Wiley and\n    Sons, 1998.\n    \"\"\"\n\n    def __init__(self, x, dx, ddx, dt, g, h, k):\n        self.x = x\n        self.dx = dx\n        self.ddx = ddx\n        self.x_prediction = self.x\n        self.dx_prediction = self.dx\n        self.ddx_prediction = self.ddx\n\n        self.dt = dt\n        self.g = g\n        self.h = h\n        self.k = k\n\n        if np.ndim(x) == 0:\n            self.y = 0.0  # residual\n            self.z = 0.0\n        else:\n            self.y = np.zeros(len(x))\n            self.z = np.zeros(len(x))\n\n    def update(self, z, g=None, h=None, k=None):\n        \"\"\"\n        Performs the g-h filter predict and update step on the\n        measurement z.\n\n        On return, self.x, self.dx, self.y, and self.x_prediction\n        will have been updated with the results of the computation. For\n        convienence, self.x and self.dx are returned in a tuple.\n\n        Parameters\n        ----------\n\n        z : scalar\n            the measurement\n        g : scalar (optional)\n            Override the fixed self.g value for this update\n        h : scalar (optional)\n            Override the fixed self.h value for this update\n        k : scalar (optional)\n            Override the fixed self.k value for this update\n\n        Returns\n        -------\n\n        x filter output for x\n        dx filter output for dx (derivative of x\n\n        \"\"\"\n\n        if g is None:\n            g = self.g\n        if h is None:\n            h = self.h\n        if k is None:\n            k = self.k\n\n        dt = self.dt\n        dt_sqr = dt**2\n        # prediction step\n        self.ddx_prediction = self.ddx\n        self.dx_prediction = self.dx + self.ddx * dt\n        self.x_prediction = self.x + self.dx * dt + 0.5 * self.ddx * (dt_sqr)\n\n        # update step\n        self.y = z - self.x_prediction\n\n        self.ddx = self.ddx_prediction + 2 * k * self.y / dt_sqr\n        self.dx = self.dx_prediction + h * self.y / dt\n        self.x = self.x_prediction + g * self.y\n\n        return (self.x, self.dx)\n\n    def batch_filter(self, data, save_predictions=False):\n        \"\"\"\n        Performs g-h filter with a fixed g and h.\n\n        Uses self.x and self.dx to initialize the filter, but DOES NOT\n        alter self.x and self.dx during execution, allowing you to use this\n        class multiple times without reseting self.x and self.dx. I'm not sure\n        how often you would need to do that, but the capability is there.\n        More exactly, none of the class member variables are modified\n        by this function.\n\n        Parameters\n        ----------\n\n        data : list_like\n            contains the data to be filtered.\n\n        save_predictions : boolean\n            The predictions will be saved and returned if this is true\n\n        Returns\n        -------\n\n        results : np.array shape (n+1, 2), where n=len(data)\n            contains the results of the filter, where\n            results[i,0] is x , and\n            results[i,1] is dx (derivative of x)\n            First entry is the initial values of x and dx as set by __init__.\n\n        predictions : np.array shape(n), or None\n            the predictions for each step in the filter. Only returned if\n            save_predictions == True\n        \"\"\"\n\n        x = self.x\n        dx = self.dx\n        n = len(data)\n\n        results = np.zeros((n + 1, 2))\n        results[0, 0] = x\n        results[0, 1] = dx\n\n        if save_predictions:\n            predictions = np.zeros(n)\n\n        # optimization to avoid n computations of h / dt\n        h_dt = self.h / self.dt\n\n        for i, z in enumerate(data):\n            # prediction step\n            x_est = x + (dx * self.dt)\n\n            # update step\n            residual = z - x_est\n            dx = dx + h_dt * residual  # i.e. dx = dx + h * residual / dt\n            x = x_est + self.g * residual\n\n            results[i + 1, 0] = x\n            results[i + 1, 1] = dx\n            if save_predictions:\n                predictions[i] = x_est\n\n        if save_predictions:\n            return results, predictions\n\n        return results\n\n    def VRF_prediction(self):\n        r\"\"\"\n        Returns the Variance Reduction Factor for x of the prediction\n        step of the filter.\n\n        This implements the equation\n\n        .. math::\n            VRF(\\hat{x}_{n+1,n}) = \\\\frac{VAR(\\hat{x}_{n+1,n})}{\\sigma^2_x}\n\n        References\n        ----------\n\n        Asquith and Woods, \"Total Error Minimization in First\n        and Second Order Prediction Filters\" Report No RE-TR-70-17, U.S.\n        Army Missle Command. Redstone Arsenal, Al. November 24, 1970.\n        \"\"\"\n\n        g = self.g\n        h = self.h\n        k = self.k\n        gh2 = 2 * g + h\n        return (g * k * (gh2 - 4) + h * (g * gh2 + 2 * h)) / (2 * k - (g * (h + k) * (gh2 - 4)))\n\n    def bias_error(self, dddx):\n        \"\"\"\n        Returns the bias error given the specified constant jerk(dddx)\n\n        Parameters\n        ----------\n\n        dddx : type(self.x)\n            3rd derivative (jerk) of the state variable x.\n\n        References\n        ----------\n\n        Asquith and Woods, \"Total Error Minimization in First\n        and Second Order Prediction Filters\" Report No RE-TR-70-17, U.S.\n        Army Missle Command. Redstone Arsenal, Al. November 24, 1970.\n        \"\"\"\n        return -(self.dt**3) * dddx / (2 * self.k)\n\n    def VRF(self):\n        r\"\"\"\n        Returns the Variance Reduction Factor (VRF) of the state variable\n        of the filter (x) and its derivatives (dx, ddx). The VRF is the\n        normalized variance for the filter, as given in the equations below.\n\n        .. math::\n            VRF(\\hat{x}_{n,n}) = \\\\frac{VAR(\\hat{x}_{n,n})}{\\sigma^2_x}\n\n            VRF(\\hat{\\dot{x}}_{n,n}) = \\\\frac{VAR(\\hat{\\dot{x}}_{n,n})}{\\sigma^2_x}\n\n            VRF(\\hat{\\ddot{x}}_{n,n}) = \\\\frac{VAR(\\hat{\\ddot{x}}_{n,n})}{\\sigma^2_x}\n\n        Returns\n        -------\n\n        vrf_x : type(x)\n            VRF of x state variable\n\n        vrf_dx : type(x)\n            VRF of the dx state variable (derivative of x)\n\n        vrf_ddx : type(x)\n            VRF of the ddx state variable (second derivative of x)\n        \"\"\"\n\n        g = self.g\n        h = self.h\n        k = self.k\n\n        # common subexpressions in the equations pulled out for efficiency,\n        # they don't 'mean' anything.\n        hg4 = 4 - 2 * g - h\n        ghk = g * h + g * k - 2 * k\n\n        vx = (2 * h * (2 * (g**2) + 2 * h - 3 * g * h) - 2 * g * k * hg4) / (2 * k - g * (h + k) * hg4)\n        vdx = (2 * (h**3) - 4 * (h**2) * k + 4 * (k**2) * (2 - g)) / (2 * hg4 * ghk)\n        vddx = 8 * h * (k**2) / ((self.dt**4) * hg4 * ghk)\n\n        return (vx, vdx, vddx)\n\n    def __repr__(self):\n        return \"\\n\".join(\n            [\n                \"GHFilter object\",\n                pretty_str(\"dt\", self.dt),\n                pretty_str(\"g\", self.g),\n                pretty_str(\"h\", self.h),\n                pretty_str(\"k\", self.k),\n                pretty_str(\"x\", self.x),\n                pretty_str(\"dx\", self.dx),\n                pretty_str(\"ddx\", self.ddx),\n                pretty_str(\"x_prediction\", self.x_prediction),\n                pretty_str(\"dx_prediction\", self.dx_prediction),\n                pretty_str(\"ddx_prediction\", self.dx_prediction),\n                pretty_str(\"y\", self.y),\n                pretty_str(\"z\", self.z),\n            ]\n        )\n</code></pre>"},{"location":"algorithms/ghk-filter/#bayesian_filters.gh.gh_filter.GHKFilter.VRF","title":"<code>VRF()</code>","text":"<p>Returns the Variance Reduction Factor (VRF) of the state variable of the filter (x) and its derivatives (dx, ddx). The VRF is the normalized variance for the filter, as given in the equations below.</p> <p>.. math::     VRF(\\hat{x}{n,n}) = \\frac{VAR(\\hat{x}})}{\\sigma^2_x</p> <pre><code>VRF(\\hat{\\dot{x}}_{n,n}) = \\\\frac{VAR(\\hat{\\dot{x}}_{n,n})}{\\sigma^2_x}\n\nVRF(\\hat{\\ddot{x}}_{n,n}) = \\\\frac{VAR(\\hat{\\ddot{x}}_{n,n})}{\\sigma^2_x}\n</code></pre> <p>Returns:</p> Name Type Description <code>vrf_x</code> <code>type(x)</code> <p>VRF of x state variable</p> <code>vrf_dx</code> <code>type(x)</code> <p>VRF of the dx state variable (derivative of x)</p> <code>vrf_ddx</code> <code>type(x)</code> <p>VRF of the ddx state variable (second derivative of x)</p> Source code in <code>bayesian_filters/gh/gh_filter.py</code> <pre><code>def VRF(self):\n    r\"\"\"\n    Returns the Variance Reduction Factor (VRF) of the state variable\n    of the filter (x) and its derivatives (dx, ddx). The VRF is the\n    normalized variance for the filter, as given in the equations below.\n\n    .. math::\n        VRF(\\hat{x}_{n,n}) = \\\\frac{VAR(\\hat{x}_{n,n})}{\\sigma^2_x}\n\n        VRF(\\hat{\\dot{x}}_{n,n}) = \\\\frac{VAR(\\hat{\\dot{x}}_{n,n})}{\\sigma^2_x}\n\n        VRF(\\hat{\\ddot{x}}_{n,n}) = \\\\frac{VAR(\\hat{\\ddot{x}}_{n,n})}{\\sigma^2_x}\n\n    Returns\n    -------\n\n    vrf_x : type(x)\n        VRF of x state variable\n\n    vrf_dx : type(x)\n        VRF of the dx state variable (derivative of x)\n\n    vrf_ddx : type(x)\n        VRF of the ddx state variable (second derivative of x)\n    \"\"\"\n\n    g = self.g\n    h = self.h\n    k = self.k\n\n    # common subexpressions in the equations pulled out for efficiency,\n    # they don't 'mean' anything.\n    hg4 = 4 - 2 * g - h\n    ghk = g * h + g * k - 2 * k\n\n    vx = (2 * h * (2 * (g**2) + 2 * h - 3 * g * h) - 2 * g * k * hg4) / (2 * k - g * (h + k) * hg4)\n    vdx = (2 * (h**3) - 4 * (h**2) * k + 4 * (k**2) * (2 - g)) / (2 * hg4 * ghk)\n    vddx = 8 * h * (k**2) / ((self.dt**4) * hg4 * ghk)\n\n    return (vx, vdx, vddx)\n</code></pre>"},{"location":"algorithms/ghk-filter/#bayesian_filters.gh.gh_filter.GHKFilter.VRF_prediction","title":"<code>VRF_prediction()</code>","text":"<p>Returns the Variance Reduction Factor for x of the prediction step of the filter.</p> <p>This implements the equation</p> <p>.. math::     VRF(\\hat{x}{n+1,n}) = \\frac{VAR(\\hat{x}})}{\\sigma^2_x</p> References <p>Asquith and Woods, \"Total Error Minimization in First and Second Order Prediction Filters\" Report No RE-TR-70-17, U.S. Army Missle Command. Redstone Arsenal, Al. November 24, 1970.</p> Source code in <code>bayesian_filters/gh/gh_filter.py</code> <pre><code>def VRF_prediction(self):\n    r\"\"\"\n    Returns the Variance Reduction Factor for x of the prediction\n    step of the filter.\n\n    This implements the equation\n\n    .. math::\n        VRF(\\hat{x}_{n+1,n}) = \\\\frac{VAR(\\hat{x}_{n+1,n})}{\\sigma^2_x}\n\n    References\n    ----------\n\n    Asquith and Woods, \"Total Error Minimization in First\n    and Second Order Prediction Filters\" Report No RE-TR-70-17, U.S.\n    Army Missle Command. Redstone Arsenal, Al. November 24, 1970.\n    \"\"\"\n\n    g = self.g\n    h = self.h\n    k = self.k\n    gh2 = 2 * g + h\n    return (g * k * (gh2 - 4) + h * (g * gh2 + 2 * h)) / (2 * k - (g * (h + k) * (gh2 - 4)))\n</code></pre>"},{"location":"algorithms/ghk-filter/#bayesian_filters.gh.gh_filter.GHKFilter.batch_filter","title":"<code>batch_filter(data, save_predictions=False)</code>","text":"<p>Performs g-h filter with a fixed g and h.</p> <p>Uses self.x and self.dx to initialize the filter, but DOES NOT alter self.x and self.dx during execution, allowing you to use this class multiple times without reseting self.x and self.dx. I'm not sure how often you would need to do that, but the capability is there. More exactly, none of the class member variables are modified by this function.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list_like</code> <p>contains the data to be filtered.</p> required <code>save_predictions</code> <code>boolean</code> <p>The predictions will be saved and returned if this is true</p> <code>False</code> <p>Returns:</p> Name Type Description <code>results</code> <code>np.array shape (n+1, 2), where n=len(data)</code> <p>contains the results of the filter, where results[i,0] is x , and results[i,1] is dx (derivative of x) First entry is the initial values of x and dx as set by init.</p> <code>predictions</code> <code>np.array shape(n), or None</code> <p>the predictions for each step in the filter. Only returned if save_predictions == True</p> Source code in <code>bayesian_filters/gh/gh_filter.py</code> <pre><code>def batch_filter(self, data, save_predictions=False):\n    \"\"\"\n    Performs g-h filter with a fixed g and h.\n\n    Uses self.x and self.dx to initialize the filter, but DOES NOT\n    alter self.x and self.dx during execution, allowing you to use this\n    class multiple times without reseting self.x and self.dx. I'm not sure\n    how often you would need to do that, but the capability is there.\n    More exactly, none of the class member variables are modified\n    by this function.\n\n    Parameters\n    ----------\n\n    data : list_like\n        contains the data to be filtered.\n\n    save_predictions : boolean\n        The predictions will be saved and returned if this is true\n\n    Returns\n    -------\n\n    results : np.array shape (n+1, 2), where n=len(data)\n        contains the results of the filter, where\n        results[i,0] is x , and\n        results[i,1] is dx (derivative of x)\n        First entry is the initial values of x and dx as set by __init__.\n\n    predictions : np.array shape(n), or None\n        the predictions for each step in the filter. Only returned if\n        save_predictions == True\n    \"\"\"\n\n    x = self.x\n    dx = self.dx\n    n = len(data)\n\n    results = np.zeros((n + 1, 2))\n    results[0, 0] = x\n    results[0, 1] = dx\n\n    if save_predictions:\n        predictions = np.zeros(n)\n\n    # optimization to avoid n computations of h / dt\n    h_dt = self.h / self.dt\n\n    for i, z in enumerate(data):\n        # prediction step\n        x_est = x + (dx * self.dt)\n\n        # update step\n        residual = z - x_est\n        dx = dx + h_dt * residual  # i.e. dx = dx + h * residual / dt\n        x = x_est + self.g * residual\n\n        results[i + 1, 0] = x\n        results[i + 1, 1] = dx\n        if save_predictions:\n            predictions[i] = x_est\n\n    if save_predictions:\n        return results, predictions\n\n    return results\n</code></pre>"},{"location":"algorithms/ghk-filter/#bayesian_filters.gh.gh_filter.GHKFilter.bias_error","title":"<code>bias_error(dddx)</code>","text":"<p>Returns the bias error given the specified constant jerk(dddx)</p> <p>Parameters:</p> Name Type Description Default <code>dddx</code> <code>type(x)</code> <p>3rd derivative (jerk) of the state variable x.</p> required References <p>Asquith and Woods, \"Total Error Minimization in First and Second Order Prediction Filters\" Report No RE-TR-70-17, U.S. Army Missle Command. Redstone Arsenal, Al. November 24, 1970.</p> Source code in <code>bayesian_filters/gh/gh_filter.py</code> <pre><code>def bias_error(self, dddx):\n    \"\"\"\n    Returns the bias error given the specified constant jerk(dddx)\n\n    Parameters\n    ----------\n\n    dddx : type(self.x)\n        3rd derivative (jerk) of the state variable x.\n\n    References\n    ----------\n\n    Asquith and Woods, \"Total Error Minimization in First\n    and Second Order Prediction Filters\" Report No RE-TR-70-17, U.S.\n    Army Missle Command. Redstone Arsenal, Al. November 24, 1970.\n    \"\"\"\n    return -(self.dt**3) * dddx / (2 * self.k)\n</code></pre>"},{"location":"algorithms/ghk-filter/#bayesian_filters.gh.gh_filter.GHKFilter.update","title":"<code>update(z, g=None, h=None, k=None)</code>","text":"<p>Performs the g-h filter predict and update step on the measurement z.</p> <p>On return, self.x, self.dx, self.y, and self.x_prediction will have been updated with the results of the computation. For convienence, self.x and self.dx are returned in a tuple.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>scalar</code> <p>the measurement</p> required <code>g</code> <code>scalar(optional)</code> <p>Override the fixed self.g value for this update</p> <code>None</code> <code>h</code> <code>scalar(optional)</code> <p>Override the fixed self.h value for this update</p> <code>None</code> <code>k</code> <code>scalar(optional)</code> <p>Override the fixed self.k value for this update</p> <code>None</code> <p>Returns:</p> Type Description <code>x filter output for x</code> <code>dx filter output for dx (derivative of x</code> Source code in <code>bayesian_filters/gh/gh_filter.py</code> <pre><code>def update(self, z, g=None, h=None, k=None):\n    \"\"\"\n    Performs the g-h filter predict and update step on the\n    measurement z.\n\n    On return, self.x, self.dx, self.y, and self.x_prediction\n    will have been updated with the results of the computation. For\n    convienence, self.x and self.dx are returned in a tuple.\n\n    Parameters\n    ----------\n\n    z : scalar\n        the measurement\n    g : scalar (optional)\n        Override the fixed self.g value for this update\n    h : scalar (optional)\n        Override the fixed self.h value for this update\n    k : scalar (optional)\n        Override the fixed self.k value for this update\n\n    Returns\n    -------\n\n    x filter output for x\n    dx filter output for dx (derivative of x\n\n    \"\"\"\n\n    if g is None:\n        g = self.g\n    if h is None:\n        h = self.h\n    if k is None:\n        k = self.k\n\n    dt = self.dt\n    dt_sqr = dt**2\n    # prediction step\n    self.ddx_prediction = self.ddx\n    self.dx_prediction = self.dx + self.ddx * dt\n    self.x_prediction = self.x + self.dx * dt + 0.5 * self.ddx * (dt_sqr)\n\n    # update step\n    self.y = z - self.x_prediction\n\n    self.ddx = self.ddx_prediction + 2 * k * self.y / dt_sqr\n    self.dx = self.dx_prediction + h * self.y / dt\n    self.x = self.x_prediction + g * self.y\n\n    return (self.x, self.dx)\n</code></pre>"},{"location":"algorithms/least-squares/","title":"Least Squares Filters","text":"<p>Least Squares filters provide an alternative approach to estimation that minimizes the sum of squared errors.</p>"},{"location":"algorithms/least-squares/#overview","title":"Overview","text":"<p>Least squares estimation finds the parameters that minimize the sum of the squared differences between observed and predicted values. This technique is foundational to many filtering approaches.</p>"},{"location":"algorithms/least-squares/#api-reference","title":"API Reference","text":"<p>For detailed API documentation, see the Least Squares API reference.</p>"},{"location":"algorithms/least-squares/#further-reading","title":"Further Reading","text":"<p>For comprehensive examples and theory, see the companion book: Kalman and Bayesian Filters in Python</p>"},{"location":"algorithms/monte-carlo/","title":"Monte Carlo Methods","text":"<p>Monte Carlo methods use random sampling to solve problems that might be deterministic in principle.</p>"},{"location":"algorithms/monte-carlo/#overview","title":"Overview","text":"<p>In the context of filtering, Monte Carlo methods are primarily used for:</p> <ul> <li>Particle Filters - Represent the posterior distribution using a set of particles</li> <li>Resampling - Techniques for selecting particles based on their weights</li> <li>Importance Sampling - Drawing samples from a proposal distribution</li> </ul>"},{"location":"algorithms/monte-carlo/#resampling-methods","title":"Resampling Methods","text":"<p>The library provides several resampling algorithms:</p> <ul> <li>Multinomial Resampling</li> <li>Residual Resampling</li> <li>Stratified Resampling</li> <li>Systematic Resampling</li> </ul>"},{"location":"algorithms/monte-carlo/#api-reference","title":"API Reference","text":"<p>For detailed API documentation, see the Monte Carlo API reference.</p>"},{"location":"algorithms/monte-carlo/#further-reading","title":"Further Reading","text":"<p>For comprehensive examples and theory, see the companion book: Kalman and Bayesian Filters in Python</p>"},{"location":"api/common/","title":"Common","text":""},{"location":"api/common/#common","title":"common","text":"<p>A collection of functions used throughout FilterPy, and/or functions that you will find useful when you build your filters.</p>"},{"location":"api/common/#saver","title":"Saver","text":""},{"location":"api/common/#bayesian_filters.common.helpers.Saver","title":"<code>Saver</code>","text":"<p>               Bases: <code>object</code></p> <p>Helper class to save the states of any filter object. Each time you call save() all of the attributes (state, covariances, etc) are appended to lists.</p> <p>Generally you would do this once per epoch - predict/update.</p> <p>Then, you can access any of the states by using the [] syntax or by using the . operator.</p> <p>.. code-block:: Python</p> <pre><code>my_saver = Saver()\n... do some filtering\n\nx = my_saver['x']\nx = my_save.x\n</code></pre> <p>Either returns a list of all of the state <code>x</code> values for the entire filtering process.</p> <p>If you want to convert all saved lists into numpy arrays, call to_array().</p> <p>Parameters:</p> Name Type Description Default <code>kf</code> <code>object</code> <p>any object with a dict attribute, but intended to be one of the filtering classes</p> required <code>save_current</code> <code>bool</code> <p>save the current state of <code>kf</code> when the object is created;</p> <code>False</code> <code>skip_private</code> <p>Control skipping any private attribute (anything starting with '_') Turning this on saves memory, but slows down execution a bit.</p> <code>False</code> <code>skip_callable</code> <p>Control skipping any attribute which is a method. Turning this on saves memory, but slows down execution a bit.</p> <code>False</code> <code>ignore</code> <p>list of keys to ignore.</p> <code>()</code> <p>Examples:</p> <p>.. code-block:: Python</p> <pre><code>kf = KalmanFilter(...whatever)\n# initialize kf here\n\nsaver = Saver(kf) # save data for kf filter\nfor z in zs:\n    kf.predict()\n    kf.update(z)\n    saver.save()\n\nx = np.array(s.x) # get the kf.x state in an np.array\nplt.plot(x[:, 0], x[:, 2])\n\n# ... or ...\ns.to_array()\nplt.plot(s.x[:, 0], s.x[:, 2])\n</code></pre> Source code in <code>bayesian_filters/common/helpers.py</code> <pre><code>class Saver(object):\n    \"\"\"\n    Helper class to save the states of any filter object.\n    Each time you call save() all of the attributes (state, covariances, etc)\n    are appended to lists.\n\n    Generally you would do this once per epoch - predict/update.\n\n    Then, you can access any of the states by using the [] syntax or by\n    using the . operator.\n\n    .. code-block:: Python\n\n        my_saver = Saver()\n        ... do some filtering\n\n        x = my_saver['x']\n        x = my_save.x\n\n    Either returns a list of all of the state `x` values for the entire\n    filtering process.\n\n    If you want to convert all saved lists into numpy arrays, call to_array().\n\n\n    Parameters\n    ----------\n\n    kf : object\n        any object with a __dict__ attribute, but intended to be one of the\n        filtering classes\n\n    save_current : bool, default=False\n        save the current state of `kf` when the object is created;\n\n    skip_private: bool, default=False\n        Control skipping any private attribute (anything starting with '_')\n        Turning this on saves memory, but slows down execution a bit.\n\n    skip_callable: bool, default=False\n        Control skipping any attribute which is a method. Turning this on\n        saves memory, but slows down execution a bit.\n\n    ignore: (str,) tuple of strings\n        list of keys to ignore.\n\n    Examples\n    --------\n\n    .. code-block:: Python\n\n        kf = KalmanFilter(...whatever)\n        # initialize kf here\n\n        saver = Saver(kf) # save data for kf filter\n        for z in zs:\n            kf.predict()\n            kf.update(z)\n            saver.save()\n\n        x = np.array(s.x) # get the kf.x state in an np.array\n        plt.plot(x[:, 0], x[:, 2])\n\n        # ... or ...\n        s.to_array()\n        plt.plot(s.x[:, 0], s.x[:, 2])\n\n    \"\"\"\n\n    def __init__(self, kf, save_current=False, skip_private=False, skip_callable=False, ignore=()):\n        \"\"\"Construct the save object, optionally saving the current\n        state of the filter\"\"\"\n        # pylint: disable=too-many-arguments\n\n        self._kf = kf\n        self._DL = defaultdict(list)\n        self._skip_private = skip_private\n        self._skip_callable = skip_callable\n        self._ignore = ignore\n        self._len = 0\n\n        # need to save all properties since it is possible that the property\n        # is computed only on access. I use this trick a lot to minimize\n        # computing unused information.\n        properties = inspect.getmembers(type(kf), lambda o: isinstance(o, property))\n        self.properties = [p for p in properties if p[0] not in ignore]\n\n        if save_current:\n            self.save()\n\n    def save(self):\n        \"\"\"save the current state of the Kalman filter\"\"\"\n\n        kf = self._kf\n\n        # force all attributes to be computed. this is only necessary\n        # if the class uses properties that compute data only when\n        # accessed\n        for prop in self.properties:\n            self._DL[prop[0]].append(getattr(kf, prop[0]))\n\n        v = copy.deepcopy(kf.__dict__)\n\n        if self._skip_private:\n            for key in list(v.keys()):\n                if key.startswith(\"_\"):\n                    del v[key]\n\n        if self._skip_callable:\n            for key in list(v.keys()):\n                if callable(v[key]):\n                    del v[key]\n\n        for ig in self._ignore:\n            if ig in v:\n                del v[ig]\n\n        for key in list(v.keys()):\n            self._DL[key].append(v[key])\n\n        self.__dict__.update(self._DL)\n        self._len += 1\n\n    def __getitem__(self, key):\n        return self._DL[key]\n\n    def __setitem__(self, key, newvalue):\n        self._DL[key] = newvalue\n        self.__dict__.update(self._DL)\n\n    def __len__(self):\n        return self._len\n\n    @property\n    def keys(self):\n        \"\"\"list of all keys\"\"\"\n        return list(self._DL.keys())\n\n    def to_array(self, flatten=False):\n        \"\"\"\n        Convert all saved attributes from a list to np.array.\n\n        This may or may not work - every saved attribute must have the\n        same shape for every instance. i.e., if `K` changes shape due to `z`\n        changing shape then the call will raise an exception.\n\n        This can also happen if the default initialization in __init__ gives\n        the variable a different shape then it becomes after a predict/update\n        cycle.\n        \"\"\"\n        for key in self.keys:\n            try:\n                self.__dict__[key] = np.array(self._DL[key])\n            except:\n                # get back to lists so we are in a valid state\n                self.__dict__.update(self._DL)\n                raise ValueError(\"could not convert {} into np.array\".format(key))\n        if flatten:\n            self.flatten()\n\n    def flatten(self):\n        \"\"\"\n        Flattens any np.array of column vectors into 1D arrays. Basically,\n        this makes data readable for humans if you are just inspecting via\n        the REPL. For example, if you have saved a KalmanFilter object with 89\n        epochs, self.x will be shape (89, 9, 1) (for example). After flatten\n        is run, self.x.shape == (89, 9), which displays nicely from the REPL.\n\n        There is no way to unflatten, so it's a one way trip.\n        \"\"\"\n\n        for key in self.keys:\n            try:\n                arr = self.__dict__[key]\n                shape = arr.shape\n                if shape[2] == 1:\n                    self.__dict__[key] = arr.reshape(shape[0], shape[1])\n                arr = self.__dict__[key]\n                shape = arr.shape\n                if len(shape) == 2 and shape[1] == 1:\n                    self.__dict__[key] = arr.ravel()\n            except:\n                # not an ndarray or not a column vector\n                pass\n\n    def __repr__(self):\n        return \"&lt;Saver object at {}\\n  Keys: {}&gt;\".format(hex(id(self)), \" \".join(self.keys))\n</code></pre>"},{"location":"api/common/#bayesian_filters.common.helpers.Saver.keys","title":"<code>keys</code>  <code>property</code>","text":"<p>list of all keys</p>"},{"location":"api/common/#bayesian_filters.common.helpers.Saver.__init__","title":"<code>__init__(kf, save_current=False, skip_private=False, skip_callable=False, ignore=())</code>","text":"<p>Construct the save object, optionally saving the current state of the filter</p> Source code in <code>bayesian_filters/common/helpers.py</code> <pre><code>def __init__(self, kf, save_current=False, skip_private=False, skip_callable=False, ignore=()):\n    \"\"\"Construct the save object, optionally saving the current\n    state of the filter\"\"\"\n    # pylint: disable=too-many-arguments\n\n    self._kf = kf\n    self._DL = defaultdict(list)\n    self._skip_private = skip_private\n    self._skip_callable = skip_callable\n    self._ignore = ignore\n    self._len = 0\n\n    # need to save all properties since it is possible that the property\n    # is computed only on access. I use this trick a lot to minimize\n    # computing unused information.\n    properties = inspect.getmembers(type(kf), lambda o: isinstance(o, property))\n    self.properties = [p for p in properties if p[0] not in ignore]\n\n    if save_current:\n        self.save()\n</code></pre>"},{"location":"api/common/#bayesian_filters.common.helpers.Saver.flatten","title":"<code>flatten()</code>","text":"<p>Flattens any np.array of column vectors into 1D arrays. Basically, this makes data readable for humans if you are just inspecting via the REPL. For example, if you have saved a KalmanFilter object with 89 epochs, self.x will be shape (89, 9, 1) (for example). After flatten is run, self.x.shape == (89, 9), which displays nicely from the REPL.</p> <p>There is no way to unflatten, so it's a one way trip.</p> Source code in <code>bayesian_filters/common/helpers.py</code> <pre><code>def flatten(self):\n    \"\"\"\n    Flattens any np.array of column vectors into 1D arrays. Basically,\n    this makes data readable for humans if you are just inspecting via\n    the REPL. For example, if you have saved a KalmanFilter object with 89\n    epochs, self.x will be shape (89, 9, 1) (for example). After flatten\n    is run, self.x.shape == (89, 9), which displays nicely from the REPL.\n\n    There is no way to unflatten, so it's a one way trip.\n    \"\"\"\n\n    for key in self.keys:\n        try:\n            arr = self.__dict__[key]\n            shape = arr.shape\n            if shape[2] == 1:\n                self.__dict__[key] = arr.reshape(shape[0], shape[1])\n            arr = self.__dict__[key]\n            shape = arr.shape\n            if len(shape) == 2 and shape[1] == 1:\n                self.__dict__[key] = arr.ravel()\n        except:\n            # not an ndarray or not a column vector\n            pass\n</code></pre>"},{"location":"api/common/#bayesian_filters.common.helpers.Saver.save","title":"<code>save()</code>","text":"<p>save the current state of the Kalman filter</p> Source code in <code>bayesian_filters/common/helpers.py</code> <pre><code>def save(self):\n    \"\"\"save the current state of the Kalman filter\"\"\"\n\n    kf = self._kf\n\n    # force all attributes to be computed. this is only necessary\n    # if the class uses properties that compute data only when\n    # accessed\n    for prop in self.properties:\n        self._DL[prop[0]].append(getattr(kf, prop[0]))\n\n    v = copy.deepcopy(kf.__dict__)\n\n    if self._skip_private:\n        for key in list(v.keys()):\n            if key.startswith(\"_\"):\n                del v[key]\n\n    if self._skip_callable:\n        for key in list(v.keys()):\n            if callable(v[key]):\n                del v[key]\n\n    for ig in self._ignore:\n        if ig in v:\n            del v[ig]\n\n    for key in list(v.keys()):\n        self._DL[key].append(v[key])\n\n    self.__dict__.update(self._DL)\n    self._len += 1\n</code></pre>"},{"location":"api/common/#bayesian_filters.common.helpers.Saver.to_array","title":"<code>to_array(flatten=False)</code>","text":"<p>Convert all saved attributes from a list to np.array.</p> <p>This may or may not work - every saved attribute must have the same shape for every instance. i.e., if <code>K</code> changes shape due to <code>z</code> changing shape then the call will raise an exception.</p> <p>This can also happen if the default initialization in init gives the variable a different shape then it becomes after a predict/update cycle.</p> Source code in <code>bayesian_filters/common/helpers.py</code> <pre><code>def to_array(self, flatten=False):\n    \"\"\"\n    Convert all saved attributes from a list to np.array.\n\n    This may or may not work - every saved attribute must have the\n    same shape for every instance. i.e., if `K` changes shape due to `z`\n    changing shape then the call will raise an exception.\n\n    This can also happen if the default initialization in __init__ gives\n    the variable a different shape then it becomes after a predict/update\n    cycle.\n    \"\"\"\n    for key in self.keys:\n        try:\n            self.__dict__[key] = np.array(self._DL[key])\n        except:\n            # get back to lists so we are in a valid state\n            self.__dict__.update(self._DL)\n            raise ValueError(\"could not convert {} into np.array\".format(key))\n    if flatten:\n        self.flatten()\n</code></pre>"},{"location":"api/common/#discrete-white-noise","title":"Discrete White Noise","text":""},{"location":"api/common/#bayesian_filters.common.discretization.Q_discrete_white_noise","title":"<code>Q_discrete_white_noise(dim, dt=1.0, var=1.0, block_size=1, order_by_dim=True)</code>","text":"<p>Returns the Q matrix for the Discrete Constant White Noise Model. dim may be either 2, 3, or 4 dt is the time step, and sigma is the variance in the noise.</p> <p>Q is computed as the G * G^T * variance, where G is the process noise per time step. In other words, G = [[.5dt^2][dt]]^T for the constant velocity model.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int (2, 3, or 4)</code> <p>dimension for Q, where the final dimension is (dim x dim)</p> required <code>dt</code> <code>float</code> <p>time step in whatever units your filter is using for time. i.e. the amount of time between innovations</p> <code>1.0</code> <code>var</code> <code>float</code> <p>variance in the noise</p> <code>1.0</code> <code>block_size</code> <code>int &gt;= 1</code> <p>If your state variable contains more than one dimension, such as a 3d constant velocity model [x x' y y' z z']^T, then Q must be a block diagonal matrix.</p> <code>1</code> <code>order_by_dim</code> <code>bool</code> <p>Defines ordering of variables in the state vector. <code>True</code> orders by keeping all derivatives of each dimensions)</p> <p>[x x' x'' y y' y'']</p> <p>whereas <code>False</code> interleaves the dimensions</p> <p>[x y z x' y' z' x'' y'' z'']</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # constant velocity model in a 3D world with a 10 Hz update rate\n&gt;&gt;&gt; Q_discrete_white_noise(2, dt=0.1, var=1., block_size=3)\narray([[0.000025, 0.0005  , 0.      , 0.      , 0.      , 0.      ],\n       [0.0005  , 0.01    , 0.      , 0.      , 0.      , 0.      ],\n       [0.      , 0.      , 0.000025, 0.0005  , 0.      , 0.      ],\n       [0.      , 0.      , 0.0005  , 0.01    , 0.      , 0.      ],\n       [0.      , 0.      , 0.      , 0.      , 0.000025, 0.0005  ],\n       [0.      , 0.      , 0.      , 0.      , 0.0005  , 0.01    ]])\n</code></pre> References <p>Bar-Shalom. \"Estimation with Applications To Tracking and Navigation\". John Wiley &amp; Sons, 2001. Page 274.</p> Source code in <code>bayesian_filters/common/discretization.py</code> <pre><code>def Q_discrete_white_noise(dim, dt=1.0, var=1.0, block_size=1, order_by_dim=True):\n    \"\"\"\n    Returns the Q matrix for the Discrete Constant White Noise\n    Model. dim may be either 2, 3, or 4 dt is the time step, and sigma\n    is the variance in the noise.\n\n    Q is computed as the G * G^T * variance, where G is the process noise per\n    time step. In other words, G = [[.5dt^2][dt]]^T for the constant velocity\n    model.\n\n    Parameters\n    -----------\n\n    dim : int (2, 3, or 4)\n        dimension for Q, where the final dimension is (dim x dim)\n\n    dt : float, default=1.0\n        time step in whatever units your filter is using for time. i.e. the\n        amount of time between innovations\n\n    var : float, default=1.0\n        variance in the noise\n\n    block_size : int &gt;= 1\n        If your state variable contains more than one dimension, such as\n        a 3d constant velocity model [x x' y y' z z']^T, then Q must be\n        a block diagonal matrix.\n\n    order_by_dim : bool, default=True\n        Defines ordering of variables in the state vector. `True` orders\n        by keeping all derivatives of each dimensions)\n\n        [x x' x'' y y' y'']\n\n        whereas `False` interleaves the dimensions\n\n        [x y z x' y' z' x'' y'' z'']\n\n\n    Examples\n    --------\n    &gt;&gt;&gt; # constant velocity model in a 3D world with a 10 Hz update rate\n    &gt;&gt;&gt; Q_discrete_white_noise(2, dt=0.1, var=1., block_size=3)\n    array([[0.000025, 0.0005  , 0.      , 0.      , 0.      , 0.      ],\n           [0.0005  , 0.01    , 0.      , 0.      , 0.      , 0.      ],\n           [0.      , 0.      , 0.000025, 0.0005  , 0.      , 0.      ],\n           [0.      , 0.      , 0.0005  , 0.01    , 0.      , 0.      ],\n           [0.      , 0.      , 0.      , 0.      , 0.000025, 0.0005  ],\n           [0.      , 0.      , 0.      , 0.      , 0.0005  , 0.01    ]])\n\n    References\n    ----------\n\n    Bar-Shalom. \"Estimation with Applications To Tracking and Navigation\".\n    John Wiley &amp; Sons, 2001. Page 274.\n    \"\"\"\n\n    if dim not in [2, 3, 4]:\n        raise ValueError(\"dim must be between 2 and 4\")\n\n    if dim == 2:\n        Q = [[0.25 * dt**4, 0.5 * dt**3], [0.5 * dt**3, dt**2]]\n    elif dim == 3:\n        Q = [\n            [0.25 * dt**4, 0.5 * dt**3, 0.5 * dt**2],\n            [0.5 * dt**3, dt**2, dt],\n            [0.5 * dt**2, dt, 1],\n        ]\n    else:\n        Q = [\n            [(dt**6) / 36, (dt**5) / 12, (dt**4) / 6, (dt**3) / 6],\n            [(dt**5) / 12, (dt**4) / 4, (dt**3) / 2, (dt**2) / 2],\n            [(dt**4) / 6, (dt**3) / 2, dt**2, dt],\n            [(dt**3) / 6, (dt**2) / 2, dt, 1.0],\n        ]\n\n    if order_by_dim:\n        return block_diag(*[Q] * block_size) * var\n    return order_by_derivative(array(Q), dim, block_size) * var\n</code></pre>"},{"location":"api/common/#continuous-white-noise","title":"Continuous White Noise","text":""},{"location":"api/common/#bayesian_filters.common.discretization.Q_continuous_white_noise","title":"<code>Q_continuous_white_noise(dim, dt=1.0, spectral_density=1.0, block_size=1, order_by_dim=True)</code>","text":"<p>Returns the Q matrix for the Discretized Continuous White Noise Model. dim may be either 2, 3, 4, dt is the time step, and sigma is the variance in the noise.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int(2 or 3 or 4)</code> <p>dimension for Q, where the final dimension is (dim x dim) 2 is constant velocity, 3 is constant acceleration, 4 is constant jerk</p> required <code>dt</code> <code>float</code> <p>time step in whatever units your filter is using for time. i.e. the amount of time between innovations</p> <code>1.0</code> <code>spectral_density</code> <code>float</code> <p>spectral density for the continuous process</p> <code>1.0</code> <code>block_size</code> <code>int &gt;= 1</code> <p>If your state variable contains more than one dimension, such as a 3d constant velocity model [x x' y y' z z']^T, then Q must be a block diagonal matrix.</p> <code>1</code> <code>order_by_dim</code> <code>bool</code> <p>Defines ordering of variables in the state vector. <code>True</code> orders by keeping all derivatives of each dimensions)</p> <p>[x x' x'' y y' y'']</p> <p>whereas <code>False</code> interleaves the dimensions</p> <p>[x y z x' y' z' x'' y'' z'']</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # constant velocity model in a 3D world with a 10 Hz update rate\n&gt;&gt;&gt; Q_continuous_white_noise(2, dt=0.1, block_size=3)\narray([[0.00033333, 0.005     , 0.        , 0.        , 0.        , 0.        ],\n       [0.005     , 0.1       , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.00033333, 0.005     , 0.        , 0.        ],\n       [0.        , 0.        , 0.005     , 0.1       , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.00033333, 0.005     ],\n       [0.        , 0.        , 0.        , 0.        , 0.005     , 0.1       ]])\n</code></pre> Source code in <code>bayesian_filters/common/discretization.py</code> <pre><code>def Q_continuous_white_noise(dim, dt=1.0, spectral_density=1.0, block_size=1, order_by_dim=True):\n    \"\"\"\n    Returns the Q matrix for the Discretized Continuous White Noise\n    Model. dim may be either 2, 3, 4, dt is the time step, and sigma is the\n    variance in the noise.\n\n    Parameters\n    ----------\n\n    dim : int (2 or 3 or 4)\n        dimension for Q, where the final dimension is (dim x dim)\n        2 is constant velocity, 3 is constant acceleration, 4 is\n        constant jerk\n\n    dt : float, default=1.0\n        time step in whatever units your filter is using for time. i.e. the\n        amount of time between innovations\n\n    spectral_density : float, default=1.0\n        spectral density for the continuous process\n\n    block_size : int &gt;= 1\n        If your state variable contains more than one dimension, such as\n        a 3d constant velocity model [x x' y y' z z']^T, then Q must be\n        a block diagonal matrix.\n\n    order_by_dim : bool, default=True\n        Defines ordering of variables in the state vector. `True` orders\n        by keeping all derivatives of each dimensions)\n\n        [x x' x'' y y' y'']\n\n        whereas `False` interleaves the dimensions\n\n        [x y z x' y' z' x'' y'' z'']\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; # constant velocity model in a 3D world with a 10 Hz update rate\n    &gt;&gt;&gt; Q_continuous_white_noise(2, dt=0.1, block_size=3)\n    array([[0.00033333, 0.005     , 0.        , 0.        , 0.        , 0.        ],\n           [0.005     , 0.1       , 0.        , 0.        , 0.        , 0.        ],\n           [0.        , 0.        , 0.00033333, 0.005     , 0.        , 0.        ],\n           [0.        , 0.        , 0.005     , 0.1       , 0.        , 0.        ],\n           [0.        , 0.        , 0.        , 0.        , 0.00033333, 0.005     ],\n           [0.        , 0.        , 0.        , 0.        , 0.005     , 0.1       ]])\n    \"\"\"\n\n    if dim not in [2, 3, 4]:\n        raise ValueError(\"dim must be between 2 and 4\")\n\n    if dim == 2:\n        Q = [[(dt**3) / 3.0, (dt**2) / 2.0], [(dt**2) / 2.0, dt]]\n    elif dim == 3:\n        Q = [\n            [(dt**5) / 20.0, (dt**4) / 8.0, (dt**3) / 6.0],\n            [(dt**4) / 8.0, (dt**3) / 3.0, (dt**2) / 2.0],\n            [(dt**3) / 6.0, (dt**2) / 2.0, dt],\n        ]\n\n    else:\n        Q = [\n            [(dt**7) / 252.0, (dt**6) / 72.0, (dt**5) / 30.0, (dt**4) / 24.0],\n            [(dt**6) / 72.0, (dt**5) / 20.0, (dt**4) / 8.0, (dt**3) / 6.0],\n            [(dt**5) / 30.0, (dt**4) / 8.0, (dt**3) / 3.0, (dt**2) / 2.0],\n            [(dt**4) / 24.0, (dt**3) / 6.0, (dt**2 / 2.0), dt],\n        ]\n\n    if order_by_dim:\n        return block_diag(*[Q] * block_size) * spectral_density\n\n    return order_by_derivative(array(Q), dim, block_size) * spectral_density\n</code></pre>"},{"location":"api/common/#van-loan-discretization","title":"Van Loan Discretization","text":""},{"location":"api/common/#bayesian_filters.common.discretization.van_loan_discretization","title":"<code>van_loan_discretization(F, G, dt)</code>","text":"<p>Discretizes a linear differential equation which includes white noise according to the method of C. F. van Loan [1]. Given the continuous model</p> <pre><code>x' =  Fx + Gu\n</code></pre> <p>where u is the unity white noise, we compute and return the sigma and Q_k that discretizes that equation.</p> <p>Examples:</p> <p>Given y'' + y = 2u(t), we create the continuous state model of</p> <p>x' = [ 0 1] * x + [0]*u(t)      [-1 0]       [2]</p> <p>and a time step of 0.1:</p> <pre><code>&gt;&gt;&gt; F = np.array([[0,1],[-1,0]], dtype=float)\n&gt;&gt;&gt; G = np.array([[0.],[2.]])\n&gt;&gt;&gt; phi, Q = van_loan_discretization(F, G, 0.1)\n</code></pre> <pre><code>&gt;&gt;&gt; phi\narray([[ 0.99500417,  0.09983342],\n       [-0.09983342,  0.99500417]])\n</code></pre> <pre><code>&gt;&gt;&gt; Q\narray([[ 0.00133067,  0.01993342],\n       [ 0.01993342,  0.39866933]])\n</code></pre> <p>(example taken from Brown[2])</p> References <p>[1] C. F. van Loan. \"Computing Integrals Involving the Matrix Exponential.\"     IEEE Trans. Automomatic Control, AC-23 (3): 395-404 (June 1978)</p> <p>[2] Robert Grover Brown. \"Introduction to Random Signals and Applied     Kalman Filtering.\" Forth edition. John Wiley &amp; Sons. p. 126-7. (2012)</p> Source code in <code>bayesian_filters/common/discretization.py</code> <pre><code>def van_loan_discretization(F, G, dt):\n    \"\"\"\n    Discretizes a linear differential equation which includes white noise\n    according to the method of C. F. van Loan [1]. Given the continuous\n    model\n\n        x' =  Fx + Gu\n\n    where u is the unity white noise, we compute and return the sigma and Q_k\n    that discretizes that equation.\n\n\n    Examples\n    --------\n\n    Given y'' + y = 2u(t), we create the continuous state model of\n\n    x' = [ 0 1] * x + [0]*u(t)\n         [-1 0]       [2]\n\n    and a time step of 0.1:\n\n\n    &gt;&gt;&gt; F = np.array([[0,1],[-1,0]], dtype=float)\n    &gt;&gt;&gt; G = np.array([[0.],[2.]])\n    &gt;&gt;&gt; phi, Q = van_loan_discretization(F, G, 0.1)\n\n    &gt;&gt;&gt; phi\n    array([[ 0.99500417,  0.09983342],\n           [-0.09983342,  0.99500417]])\n\n    &gt;&gt;&gt; Q\n    array([[ 0.00133067,  0.01993342],\n           [ 0.01993342,  0.39866933]])\n\n    (example taken from Brown[2])\n\n\n    References\n    ----------\n\n    [1] C. F. van Loan. \"Computing Integrals Involving the Matrix Exponential.\"\n        IEEE Trans. Automomatic Control, AC-23 (3): 395-404 (June 1978)\n\n    [2] Robert Grover Brown. \"Introduction to Random Signals and Applied\n        Kalman Filtering.\" Forth edition. John Wiley &amp; Sons. p. 126-7. (2012)\n    \"\"\"\n\n    n = F.shape[0]\n\n    A = zeros((2 * n, 2 * n))\n\n    # we assume u(t) is unity, and require that G incorporate the scaling term\n    # for the noise. Hence W = 1, and GWG' reduces to GG\"\n\n    A[0:n, 0:n] = -F.dot(dt)\n    A[0:n, n : 2 * n] = G.dot(G.T).dot(dt)\n    A[n : 2 * n, n : 2 * n] = F.T.dot(dt)\n\n    B = expm(A)\n\n    sigma = B[n : 2 * n, n : 2 * n].T\n\n    Q = sigma.dot(B[0:n, n : 2 * n])\n\n    return (sigma, Q)\n</code></pre>"},{"location":"api/common/#linear-ode-discretization","title":"Linear ODE Discretization","text":""},{"location":"api/common/#bayesian_filters.common.discretization.linear_ode_discretation","title":"<code>linear_ode_discretation(F, L=None, Q=None, dt=1.0)</code>","text":"<p>Not sure what this does, probably should be removed</p> Source code in <code>bayesian_filters/common/discretization.py</code> <pre><code>def linear_ode_discretation(F, L=None, Q=None, dt=1.0):\n    \"\"\"\n    Not sure what this does, probably should be removed\n    \"\"\"\n\n    n = F.shape[0]\n\n    if L is None:\n        L = eye(n)\n\n    if Q is None:\n        Q = zeros((n, n))\n\n    A = expm(F * dt)\n\n    phi = zeros((2 * n, 2 * n))\n\n    phi[0:n, 0:n] = F\n    phi[0:n, n : 2 * n] = L.dot(Q).dot(L.T)\n    phi[n : 2 * n, n : 2 * n] = -F.T\n\n    zo = vstack((zeros((n, n)), eye(n)))\n\n    CD = expm(phi * dt).dot(zo)\n\n    C = CD[0:n, :]\n    D = CD[n : 2 * n, :]\n    q = C.dot(inv(D))\n\n    return (A, q)\n</code></pre>"},{"location":"api/common/#kinematic-kalman-filter","title":"Kinematic Kalman Filter","text":""},{"location":"api/common/#bayesian_filters.common.kinematic.kinematic_kf","title":"<code>kinematic_kf(dim, order, dt=1.0, dim_z=1, order_by_dim=True, kf=None)</code>","text":"<p>Returns a KalmanFilter using newtonian kinematics of arbitrary order for any number of dimensions. For example, a constant velocity filter in 3D space would have order 1 dimension 3.</p> <p>Examples:</p> <p>A constant velocity filter in 3D space with delta time = .2 seconds would be created with</p> <pre><code>&gt;&gt;&gt; kf = kinematic_kf(dim=3, order=1, dt=.2)\n&gt;&gt;&gt; kf.F\n&gt;&gt;&gt; array([[1. , 0.2, 0. , 0. , 0. , 0. ],\n           [0. , 1. , 0. , 0. , 0. , 0. ],\n           [0. , 0. , 1. , 0.2, 0. , 0. ],\n           [0. , 0. , 0. , 1. , 0. , 0. ],\n           [0. , 0. , 0. , 0. , 1. , 0.2],\n           [0. , 0. , 0. , 0. , 0. , 1. ]])\n</code></pre> <p>which will set the state <code>x</code> to be interpreted as</p> <p>[x, x', y, y', z, z'].T</p> <p>If you set <code>order_by_dim</code> to False, then <code>x</code> is ordered as</p> <p>[x y z x' y' z'].T</p> <p>As another example, a 2D constant jerk is created with</p> <p>kinematic_kf(2, 3)</p> <p>Assumes that the measurement z is position in each dimension. If this is not true you will have to alter the H matrix by hand.</p> <p>P, Q, R are all set to the Identity matrix.</p> <p>H is assigned assuming the measurement is position, one per dimension <code>dim</code>.</p> <pre><code>&gt;&gt;&gt; kf = kinematic_kf(2, 1, dt=3.0)\n&gt;&gt;&gt; kf.F\narray([[1., 3., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 3.],\n       [0., 0., 0., 1.]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int, &gt;= 1</code> <p>number of dimensions (2D space would be dim=2)</p> required <code>order</code> <code>int, &gt;= 0</code> <p>order of the filter. 2 would be a const acceleration model with a stat</p> required <code>dim_z</code> <code>int</code> <p>size of z vector per dimension <code>dim</code>. Normally should be 1</p> <code>1</code> <code>dt</code> <code>float</code> <p>Time step. Used to create the state transition matrix</p> <code>1.0</code> <code>order_by_dim</code> <code>bool</code> <p>Defines ordering of variables in the state vector. <code>True</code> orders by keeping all derivatives of each dimensions)</p> <p>[x x' x'' y y' y'']</p> <p>whereas <code>False</code> interleaves the dimensions</p> <p>[x y z x' y' z' x'' y'' z'']</p> <code>True</code> <code>kf</code> <code>kalman filter like object</code> <p>Provide your own pre-created filter. This lets you use classes other than KalmanFilter.</p> <code>None</code> Source code in <code>bayesian_filters/common/kinematic.py</code> <pre><code>def kinematic_kf(dim, order, dt=1.0, dim_z=1, order_by_dim=True, kf=None):\n    \"\"\"\n    Returns a KalmanFilter using newtonian kinematics of arbitrary order\n    for any number of dimensions. For example, a constant velocity filter\n    in 3D space would have order 1 dimension 3.\n\n\n    Examples\n    --------\n\n    A constant velocity filter in 3D space with delta time = .2 seconds\n    would be created with\n\n    &gt;&gt;&gt; kf = kinematic_kf(dim=3, order=1, dt=.2)\n    &gt;&gt;&gt; kf.F\n    &gt;&gt;&gt; array([[1. , 0.2, 0. , 0. , 0. , 0. ],\n               [0. , 1. , 0. , 0. , 0. , 0. ],\n               [0. , 0. , 1. , 0.2, 0. , 0. ],\n               [0. , 0. , 0. , 1. , 0. , 0. ],\n               [0. , 0. , 0. , 0. , 1. , 0.2],\n               [0. , 0. , 0. , 0. , 0. , 1. ]])\n\n\n    which will set the state `x` to be interpreted as\n\n    [x, x', y, y', z, z'].T\n\n    If you set `order_by_dim` to False, then `x` is ordered as\n\n    [x y z x' y' z'].T\n\n    As another example, a 2D constant jerk is created with\n\n    &gt;&gt; kinematic_kf(2, 3)\n\n\n    Assumes that the measurement z is position in each dimension. If this is not\n    true you will have to alter the H matrix by hand.\n\n    P, Q, R are all set to the Identity matrix.\n\n    H is assigned assuming the measurement is position, one per dimension `dim`.\n\n\n    &gt;&gt;&gt; kf = kinematic_kf(2, 1, dt=3.0)\n    &gt;&gt;&gt; kf.F\n    array([[1., 3., 0., 0.],\n           [0., 1., 0., 0.],\n           [0., 0., 1., 3.],\n           [0., 0., 0., 1.]])\n\n    Parameters\n    ----------\n\n    dim : int, &gt;= 1\n        number of dimensions (2D space would be dim=2)\n\n    order : int, &gt;= 0\n        order of the filter. 2 would be a const acceleration model with\n        a stat\n\n    dim_z : int, default 1\n        size of z vector *per* dimension `dim`. Normally should be 1\n\n    dt : float, default 1.0\n        Time step. Used to create the state transition matrix\n\n    order_by_dim : bool, default=True\n        Defines ordering of variables in the state vector. `True` orders\n        by keeping all derivatives of each dimensions)\n\n        [x x' x'' y y' y'']\n\n        whereas `False` interleaves the dimensions\n\n        [x y z x' y' z' x'' y'' z'']\n\n    kf : kalman filter like object, optional, default None\n        Provide your own pre-created filter. This lets you use classes other\n        than KalmanFilter.\n    \"\"\"\n\n    from bayesian_filters.kalman import KalmanFilter\n\n    if dim &lt; 1:\n        raise ValueError(\"dim must be &gt;= 1\")\n    if order &lt; 0:\n        raise ValueError(\"order must be &gt;= 0\")\n    if dim_z &lt; 1:\n        raise ValueError(\"dim_z must be &gt;= 1\")\n\n    dim_x = order + 1\n\n    if kf is None:\n        kf = KalmanFilter(dim_x=dim * dim_x, dim_z=dim_z)\n    assert kf.dim_x == dim * dim_x\n    assert kf.dim_z == dim_z\n\n    F = kinematic_state_transition(order, dt)\n    if order_by_dim:\n        diag = [F] * dim\n        kf.F = block_diag(*diag)\n    else:\n        kf.F.fill(0.0)\n        for i, x in enumerate(F.ravel()):\n            f = np.eye(dim) * x\n\n            ix, iy = (i // dim_x) * dim, (i % dim_x) * dim\n            kf.F[ix : ix + dim, iy : iy + dim] = f\n\n    if order_by_dim:\n        for i in range(dim_z):\n            for j in range(dim):\n                kf.H[i, j * dim_x] = 1.0\n    else:\n        for i in range(dim_z):\n            for j in range(dim):\n                kf.H[i, j] = 1.0\n\n    return kf\n</code></pre>"},{"location":"api/common/#kinematic-state-transition","title":"Kinematic State Transition","text":""},{"location":"api/common/#bayesian_filters.common.kinematic.kinematic_state_transition","title":"<code>kinematic_state_transition(order, dt)</code>","text":"<p>create a state transition matrix of a given order for a given time step <code>dt</code>.</p> Source code in <code>bayesian_filters/common/kinematic.py</code> <pre><code>def kinematic_state_transition(order, dt):\n    \"\"\"\n    create a state transition matrix of a given order for a given time\n    step `dt`.\n    \"\"\"\n\n    if not (order &gt;= 0 and int(order) == order):\n        raise ValueError(\"order must be an int &gt;= 0\")\n\n    # hard code common cases for computational efficiency\n    if order == 0:\n        return np.array([[1.0]])\n    if order == 1:\n        return np.array([[1.0, dt], [0.0, 1.0]])\n    if order == 2:\n        return np.array([[1.0, dt, 0.5 * dt * dt], [0.0, 1.0, dt], [0.0, 0.0, 1.0]])\n\n    # grind it out computationally....\n    N = order + 1\n\n    F = np.zeros((N, N))\n    # compute highest order row\n    for n in range(N):\n        F[0, n] = float(dt**n) / math.factorial(n)\n\n    # copy with a shift to get lower order rows\n    for j in range(1, N):\n        F[j, j:] = F[0, 0:-j]\n\n    return F\n</code></pre>"},{"location":"api/common/#runge-kutta-4th-order","title":"Runge-Kutta 4th Order","text":""},{"location":"api/common/#bayesian_filters.common.helpers.runge_kutta4","title":"<code>runge_kutta4(y, x, dx, f)</code>","text":"<p>computes 4th order Runge-Kutta for dy/dx.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>scalar</code> <p>Initial/current value for y</p> required <code>x</code> <code>scalar</code> <p>Initial/current value for x</p> required <code>dx</code> <code>scalar</code> <p>difference in x (e.g. the time step)</p> required <code>f</code> <code>ufunc(y, x)</code> <p>Callable function (y, x) that you supply to compute dy/dx for the specified values.</p> required Source code in <code>bayesian_filters/common/helpers.py</code> <pre><code>def runge_kutta4(y, x, dx, f):\n    \"\"\"computes 4th order Runge-Kutta for dy/dx.\n\n    Parameters\n    ----------\n\n    y : scalar\n        Initial/current value for y\n    x : scalar\n        Initial/current value for x\n    dx : scalar\n        difference in x (e.g. the time step)\n    f : ufunc(y,x)\n        Callable function (y, x) that you supply to compute dy/dx for\n        the specified values.\n\n    \"\"\"\n\n    k1 = dx * f(y, x)\n    k2 = dx * f(y + 0.5 * k1, x + 0.5 * dx)\n    k3 = dx * f(y + 0.5 * k2, x + 0.5 * dx)\n    k4 = dx * f(y + k3, x + dx)\n\n    return y + (k1 + 2 * k2 + 2 * k3 + k4) / 6.0\n</code></pre>"},{"location":"api/common/#inverse-diagonal","title":"Inverse Diagonal","text":""},{"location":"api/common/#bayesian_filters.common.helpers.inv_diagonal","title":"<code>inv_diagonal(S)</code>","text":"<p>Computes the inverse of a diagonal NxN np.array S. In general this will be much faster than calling np.linalg.inv().</p> <p>However, does NOT check if the off diagonal elements are non-zero. So long as S is truly diagonal, the output is identical to np.linalg.inv().</p> <p>Parameters:</p> Name Type Description Default <code>S</code> <code>array</code> <p>diagonal NxN array to take inverse of</p> required <p>Returns:</p> Name Type Description <code>S_inv</code> <code>array</code> <p>inverse of S</p> <p>Examples:</p> <p>This is meant to be used as a replacement inverse function for the KalmanFilter class when you know the system covariance S is diagonal. It just makes the filter run faster, there is</p> <pre><code>&gt;&gt;&gt; kf = KalmanFilter(dim_x=3, dim_z=1)\n&gt;&gt;&gt; kf.inv = inv_diagonal  # S is 1x1, so safely diagonal\n</code></pre> Source code in <code>bayesian_filters/common/helpers.py</code> <pre><code>def inv_diagonal(S):\n    \"\"\"\n    Computes the inverse of a diagonal NxN np.array S. In general this will\n    be much faster than calling np.linalg.inv().\n\n    However, does NOT check if the off diagonal elements are non-zero. So long\n    as S is truly diagonal, the output is identical to np.linalg.inv().\n\n    Parameters\n    ----------\n    S : np.array\n        diagonal NxN array to take inverse of\n\n    Returns\n    -------\n    S_inv : np.array\n        inverse of S\n\n\n    Examples\n    --------\n\n    This is meant to be used as a replacement inverse function for\n    the KalmanFilter class when you know the system covariance S is\n    diagonal. It just makes the filter run faster, there is\n\n    &gt;&gt;&gt; kf = KalmanFilter(dim_x=3, dim_z=1)\n    &gt;&gt;&gt; kf.inv = inv_diagonal  # S is 1x1, so safely diagonal\n    \"\"\"\n\n    S = np.asarray(S)\n\n    if S.ndim != 2 or S.shape[0] != S.shape[1]:\n        raise ValueError(\"S must be a square Matrix\")\n\n    si = np.zeros(S.shape)\n    for i in range(len(S)):\n        si[i, i] = 1.0 / S[i, i]\n    return si\n</code></pre>"},{"location":"api/common/#outer-product-sum","title":"Outer Product Sum","text":""},{"location":"api/common/#bayesian_filters.common.helpers.outer_product_sum","title":"<code>outer_product_sum(A, B=None)</code>","text":"<p>Computes the sum of the outer products of the rows in A and B</p> <pre><code>P = \\Sum {A[i] B[i].T} for i in 0..N\n\nNotionally:\n\nP = 0\nfor y in A:\n    P += np.outer(y, y)\n</code></pre> <p>This is a standard computation for sigma points used in the UKF, ensemble Kalman filter, etc., where A would be the residual of the sigma points and the filter's state or measurement.</p> <p>The computation is vectorized, so it is much faster than the for loop for large A.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>(array, shape(M, N))</code> <p>rows of N-vectors to have the outer product summed</p> required <code>B</code> <code>(array, shape(M, N))</code> <p>rows of N-vectors to have the outer product summed If it is <code>None</code>, it is set to A.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>P</code> <code>(array, shape(N, N))</code> <p>sum of the outer product of the rows of A and B</p> <p>Examples:</p> <p>Here sigmas is of shape (M, N), and x is of shape (N). The two sets of code compute the same thing.</p> <pre><code>&gt;&gt;&gt; P = outer_product_sum(sigmas - x)\n&gt;&gt;&gt;\n&gt;&gt;&gt; P = 0\n&gt;&gt;&gt; for s in sigmas:\n&gt;&gt;&gt;     y = s - x\n&gt;&gt;&gt;     P += np.outer(y, y)\n</code></pre> Source code in <code>bayesian_filters/common/helpers.py</code> <pre><code>def outer_product_sum(A, B=None):\n    r\"\"\"\n    Computes the sum of the outer products of the rows in A and B\n\n        P = \\Sum {A[i] B[i].T} for i in 0..N\n\n        Notionally:\n\n        P = 0\n        for y in A:\n            P += np.outer(y, y)\n\n    This is a standard computation for sigma points used in the UKF, ensemble\n    Kalman filter, etc., where A would be the residual of the sigma points\n    and the filter's state or measurement.\n\n    The computation is vectorized, so it is much faster than the for loop\n    for large A.\n\n    Parameters\n    ----------\n    A : np.array, shape (M, N)\n        rows of N-vectors to have the outer product summed\n\n    B : np.array, shape (M, N)\n        rows of N-vectors to have the outer product summed\n        If it is `None`, it is set to A.\n\n    Returns\n    -------\n    P : np.array, shape(N, N)\n        sum of the outer product of the rows of A and B\n\n    Examples\n    --------\n\n    Here sigmas is of shape (M, N), and x is of shape (N). The two sets of\n    code compute the same thing.\n\n    &gt;&gt;&gt; P = outer_product_sum(sigmas - x)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; P = 0\n    &gt;&gt;&gt; for s in sigmas:\n    &gt;&gt;&gt;     y = s - x\n    &gt;&gt;&gt;     P += np.outer(y, y)\n    \"\"\"\n\n    if B is None:\n        B = A\n\n    outer = np.einsum(\"ij,ik-&gt;ijk\", A, B)\n    return np.sum(outer, axis=0)\n</code></pre>"},{"location":"api/discrete-bayes/","title":"Discrete Bayes Module","text":"<p>The discrete_bayes module contains discrete Bayesian filter implementations.</p>"},{"location":"api/discrete-bayes/#bayesian_filters.discrete_bayes","title":"<code>discrete_bayes</code>","text":"<p>Copyright 2015 Roger R Labbe Jr.</p> <p>FilterPy library. http://github.com/rlabbe/filterpy</p> <p>Documentation at: https://filterpy.readthedocs.org</p> <p>Supporting book at: https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> <p>This is licensed under an MIT license. See the readme.MD file for more information.</p>"},{"location":"api/discrete-bayes/#bayesian_filters.discrete_bayes.__all__","title":"<code>__all__ = ['discrete_bayes']</code>  <code>module-attribute</code>","text":""},{"location":"api/discrete-bayes/#bayesian_filters.discrete_bayes.normalize","title":"<code>normalize(pdf)</code>","text":"<p>Normalize distribution <code>pdf</code> in-place so it sums to 1.0.</p> <p>Returns pdf for convienence, so you can write things like:</p> <p>kernel = normalize(randn(7))</p> <p>Parameters:</p> Name Type Description Default <code>pdf</code> <code>ndarray</code> <p>discrete distribution that needs to be converted to a pdf. Converted in-place, i.e., this is modified.</p> required <p>Returns:</p> Name Type Description <code>pdf</code> <code>ndarray</code> <p>The converted pdf.</p>"},{"location":"api/discrete-bayes/#bayesian_filters.discrete_bayes.predict","title":"<code>predict(pdf, offset, kernel, mode='wrap', cval=0.0)</code>","text":"<p>Performs the discrete Bayes filter prediction step, generating the prior.</p> <p><code>pdf</code> is a discrete probability distribution expressing our initial belief.</p> <p><code>offset</code> is an integer specifying how much we want to move to the right (negative values means move to the left)</p> <p>We assume there is some noise in that offset, which we express in <code>kernel</code>. For example, if offset=3 and kernel=[.1, .7., .2], that means we think there is a 70% chance of moving right by 3, a 10% chance of moving 2 spaces, and a 20% chance of moving by 4.</p> <p>It returns the resulting distribution.</p> <p>If <code>mode='wrap'</code>, then the probability distribution is wrapped around the array.</p> <p>If <code>mode='constant'</code>, or any other value the pdf is shifted, with <code>cval</code> used to fill in missing elements.</p> <p>Examples:</p> <p>.. code-block:: Python</p> <pre><code>belief = [.05, .05, .05, .05, .55, .05, .05, .05, .05, .05]\nprior = predict(belief, offset=2, kernel=[.1, .8, .1])\n</code></pre>"},{"location":"api/discrete-bayes/#bayesian_filters.discrete_bayes.update","title":"<code>update(likelihood, prior)</code>","text":"<p>Computes the posterior of a discrete random variable given a discrete likelihood and prior. In a typical application the likelihood will be the likelihood of a measurement matching your current environment, and the prior comes from discrete_bayes.predict().</p> <p>Parameters:</p> Name Type Description Default <code>likelihood</code> <code>ndarray, dtype=flaot</code> <p>array of likelihood values</p> required <code>prior</code> <code>ndarray, dtype=flaot</code> <p>prior pdf.</p> required <p>Returns:</p> Name Type Description <code>posterior</code> <code>ndarray, dtype=float</code> <p>Returns array representing the posterior.</p> <p>Examples:</p> <p>.. code-block:: Python</p> <pre><code># self driving car. Sensor returns values that can be equated to positions\n# on the road. A real likelihood compuation would be much more complicated\n# than this example.\n\nlikelihood = np.ones(len(road))\nlikelihood[road==z] *= scale_factor\n\nprior = predict(posterior, velocity, kernel)\nposterior = update(likelihood, prior)\n</code></pre>"},{"location":"api/gh/","title":"GH Filter Module","text":"<p>The gh module contains g-h filters and related functionality.</p>"},{"location":"api/gh/#ghfilter","title":"GHFilter","text":""},{"location":"api/gh/#bayesian_filters.gh.gh_filter.GHFilter","title":"<code>GHFilter</code>","text":"<p>               Bases: <code>object</code></p> <p>Implements the g-h filter. The topic is too large to cover in this comment. See my book \"Kalman and Bayesian Filters in Python\" [1] or Eli Brookner's \"Tracking and Kalman Filters Made Easy\" [2].</p> <p>A few basic examples are below, and the tests in ./gh_tests.py may give you more ideas on use.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>1D np.array or scalar</code> <p>Initial value for the filter state. Each value can be a scalar or a np.array.</p> <p>You can use a scalar for x0. If order &gt; 0, then 0.0 is assumed for the higher order terms.</p> <p>x[0] is the value being tracked x[1] is the first derivative (for order 1 and 2 filters) x[2] is the second derivative (for order 2 filters)</p> required <code>dx</code> <code>1D np.array or scalar</code> <p>Initial value for the derivative of the filter state.</p> required <code>dt</code> <code>scalar</code> <p>time step</p> required <code>g</code> <code>float</code> <p>filter g gain parameter.</p> required <code>h</code> <code>float</code> <p>filter h gain parameter.</p> required <p>Attributes:</p> Name Type Description <code>x</code> <code>1D np.array or scalar</code> <p>filter state</p> <code>dx</code> <code>1D np.array or scalar</code> <p>derivative of the filter state.</p> <code>x_prediction</code> <code>1D np.array or scalar</code> <p>predicted filter state</p> <code>dx_prediction</code> <code>1D np.array or scalar</code> <p>predicted derivative of the filter state.</p> <code>dt</code> <code>scalar</code> <p>time step</p> <code>g</code> <code>float</code> <p>filter g gain parameter.</p> <code>h</code> <code>float</code> <p>filter h gain parameter.</p> <code>y</code> <code>np.array, or scalar</code> <p>residual (difference between measurement and prior)</p> <code>z</code> <code>np.array, or scalar</code> <p>measurement passed into update()</p> <p>Examples:</p> <p>Create a basic filter for a scalar value with g=.8, h=.2. Initialize to 0, with a derivative(velocity) of 0.</p> <pre><code>&gt;&gt;&gt; from bayesian_filters.gh import GHFilter\n&gt;&gt;&gt; f = GHFilter (x=0., dx=0., dt=1., g=.8, h=.2)\n</code></pre> <p>Incorporate the measurement of 1</p> <pre><code>&gt;&gt;&gt; f.update(z=1)\n(0.8, 0.2)\n</code></pre> <p>Incorporate a measurement of 2 with g=1 and h=0.01</p> <pre><code>&gt;&gt;&gt; f.update(z=2, g=1, h=0.01)\n(2.0, 0.21000000000000002)\n</code></pre> <p>Create a filter with two independent variables.</p> <pre><code>&gt;&gt;&gt; from numpy import array\n&gt;&gt;&gt; f = GHFilter (x=array([0,1]), dx=array([0,0]), dt=1, g=.8, h=.02)\n</code></pre> <p>and update with the measurements (2,4)</p> <pre><code>&gt;&gt;&gt; f.update(array([2,4])\n(array([ 1.6,  3.4]), array([ 0.04,  0.06]))\n</code></pre> References <p>[1] Labbe, \"Kalman and Bayesian Filters in Python\" http://rlabbe.github.io/Kalman-and-Bayesian-Filters-in-Python</p> <p>[2] Brookner, \"Tracking and Kalman Filters Made Easy\". John Wiley and Sons, 1998.</p>"},{"location":"api/gh/#bayesian_filters.gh.gh_filter.GHFilter.VRF","title":"<code>VRF()</code>","text":"<p>Returns the Variance Reduction Factor (VRF) of the state variable of the filter (x) and its derivatives (dx, ddx). The VRF is the normalized variance for the filter, as given in the equations below.</p> <p>.. math::     VRF(\\hat{x}{n,n}) = \\frac{VAR(\\hat{x}})}{\\sigma^2_x</p> <pre><code>VRF(\\hat{\\dot{x}}_{n,n}) = \\\\frac{VAR(\\hat{\\dot{x}}_{n,n})}{\\sigma^2_x}\n\nVRF(\\hat{\\ddot{x}}_{n,n}) = \\\\frac{VAR(\\hat{\\ddot{x}}_{n,n})}{\\sigma^2_x}\n</code></pre> <p>Returns:</p> Type Description <code>vrf_x   VRF of x state variable</code> <code>vrf_dx  VRF of the dx state variable (derivative of x)</code>"},{"location":"api/gh/#bayesian_filters.gh.gh_filter.GHFilter.VRF_prediction","title":"<code>VRF_prediction()</code>","text":"<p>Returns the Variance Reduction Factor of the prediction step of the filter. The VRF is the normalized variance for the filter, as given in the equation below.</p> <p>.. math::     VRF(\\hat{x}{n+1,n}) = \\frac{VAR(\\hat{x}})}{\\sigma^2_x</p> References <p>Asquith, \"Weight Selection in First Order Linear Filters\" Report No RG-TR-69-12, U.S. Army Missle Command. Redstone Arsenal, Al. November 24, 1970.</p>"},{"location":"api/gh/#bayesian_filters.gh.gh_filter.GHFilter.batch_filter","title":"<code>batch_filter(data, save_predictions=False, saver=None)</code>","text":"<p>Given a sequenced list of data, performs g-h filter with a fixed g and h. See update() if you need to vary g and/or h.</p> <p>Uses self.x and self.dx to initialize the filter, but DOES NOT alter self.x and self.dx during execution, allowing you to use this class multiple times without reseting self.x and self.dx. I'm not sure how often you would need to do that, but the capability is there. More exactly, none of the class member variables are modified by this function, in distinct contrast to update(), which changes most of them.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list like</code> <p>contains the data to be filtered.</p> required <code>save_predictions</code> <code>boolean</code> <p>the predictions will be saved and returned if this is true</p> <code>False</code> <code>saver</code> <code>Saver</code> <p>filterpy.common.Saver object. If provided, saver.save() will be called after every epoch</p> <code>None</code> <p>Returns:</p> Name Type Description <code>results</code> <code>np.array shape (n+1, 2), where n=len(data)</code> <p>contains the results of the filter, where results[i,0] is x , and results[i,1] is dx (derivative of x) First entry is the initial values of x and dx as set by init.</p> <code>predictions</code> <code>np.array shape(n), optional</code> <p>the predictions for each step in the filter. Only retured if save_predictions == True</p>"},{"location":"api/gh/#bayesian_filters.gh.gh_filter.GHFilter.update","title":"<code>update(z, g=None, h=None)</code>","text":"<p>performs the g-h filter predict and update step on the measurement z. Modifies the member variables listed below, and returns the state of x and dx as a tuple as a convienence.</p> <p>Modified Members</p> <p>x     filtered state variable</p> <p>dx     derivative (velocity) of x</p> <p>residual     difference between the measurement and the prediction for x</p> <p>x_prediction     predicted value of x before incorporating the measurement z.</p> <p>dx_prediction     predicted value of the derivative of x before incorporating the     measurement z.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>any</code> <p>the measurement</p> required <code>g</code> <code>scalar(optional)</code> <p>Override the fixed self.g value for this update</p> <code>None</code> <code>h</code> <code>scalar(optional)</code> <p>Override the fixed self.h value for this update</p> <code>None</code> <p>Returns:</p> Type Description <code>x filter output for x</code> <code>dx filter output for dx (derivative of x</code>"},{"location":"api/gh/#ghkfilter","title":"GHKFilter","text":""},{"location":"api/gh/#bayesian_filters.gh.gh_filter.GHKFilter","title":"<code>GHKFilter</code>","text":"<p>               Bases: <code>object</code></p> <p>Implements the g-h-k filter.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>1D np.array or scalar</code> <p>Initial value for the filter state. Each value can be a scalar or a np.array.</p> <p>You can use a scalar for x0. If order &gt; 0, then 0.0 is assumed for the higher order terms.</p> <p>x[0] is the value being tracked x[1] is the first derivative (for order 1 and 2 filters) x[2] is the second derivative (for order 2 filters)</p> required <code>dx</code> <code>1D np.array or scalar</code> <p>Initial value for the derivative of the filter state.</p> required <code>ddx</code> <code>1D np.array or scalar</code> <p>Initial value for the second derivative of the filter state.</p> required <code>dt</code> <code>scalar</code> <p>time step</p> required <code>g</code> <code>float</code> <p>filter g gain parameter.</p> required <code>h</code> <code>float</code> <p>filter h gain parameter.</p> required <code>k</code> <code>float</code> <p>filter k gain parameter.</p> required <p>Attributes:</p> Name Type Description <code>x</code> <code>1D np.array or scalar</code> <p>filter state</p> <code>dx</code> <code>1D np.array or scalar</code> <p>derivative of the filter state.</p> <code>ddx</code> <code>1D np.array or scalar</code> <p>second derivative of the filter state.</p> <code>x_prediction</code> <code>1D np.array or scalar</code> <p>predicted filter state</p> <code>dx_prediction</code> <code>1D np.array or scalar</code> <p>predicted derivative of the filter state.</p> <code>ddx_prediction</code> <code>1D np.array or scalar</code> <p>second predicted derivative of the filter state.</p> <code>dt</code> <code>scalar</code> <p>time step</p> <code>g</code> <code>float</code> <p>filter g gain parameter.</p> <code>h</code> <code>float</code> <p>filter h gain parameter.</p> <code>k</code> <code>float</code> <p>filter k gain parameter.</p> <code>y</code> <code>np.array, or scalar</code> <p>residual (difference between measurement and prior)</p> <code>z</code> <code>np.array, or scalar</code> <p>measurement passed into update()</p> References <p>Brookner, \"Tracking and Kalman Filters Made Easy\". John Wiley and Sons, 1998.</p>"},{"location":"api/gh/#bayesian_filters.gh.gh_filter.GHKFilter.VRF","title":"<code>VRF()</code>","text":"<p>Returns the Variance Reduction Factor (VRF) of the state variable of the filter (x) and its derivatives (dx, ddx). The VRF is the normalized variance for the filter, as given in the equations below.</p> <p>.. math::     VRF(\\hat{x}{n,n}) = \\frac{VAR(\\hat{x}})}{\\sigma^2_x</p> <pre><code>VRF(\\hat{\\dot{x}}_{n,n}) = \\\\frac{VAR(\\hat{\\dot{x}}_{n,n})}{\\sigma^2_x}\n\nVRF(\\hat{\\ddot{x}}_{n,n}) = \\\\frac{VAR(\\hat{\\ddot{x}}_{n,n})}{\\sigma^2_x}\n</code></pre> <p>Returns:</p> Name Type Description <code>vrf_x</code> <code>type(x)</code> <p>VRF of x state variable</p> <code>vrf_dx</code> <code>type(x)</code> <p>VRF of the dx state variable (derivative of x)</p> <code>vrf_ddx</code> <code>type(x)</code> <p>VRF of the ddx state variable (second derivative of x)</p>"},{"location":"api/gh/#bayesian_filters.gh.gh_filter.GHKFilter.VRF_prediction","title":"<code>VRF_prediction()</code>","text":"<p>Returns the Variance Reduction Factor for x of the prediction step of the filter.</p> <p>This implements the equation</p> <p>.. math::     VRF(\\hat{x}{n+1,n}) = \\frac{VAR(\\hat{x}})}{\\sigma^2_x</p> References <p>Asquith and Woods, \"Total Error Minimization in First and Second Order Prediction Filters\" Report No RE-TR-70-17, U.S. Army Missle Command. Redstone Arsenal, Al. November 24, 1970.</p>"},{"location":"api/gh/#bayesian_filters.gh.gh_filter.GHKFilter.batch_filter","title":"<code>batch_filter(data, save_predictions=False)</code>","text":"<p>Performs g-h filter with a fixed g and h.</p> <p>Uses self.x and self.dx to initialize the filter, but DOES NOT alter self.x and self.dx during execution, allowing you to use this class multiple times without reseting self.x and self.dx. I'm not sure how often you would need to do that, but the capability is there. More exactly, none of the class member variables are modified by this function.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list_like</code> <p>contains the data to be filtered.</p> required <code>save_predictions</code> <code>boolean</code> <p>The predictions will be saved and returned if this is true</p> <code>False</code> <p>Returns:</p> Name Type Description <code>results</code> <code>np.array shape (n+1, 2), where n=len(data)</code> <p>contains the results of the filter, where results[i,0] is x , and results[i,1] is dx (derivative of x) First entry is the initial values of x and dx as set by init.</p> <code>predictions</code> <code>np.array shape(n), or None</code> <p>the predictions for each step in the filter. Only returned if save_predictions == True</p>"},{"location":"api/gh/#bayesian_filters.gh.gh_filter.GHKFilter.bias_error","title":"<code>bias_error(dddx)</code>","text":"<p>Returns the bias error given the specified constant jerk(dddx)</p> <p>Parameters:</p> Name Type Description Default <code>dddx</code> <code>type(x)</code> <p>3rd derivative (jerk) of the state variable x.</p> required References <p>Asquith and Woods, \"Total Error Minimization in First and Second Order Prediction Filters\" Report No RE-TR-70-17, U.S. Army Missle Command. Redstone Arsenal, Al. November 24, 1970.</p>"},{"location":"api/gh/#bayesian_filters.gh.gh_filter.GHKFilter.update","title":"<code>update(z, g=None, h=None, k=None)</code>","text":"<p>Performs the g-h filter predict and update step on the measurement z.</p> <p>On return, self.x, self.dx, self.y, and self.x_prediction will have been updated with the results of the computation. For convienence, self.x and self.dx are returned in a tuple.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>scalar</code> <p>the measurement</p> required <code>g</code> <code>scalar(optional)</code> <p>Override the fixed self.g value for this update</p> <code>None</code> <code>h</code> <code>scalar(optional)</code> <p>Override the fixed self.h value for this update</p> <code>None</code> <code>k</code> <code>scalar(optional)</code> <p>Override the fixed self.k value for this update</p> <code>None</code> <p>Returns:</p> Type Description <code>x filter output for x</code> <code>dx filter output for dx (derivative of x</code>"},{"location":"api/kalman/","title":"Kalman Filter Module","text":"<p>The kalman module contains various Kalman filter implementations.</p>"},{"location":"api/kalman/#kalmanfilter","title":"KalmanFilter","text":""},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter","title":"<code>KalmanFilter</code>","text":"<p>               Bases: <code>object</code></p> <p>Implements a Kalman filter. You are responsible for setting the various state variables to reasonable values; the defaults  will not give you a functional filter.</p> <p>For now the best documentation is my free book Kalman and Bayesian Filters in Python [2]_. The test files in this directory also give you a basic idea of use, albeit without much description.</p> <p>In brief, you will first construct this object, specifying the size of the state vector with dim_x and the size of the measurement vector that you will be using with dim_z. These are mostly used to perform size checks when you assign values to the various matrices. For example, if you specified dim_z=2 and then try to assign a 3x3 matrix to R (the measurement noise matrix you will get an assert exception because R should be 2x2. (If for whatever reason you need to alter the size of things midstream just use the underscore version of the matrices to assign directly: your_filter._R = a_3x3_matrix.)</p> <p>After construction the filter will have default matrices created for you, but you must specify the values for each. It\u2019s usually easiest to just overwrite them rather than assign to each element yourself. This will be clearer in the example below. All are of type numpy.array.</p> <p>Examples:</p> <p>Here is a filter that tracks position and velocity using a sensor that only reads position.</p> <p>First construct the object with the required dimensionality. Here the state (<code>dim_x</code>) has 2 coefficients (position and velocity), and the measurement (<code>dim_z</code>) has one. In FilterPy <code>x</code> is the state, <code>z</code> is the measurement.</p> <p>.. code::</p> <pre><code>from bayesian_filters.kalman import KalmanFilter\nf = KalmanFilter (dim_x=2, dim_z=1)\n</code></pre> <p>Assign the initial value for the state (position and velocity). You can do this with a two dimensional array like so:</p> <pre><code>.. code::\n\n    f.x = np.array([[2.],    # position\n                    [0.]])   # velocity\n</code></pre> <p>or just use a one dimensional array, which I prefer doing.</p> <p>.. code::</p> <pre><code>f.x = np.array([2., 0.])\n</code></pre> <p>Define the state transition matrix:</p> <pre><code>.. code::\n\n    f.F = np.array([[1.,1.],\n                    [0.,1.]])\n</code></pre> <p>Define the measurement function. Here we need to convert a position-velocity vector into just a position vector, so we use:</p> <pre><code>.. code::\n\nf.H = np.array([[1., 0.]])\n</code></pre> <p>Define the state's covariance matrix P.</p> <p>.. code::</p> <pre><code>f.P = np.array([[1000.,    0.],\n                [   0., 1000.] ])\n</code></pre> <p>Now assign the measurement noise. Here the dimension is 1x1, so I can use a scalar</p> <p>.. code::</p> <pre><code>f.R = 5\n</code></pre> <p>I could have done this instead:</p> <p>.. code::</p> <pre><code>f.R = np.array([[5.]])\n</code></pre> <p>Note that this must be a 2 dimensional array.</p> <p>Finally, I will assign the process noise. Here I will take advantage of another FilterPy library function:</p> <p>.. code::</p> <pre><code>from bayesian_filters.common import Q_discrete_white_noise\nf.Q = Q_discrete_white_noise(dim=2, dt=0.1, var=0.13)\n</code></pre> <p>Now just perform the standard predict/update loop:</p> <p>.. code::</p> <pre><code>while some_condition_is_true:\n    z = get_sensor_reading()\n    f.predict()\n    f.update(z)\n\n    do_something_with_estimate (f.x)\n</code></pre> <p>Procedural Form</p> <p>This module also contains stand alone functions to perform Kalman filtering. Use these if you are not a fan of objects.</p> <p>Example</p> <p>.. code::</p> <pre><code>while True:\n    z, R = read_sensor()\n    x, P = predict(x, P, F, Q)\n    x, P = update(x, P, z, R, H)\n</code></pre> <p>See my book Kalman and Bayesian Filters in Python [2]_.</p> <p>You will have to set the following attributes after constructing this object for the filter to perform properly. Please note that there are various checks in place to ensure that you have made everything the 'correct' size. However, it is possible to provide incorrectly sized arrays such that the linear algebra can not perform an operation. It can also fail silently - you can end up with matrices of a size that allows the linear algebra to work, but are the wrong shape for the problem you are trying to solve.</p> <p>Parameters:</p> Name Type Description Default <code>dim_x</code> <code>int</code> <p>Number of state variables for the Kalman filter. For example, if you are tracking the position and velocity of an object in two dimensions, dim_x would be 4. This is used to set the default size of P, Q, and u</p> required <code>dim_z</code> <code>int</code> <p>Number of of measurement inputs. For example, if the sensor provides you with position in (x,y), dim_z would be 2.</p> required <code>dim_u</code> <code>int(optional)</code> <p>size of the control input, if it is being used. Default value of 0 indicates it is not used.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>x</code> <code>array(dim_x, 1)</code> <p>Current state estimate. Any call to update() or predict() updates this variable.</p> <code>P</code> <code>array(dim_x, dim_x)</code> <p>Current state covariance matrix. Any call to update() or predict() updates this variable.</p> <code>x_prior</code> <code>array(dim_x, 1)</code> <p>Prior (predicted) state estimate. The _prior and _post attributes are for convenience; they store the  prior and posterior of the current epoch. Read Only.</p> <code>P_prior</code> <code>array(dim_x, dim_x)</code> <p>Prior (predicted) state covariance matrix. Read Only.</p> <code>x_post</code> <code>array(dim_x, 1)</code> <p>Posterior (updated) state estimate. Read Only.</p> <code>P_post</code> <code>array(dim_x, dim_x)</code> <p>Posterior (updated) state covariance matrix. Read Only.</p> <code>z</code> <code>array</code> <p>Last measurement used in update(). Read only.</p> <code>R</code> <code>array(dim_z, dim_z)</code> <p>Measurement noise covariance matrix. Also known as the observation covariance.</p> <code>Q</code> <code>array(dim_x, dim_x)</code> <p>Process noise covariance matrix. Also known as the transition covariance.</p> <code>F</code> <code>array()</code> <p>State Transition matrix. Also known as <code>A</code> in some formulation.</p> <code>H</code> <code>array(dim_z, dim_x)</code> <p>Measurement function. Also known as the observation matrix, or as <code>C</code>.</p> <code>y</code> <code>array</code> <p>Residual of the update step. Read only.</p> <code>K</code> <code>array(dim_x, dim_z)</code> <p>Kalman gain of the update step. Read only.</p> <code>S</code> <code>array</code> <p>System uncertainty (P projected to measurement space). Read only.</p> <code>SI</code> <code>array</code> <p>Inverse system uncertainty. Read only.</p> <code>log_likelihood</code> <code>float</code> <p>log-likelihood of the last measurement. Read only.</p> <code>likelihood</code> <code>float</code> <p>likelihood of last measurement. Read only.</p> <p>Computed from the log-likelihood. The log-likelihood can be very small,  meaning a large negative value such as -28000. Taking the exp() of that results in 0.0, which can break typical algorithms which multiply by this value, so by default we always return a number &gt;= sys.float_info.min.</p> <code>mahalanobis</code> <code>float</code> <p>mahalanobis distance of the innovation. Read only.</p> <code>inv</code> <code>function, default numpy.linalg.inv</code> <p>If you prefer another inverse function, such as the Moore-Penrose pseudo inverse, set it to that instead: kf.inv = np.linalg.pinv</p> <p>This is only used to invert self.S. If you know it is diagonal, you might choose to set it to filterpy.common.inv_diagonal, which is several times faster than numpy.linalg.inv for diagonal matrices.</p> <code>alpha</code> <code>float</code> <p>Fading memory setting. 1.0 gives the normal Kalman filter, and values slightly larger than 1.0 (such as 1.02) give a fading memory effect - previous measurements have less influence on the filter's estimates. This formulation of the Fading memory filter (there are many) is due to Dan Simon [1]_.</p> References <p>.. [1] Dan Simon. \"Optimal State Estimation.\" John Wiley &amp; Sons.    p. 208-212. (2006)</p> <p>.. [2] Roger Labbe. \"Kalman and Bayesian Filters in Python\"    https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p>"},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter.alpha","title":"<code>alpha</code>  <code>property</code> <code>writable</code>","text":"<p>Fading memory setting. 1.0 gives the normal Kalman filter, and values slightly larger than 1.0 (such as 1.02) give a fading memory effect - previous measurements have less influence on the filter's estimates. This formulation of the Fading memory filter (there are many) is due to Dan Simon [1]_.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter.likelihood","title":"<code>likelihood</code>  <code>property</code>","text":"<p>Computed from the log-likelihood. The log-likelihood can be very small,  meaning a large negative value such as -28000. Taking the exp() of that results in 0.0, which can break typical algorithms which multiply by this value, so by default we always return a number &gt;= sys.float_info.min.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter.log_likelihood","title":"<code>log_likelihood</code>  <code>property</code>","text":"<p>log-likelihood of the last measurement.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter.mahalanobis","title":"<code>mahalanobis</code>  <code>property</code>","text":"<p>\" Mahalanobis distance of measurement. E.g. 3 means measurement was 3 standard deviations away from the predicted value.</p> <p>Returns:</p> Name Type Description <code>mahalanobis</code> <code>float</code>"},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter.batch_filter","title":"<code>batch_filter(zs, Fs=None, Qs=None, Hs=None, Rs=None, Bs=None, us=None, update_first=False, saver=None)</code>","text":"<p>Batch processes a sequences of measurements.</p> Parameters <p>zs : list-like      list of measurements at each time step <code>self.dt</code>. Missing      measurements must be represented by <code>None</code>.</p> <p>Fs : None, list-like, default=None      optional value or list of values to use for the state transition      matrix F.</p> <pre><code> If Fs is None then self.F is used for all epochs.\n\n Otherwise it must contain a list-like list of F's, one for\n each epoch.  This allows you to have varying F per epoch.\n</code></pre> <p>Qs : None, np.array or list-like, default=None      optional value or list of values to use for the process error      covariance Q.</p> <pre><code> If Qs is None then self.Q is used for all epochs.\n\n Otherwise it must contain a list-like list of Q's, one for\n each epoch.  This allows you to have varying Q per epoch.\n</code></pre> <p>Hs : None, np.array or list-like, default=None      optional list of values to use for the measurement matrix H.</p> <pre><code> If Hs is None then self.H is used for all epochs.\n\n If Hs contains a single matrix, then it is used as H for all\n epochs.\n\n Otherwise it must contain a list-like list of H's, one for\n each epoch.  This allows you to have varying H per epoch.\n</code></pre> <p>Rs : None, np.array or list-like, default=None      optional list of values to use for the measurement error      covariance R.</p> <pre><code> If Rs is None then self.R is used for all epochs.\n\n Otherwise it must contain a list-like list of R's, one for\n each epoch.  This allows you to have varying R per epoch.\n</code></pre> <p>Bs : None, np.array or list-like, default=None      optional list of values to use for the control transition matrix B.</p> <pre><code> If Bs is None then self.B is used for all epochs.\n\n Otherwise it must contain a list-like list of B's, one for\n each epoch.  This allows you to have varying B per epoch.\n</code></pre> <p>us : None, np.array or list-like, default=None      optional list of values to use for the control input vector;</p> <pre><code> If us is None then None is used for all epochs (equivalent to 0,\n or no control input).\n\n Otherwise it must contain a list-like list of u's, one for\n each epoch.\n</code></pre> <p>update_first : bool, optional, default=False      controls whether the order of operations is update followed by      predict, or predict followed by update. Default is predict-&gt;update.</p> <p>saver : filterpy.common.Saver, optional      filterpy.common.Saver object. If provided, saver.save() will be      called after every epoch</p> Returns <p>means : np.array((n,dim_x,1))      array of the state for each time step after the update. Each entry      is an np.array. In other words <code>means[k,:]</code> is the state at step      <code>k</code>.</p> <p>covariance : np.array((n,dim_x,dim_x))      array of the covariances for each time step after the update.      In other words <code>covariance[k,:,:]</code> is the covariance at step <code>k</code>.</p> <p>means_predictions : np.array((n,dim_x,1))      array of the state for each time step after the predictions. Each      entry is an np.array. In other words <code>means[k,:]</code> is the state at      step <code>k</code>.</p> <p>covariance_predictions : np.array((n,dim_x,dim_x))      array of the covariances for each time step after the prediction.      In other words <code>covariance[k,:,:]</code> is the covariance at step <code>k</code>.</p> Examples <p>.. code-block:: Python</p> <pre><code> # this example demonstrates tracking a measurement where the time\n # between measurement varies, as stored in dts. This requires\n # that F be recomputed for each epoch. The output is then smoothed\n # with an RTS smoother.\n\n zs = [t + random.randn()*4 for t in range (40)]\n Fs = [np.array([[1., dt], [0, 1]] for dt in dts]\n\n (mu, cov, _, _) = kf.batch_filter(zs, Fs=Fs)\n (xs, Ps, Ks, Pps) = kf.rts_smoother(mu, cov, Fs=Fs)\n</code></pre>"},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter.get_prediction","title":"<code>get_prediction(u=None, B=None, F=None, Q=None)</code>","text":"<p>Predict next state (prior) using the Kalman filter state propagation equations and returns it without modifying the object.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>array</code> <p>Optional control vector.</p> <code>0</code> <code>B</code> <code>np.array(dim_x, dim_u), or None</code> <p>Optional control transition matrix; a value of None will cause the filter to use <code>self.B</code>.</p> <code>None</code> <code>F</code> <code>np.array(dim_x, dim_x), or None</code> <p>Optional state transition matrix; a value of None will cause the filter to use <code>self.F</code>.</p> <code>None</code> <code>Q</code> <code>np.array(dim_x, dim_x), scalar, or None</code> <p>Optional process noise matrix; a value of None will cause the filter to use <code>self.Q</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>(x, P) : tuple</code> <p>State vector and covariance array of the prediction.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter.get_update","title":"<code>get_update(z=None)</code>","text":"<p>Computes the new estimate based on measurement <code>z</code> and returns it without altering the state of the filter.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>(dim_z, 1): array_like</code> <p>measurement for this update. z can be a scalar if dim_z is 1, otherwise it must be convertible to a column vector.</p> <code>None</code> <p>Returns:</p> Type Description <code>(x, P) : tuple</code> <p>State vector and covariance array of the update.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter.log_likelihood_of","title":"<code>log_likelihood_of(z)</code>","text":"<p>log likelihood of the measurement <code>z</code>. This should only be called after a call to update(). Calling after predict() will yield an incorrect result.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter.measurement_of_state","title":"<code>measurement_of_state(x)</code>","text":"<p>Helper function that converts a state into a measurement.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>kalman state vector</p> required <p>Returns:</p> Name Type Description <code>z</code> <code>(dim_z, 1): array_like</code> <p>measurement for this update. z can be a scalar if dim_z is 1, otherwise it must be convertible to a column vector.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter.predict","title":"<code>predict(u=None, B=None, F=None, Q=None)</code>","text":"<p>Predict next state (prior) using the Kalman filter state propagation equations.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>array</code> <p>Optional control vector.</p> <code>0</code> <code>B</code> <code>np.array(dim_x, dim_u), or None</code> <p>Optional control transition matrix; a value of None will cause the filter to use <code>self.B</code>.</p> <code>None</code> <code>F</code> <code>np.array(dim_x, dim_x), or None</code> <p>Optional state transition matrix; a value of None will cause the filter to use <code>self.F</code>.</p> <code>None</code> <code>Q</code> <code>np.array(dim_x, dim_x), scalar, or None</code> <p>Optional process noise matrix; a value of None will cause the filter to use <code>self.Q</code>.</p> <code>None</code>"},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter.predict_steadystate","title":"<code>predict_steadystate(u=0, B=None)</code>","text":"<p>Predict state (prior) using the Kalman filter state propagation equations. Only x is updated, P is left unchanged. See update_steadstate() for a longer explanation of when to use this method.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>array</code> <p>Optional control vector. If non-zero, it is multiplied by B to create the control input into the system.</p> <code>0</code> <code>B</code> <code>np.array(dim_x, dim_u), or None</code> <p>Optional control transition matrix; a value of None will cause the filter to use <code>self.B</code>.</p> <code>None</code>"},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter.residual_of","title":"<code>residual_of(z)</code>","text":"<p>Returns the residual for the given measurement (z). Does not alter the state of the filter.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter.rts_smoother","title":"<code>rts_smoother(Xs, Ps, Fs=None, Qs=None, inv=np.linalg.inv)</code>","text":"<p>Runs the Rauch-Tung-Striebel Kalman smoother on a set of means and covariances computed by a Kalman filter. The usual input would come from the output of <code>KalmanFilter.batch_filter()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>Xs</code> <code>array</code> <p>array of the means (state variable x) of the output of a Kalman filter.</p> required <code>Ps</code> <code>array</code> <p>array of the covariances of the output of a kalman filter.</p> required <code>Fs</code> <code>list-like collection of numpy.array</code> <p>State transition matrix of the Kalman filter at each time step. Optional, if not provided the filter's self.F will be used</p> <code>None</code> <code>Qs</code> <code>list-like collection of numpy.array</code> <p>Process noise of the Kalman filter at each time step. Optional, if not provided the filter's self.Q will be used</p> <code>None</code> <code>inv</code> <code>function</code> <p>If you prefer another inverse function, such as the Moore-Penrose pseudo inverse, set it to that instead: kf.inv = np.linalg.pinv</p> <code>numpy.linalg.inv</code> <p>Returns:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>smoothed means</p> <code>P</code> <code>ndarray</code> <p>smoothed state covariances</p> <code>K</code> <code>ndarray</code> <p>smoother gain at each step</p> <code>Pp</code> <code>ndarray</code> <p>Predicted state covariances</p> <p>Examples:</p> <p>.. code-block:: Python</p> <pre><code>zs = [t + random.randn()*4 for t in range (40)]\n\n(mu, cov, _, _) = kalman.batch_filter(zs)\n(x, P, K, Pp) = rts_smoother(mu, cov, kf.F, kf.Q)\n</code></pre>"},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter.test_matrix_dimensions","title":"<code>test_matrix_dimensions(z=None, H=None, R=None, F=None, Q=None)</code>","text":"<p>Performs a series of asserts to check that the size of everything is what it should be. This can help you debug problems in your design.</p> <p>If you pass in H, R, F, Q those will be used instead of this object's value for those matrices.</p> <p>Testing <code>z</code> (the measurement) is problamatic. x is a vector, and can be implemented as either a 1D array or as a nx1 column vector. Thus Hx can be of different shapes. Then, if Hx is a single value, it can be either a 1D array or 2D vector. If either is true, z can reasonably be a scalar (either '3' or np.array('3') are scalars under this definition), a 1D, 1 element array, or a 2D, 1 element array. You are allowed to pass in any combination that works.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter.update","title":"<code>update(z, R=None, H=None)</code>","text":"<p>Add a new measurement (z) to the Kalman filter.</p> <p>If z is None, nothing is computed. However, x_post and P_post are updated with the prior (x_prior, P_prior), and self.z is set to None.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>(dim_z, 1): array_like</code> <p>measurement for this update. z can be a scalar if dim_z is 1, otherwise it must be convertible to a column vector.</p> <p>If you pass in a value of H, z must be a column vector the of the correct size.</p> required <code>R</code> <code>np.array, scalar, or None</code> <p>Optionally provide R to override the measurement noise for this one call, otherwise  self.R will be used.</p> <code>None</code> <code>H</code> <code>np.array, or None</code> <p>Optionally provide H to override the measurement function for this one call, otherwise self.H will be used.</p> <code>None</code>"},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter.update_correlated","title":"<code>update_correlated(z, R=None, H=None)</code>","text":"<p>Add a new measurement (z) to the Kalman filter assuming that process noise and measurement noise are correlated as defined in the <code>self.M</code> matrix.</p> <p>A partial derivation can be found in [1]</p> <p>If z is None, nothing is changed.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>(dim_z, 1): array_like</code> <p>measurement for this update. z can be a scalar if dim_z is 1, otherwise it must be convertible to a column vector.</p> required <code>R</code> <code>np.array, scalar, or None</code> <p>Optionally provide R to override the measurement noise for this one call, otherwise  self.R will be used.</p> <code>None</code> <code>H</code> <code>np.array,  or None</code> <p>Optionally provide H to override the measurement function for this one call, otherwise  self.H will be used.</p> <code>None</code> References <p>.. [1] Bulut, Y. (2011). Applied Kalman filter theory (Doctoral dissertation, Northeastern University).        http://people.duke.edu/~hpgavin/SystemID/References/Balut-KalmanFilter-PhD-NEU-2011.pdf</p>"},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter.update_sequential","title":"<code>update_sequential(start, z_i, R_i=None, H_i=None)</code>","text":"<p>Add a single input measurement (z_i) to the Kalman filter. In sequential processing, inputs are processed one at a time.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>integer</code> <p>Index of the first measurement input updated by this call.</p> required <code>z_i</code> <code>array or scalar</code> <p>Measurement of inputs for this partial update.</p> required <code>R_i</code> <code>np.array, scalar, or None</code> <p>Optionally provide R_i to override the measurement noise of inputs for this one call, otherwise a slice of self.R will be used.</p> <code>None</code> <code>H_i</code> <code>np.array, or None</code> <p>Optionally provide H[i] to override the partial measurement function for this one call, otherwise a slice of self.H will be used.</p> <code>None</code>"},{"location":"api/kalman/#bayesian_filters.kalman.KalmanFilter.update_steadystate","title":"<code>update_steadystate(z)</code>","text":"<p>Add a new measurement (z) to the Kalman filter without recomputing the Kalman gain K, the state covariance P, or the system uncertainty S.</p> <p>You can use this for LTI systems since the Kalman gain and covariance converge to a fixed value. Precompute these and assign them explicitly, or run the Kalman filter using the normal predict()/update(0 cycle until they converge.</p> <p>The main advantage of this call is speed. We do significantly less computation, notably avoiding a costly matrix inversion.</p> <p>Use in conjunction with predict_steadystate(), otherwise P will grow without bound.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>(dim_z, 1): array_like</code> <p>measurement for this update. z can be a scalar if dim_z is 1, otherwise it must be convertible to a column vector.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cv = kinematic_kf(dim=3, order=2) # 3D const velocity filter\n&gt;&gt;&gt; # let filter converge on representative data, then save k and P\n&gt;&gt;&gt; for i in range(100):\n&gt;&gt;&gt;     cv.predict()\n&gt;&gt;&gt;     cv.update([i, i, i])\n&gt;&gt;&gt; saved_k = np.copy(cv.K)\n&gt;&gt;&gt; saved_P = np.copy(cv.P)\n</code></pre> <p>later on:</p> <pre><code>&gt;&gt;&gt; cv = kinematic_kf(dim=3, order=2) # 3D const velocity filter\n&gt;&gt;&gt; cv.K = np.copy(saved_K)\n&gt;&gt;&gt; cv.P = np.copy(saved_P)\n&gt;&gt;&gt; for i in range(100):\n&gt;&gt;&gt;     cv.predict_steadystate()\n&gt;&gt;&gt;     cv.update_steadystate([i, i, i])\n</code></pre>"},{"location":"api/kalman/#extendedkalmanfilter","title":"ExtendedKalmanFilter","text":""},{"location":"api/kalman/#bayesian_filters.kalman.ExtendedKalmanFilter","title":"<code>ExtendedKalmanFilter</code>","text":"<p>               Bases: <code>object</code></p> <p>Implements an extended Kalman filter (EKF). You are responsible for setting the various state variables to reasonable values; the defaults will  not give you a functional filter.</p> <p>You will have to set the following attributes after constructing this object for the filter to perform properly. Please note that there are various checks in place to ensure that you have made everything the 'correct' size. However, it is possible to provide incorrectly sized arrays such that the linear algebra can not perform an operation. It can also fail silently - you can end up with matrices of a size that allows the linear algebra to work, but are the wrong shape for the problem you are trying to solve.</p> <p>Parameters:</p> Name Type Description Default <code>dim_x</code> <code>int</code> <p>Number of state variables for the Kalman filter. For example, if you are tracking the position and velocity of an object in two dimensions, dim_x would be 4.</p> <p>This is used to set the default size of P, Q, and u</p> required <code>dim_z</code> <code>int</code> <p>Number of measurement inputs. For example, if the sensor provides you with position in (x,y), dim_z would be 2.</p> required <p>Attributes:</p> Name Type Description <code>x</code> <code>array(dim_x, 1)</code> <p>State estimate vector</p> <code>P</code> <code>array(dim_x, dim_x)</code> <p>Covariance matrix</p> <code>x_prior</code> <code>array(dim_x, 1)</code> <p>Prior (predicted) state estimate. The _prior and _post attributes are for convienence; they store the  prior and posterior of the current epoch. Read Only.</p> <code>P_prior</code> <code>array(dim_x, dim_x)</code> <p>Prior (predicted) state covariance matrix. Read Only.</p> <code>x_post</code> <code>array(dim_x, 1)</code> <p>Posterior (updated) state estimate. Read Only.</p> <code>P_post</code> <code>array(dim_x, dim_x)</code> <p>Posterior (updated) state covariance matrix. Read Only.</p> <code>R</code> <code>array(dim_z, dim_z)</code> <p>Measurement noise matrix</p> <code>Q</code> <code>array(dim_x, dim_x)</code> <p>Process noise matrix</p> <code>F</code> <code>array()</code> <p>State Transition matrix</p> <code>H</code> <code>array(dim_x, dim_x)</code> <p>Measurement function</p> <code>y</code> <code>array</code> <p>Residual of the update step. Read only.</p> <code>K</code> <code>array(dim_x, dim_z)</code> <p>Kalman gain of the update step. Read only.</p> <code>S</code> <code>array</code> <p>Systen uncertaintly projected to measurement space. Read only.</p> <code>z</code> <code>ndarray</code> <p>Last measurement used in update(). Read only.</p> <code>log_likelihood</code> <code>float</code> <p>log-likelihood of the last measurement. Read only.</p> <code>likelihood</code> <code>float</code> <p>likelihood of last measurment. Read only.</p> <p>Computed from the log-likelihood. The log-likelihood can be very small,  meaning a large negative value such as -28000. Taking the exp() of that results in 0.0, which can break typical algorithms which multiply by this value, so by default we always return a number &gt;= sys.float_info.min.</p> <code>mahalanobis</code> <code>float</code> <p>mahalanobis distance of the innovation. E.g. 3 means measurement was 3 standard deviations away from the predicted value.</p> <p>Read only.</p> <p>Examples:</p> <p>See my book Kalman and Bayesian Filters in Python https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p>"},{"location":"api/kalman/#bayesian_filters.kalman.ExtendedKalmanFilter.likelihood","title":"<code>likelihood</code>  <code>property</code>","text":"<p>Computed from the log-likelihood. The log-likelihood can be very small,  meaning a large negative value such as -28000. Taking the exp() of that results in 0.0, which can break typical algorithms which multiply by this value, so by default we always return a number &gt;= sys.float_info.min.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.ExtendedKalmanFilter.log_likelihood","title":"<code>log_likelihood</code>  <code>property</code>","text":"<p>log-likelihood of the last measurement.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.ExtendedKalmanFilter.mahalanobis","title":"<code>mahalanobis</code>  <code>property</code>","text":"<p>Mahalanobis distance of innovation. E.g. 3 means measurement was 3 standard deviations away from the predicted value.</p> <p>Returns:</p> Name Type Description <code>mahalanobis</code> <code>float</code>"},{"location":"api/kalman/#bayesian_filters.kalman.ExtendedKalmanFilter.predict","title":"<code>predict(u=0)</code>","text":"<p>Predict next state (prior) using the Kalman filter state propagation equations.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>array</code> <p>Optional control vector. If non-zero, it is multiplied by B to create the control input into the system.</p> <code>0</code>"},{"location":"api/kalman/#bayesian_filters.kalman.ExtendedKalmanFilter.predict_update","title":"<code>predict_update(z, HJacobian, Hx, args=(), hx_args=(), u=0)</code>","text":"<p>Performs the predict/update innovation of the extended Kalman filter.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>array</code> <p>measurement for this step. If <code>None</code>, only predict step is perfomed.</p> required <code>HJacobian</code> <code>function</code> <p>function which computes the Jacobian of the H matrix (measurement function). Takes state variable (self.x) as input, along with the optional arguments in args, and returns H.</p> required <code>Hx</code> <code>function</code> <p>function which takes as input the state variable (self.x) along with the optional arguments in hx_args, and returns the measurement that would correspond to that state.</p> required <code>args</code> <code>tuple</code> <p>arguments to be passed into HJacobian after the required state variable.</p> <code>(,)</code> <code>hx_args</code> <code>tuple</code> <p>arguments to be passed into Hx after the required state variable.</p> <code>(,)</code> <code>u</code> <code>array or scalar</code> <p>optional control vector input to the filter.</p> <code>0</code>"},{"location":"api/kalman/#bayesian_filters.kalman.ExtendedKalmanFilter.predict_x","title":"<code>predict_x(u=0)</code>","text":"<p>Predicts the next state of X. If you need to compute the next state yourself, override this function. You would need to do this, for example, if the usual Taylor expansion to generate F is not providing accurate results for you.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.ExtendedKalmanFilter.update","title":"<code>update(z, HJacobian, Hx, R=None, args=(), hx_args=(), residual=np.subtract)</code>","text":"<p>Performs the update innovation of the extended Kalman filter.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>array</code> <p>measurement for this step. If <code>None</code>, posterior is not computed</p> required <code>HJacobian</code> <code>function</code> <p>function which computes the Jacobian of the H matrix (measurement function). Takes state variable (self.x) as input, returns H.</p> required <code>Hx</code> <code>function</code> <p>function which takes as input the state variable (self.x) along with the optional arguments in hx_args, and returns the measurement that would correspond to that state.</p> required <code>R</code> <code>np.array, scalar, or None</code> <p>Optionally provide R to override the measurement noise for this one call, otherwise  self.R will be used.</p> <code>None</code> <code>args</code> <code>tuple</code> <p>arguments to be passed into HJacobian after the required state variable. for robot localization you might need to pass in information about the map and time of day, so you might have <code>args=(map_data, time)</code>, where the signature of HCacobian will be <code>def HJacobian(x, map, t)</code></p> <code>(,)</code> <code>hx_args</code> <code>tuple</code> <p>arguments to be passed into Hx function after the required state variable.</p> <code>(,)</code> <code>residual</code> <code>function(z, z2)</code> <p>Optional function that computes the residual (difference) between the two measurement vectors. If you do not provide this, then the built in minus operator will be used. You will normally want to use the built in unless your residual computation is nonlinear (for example, if they are angles)</p> <code>subtract</code>"},{"location":"api/kalman/#unscentedkalmanfilter","title":"UnscentedKalmanFilter","text":""},{"location":"api/kalman/#bayesian_filters.kalman.UnscentedKalmanFilter","title":"<code>UnscentedKalmanFilter</code>","text":"<p>               Bases: <code>object</code></p> <p>Implements the Scaled Unscented Kalman filter (UKF) as defined by Simon Julier in [1], using the formulation provided by Wan and Merle in [2]. This filter scales the sigma points to avoid strong nonlinearities.</p> <p>Parameters:</p> Name Type Description Default <code>dim_x</code> <code>int</code> <p>Number of state variables for the filter. For example, if you are tracking the position and velocity of an object in two dimensions, dim_x would be 4.</p> required <code>dim_z</code> <code>int</code> <p>Number of of measurement inputs. For example, if the sensor provides you with position in (x,y), dim_z would be 2.</p> <p>This is for convience, so everything is sized correctly on creation. If you are using multiple sensors the size of <code>z</code> can change based on the sensor. Just provide the appropriate hx function</p> required <code>hx</code> <code>function(x, **hx_args)</code> <p>Measurement function. Converts state vector x into a measurement vector of shape (dim_z).</p> required <code>fx</code> <code>function(x, dt, **fx_args)</code> <p>function that returns the state x transformed by the state transition function. dt is the time step in seconds.</p> required <code>points</code> <code>class</code> <p>Class which computes the sigma points and weights for a UKF algorithm. You can vary the UKF implementation by changing this class. For example, MerweScaledSigmaPoints implements the alpha, beta, kappa parameterization of Van der Merwe, and JulierSigmaPoints implements Julier's original kappa parameterization. See either of those for the required signature of this class if you want to implement your own.</p> required <code>sqrt_fn</code> <code>callable(ndarray)</code> <p>Defines how we compute the square root of a matrix, which has no unique answer. Cholesky is the default choice due to its speed. Typically your alternative choice will be scipy.linalg.sqrtm. Different choices affect how the sigma points are arranged relative to the eigenvectors of the covariance matrix. Usually this will not matter to you; if so the default cholesky() yields maximal performance. As of van der Merwe's dissertation of 2004 [6] this was not a well reseached area so I have no advice to give you.</p> <p>If your method returns a triangular matrix it must be upper triangular. Do not use numpy.linalg.cholesky - for historical reasons it returns a lower triangular matrix. The SciPy version does the right thing as far as this class is concerned.</p> <code>None (implies scipy.linalg.cholesky)</code> <code>x_mean_fn</code> <code>callable(sigma_points, weights)</code> <p>Function that computes the mean of the provided sigma points and weights. Use this if your state variable contains nonlinear values such as angles which cannot be summed.</p> <p>.. code-block:: Python</p> <pre><code>def state_mean(sigmas, Wm):\n    x = np.zeros(3)\n    sum_sin, sum_cos = 0., 0.\n\n    for i in range(len(sigmas)):\n        s = sigmas[i]\n        x[0] += s[0] * Wm[i]\n        x[1] += s[1] * Wm[i]\n        sum_sin += sin(s[2])*Wm[i]\n        sum_cos += cos(s[2])*Wm[i]\n    x[2] = atan2(sum_sin, sum_cos)\n    return x\n</code></pre> <code>None</code> <code>z_mean_fn</code> <code>callable(sigma_points, weights)</code> <p>Same as x_mean_fn, except it is called for sigma points which form the measurements after being passed through hx().</p> <code>None</code> <code>residual_x</code> <code>callable(x, y)</code> <code>None</code> <code>residual_z</code> <code>callable(x, y)</code> <p>Function that computes the residual (difference) between x and y. You will have to supply this if your state variable cannot support subtraction, such as angles (359-1 degreees is 2, not 358). x and y are state vectors, not scalars. One is for the state variable, the other is for the measurement state.</p> <p>.. code-block:: Python</p> <pre><code>def residual(a, b):\n    y = a[0] - b[0]\n    if y &gt; np.pi:\n        y -= 2*np.pi\n    if y &lt; -np.pi:\n        y += 2*np.pi\n    return y\n</code></pre> <code>None</code> <code>state_add</code> <p>Function that subtracts two state vectors, returning a new state vector. Used during update to compute <code>x + K@y</code> You will have to supply this if your state variable does not suport addition, such as it contains angles.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>x</code> <code>array(dim_x)</code> <p>state estimate vector</p> <code>P</code> <code>array(dim_x, dim_x)</code> <p>covariance estimate matrix</p> <code>x_prior</code> <code>array(dim_x)</code> <p>Prior (predicted) state estimate. The _prior and _post attributes are for convienence; they store the  prior and posterior of the current epoch. Read Only.</p> <code>P_prior</code> <code>array(dim_x, dim_x)</code> <p>Prior (predicted) state covariance matrix. Read Only.</p> <code>x_post</code> <code>array(dim_x)</code> <p>Posterior (updated) state estimate. Read Only.</p> <code>P_post</code> <code>array(dim_x, dim_x)</code> <p>Posterior (updated) state covariance matrix. Read Only.</p> <code>z</code> <code>ndarray</code> <p>Last measurement used in update(). Read only.</p> <code>R</code> <code>array(dim_z, dim_z)</code> <p>measurement noise matrix</p> <code>Q</code> <code>array(dim_x, dim_x)</code> <p>process noise matrix</p> <code>K</code> <code>array</code> <p>Kalman gain</p> <code>y</code> <code>array</code> <p>innovation residual</p> <code>log_likelihood</code> <code>scalar</code> <p>Log likelihood of last measurement update.</p> <code>likelihood</code> <code>float</code> <p>likelihood of last measurment. Read only.</p> <p>Computed from the log-likelihood. The log-likelihood can be very small,  meaning a large negative value such as -28000. Taking the exp() of that results in 0.0, which can break typical algorithms which multiply by this value, so by default we always return a number &gt;= sys.float_info.min.</p> <code>mahalanobis</code> <code>float</code> <p>mahalanobis distance of the measurement. Read only.</p> <code>inv</code> <code>function, default numpy.linalg.inv</code> <p>If you prefer another inverse function, such as the Moore-Penrose pseudo inverse, set it to that instead:</p> <p>.. code-block:: Python</p> <pre><code>kf.inv = np.linalg.pinv\n</code></pre> <p>Examples:</p> <p>Simple example of a linear order 1 kinematic filter in 2D. There is no need to use a UKF for this example, but it is easy to read.</p> <pre><code>&gt;&gt;&gt; def fx(x, dt):\n&gt;&gt;&gt;     # state transition function - predict next state based\n&gt;&gt;&gt;     # on constant velocity model x = vt + x_0\n&gt;&gt;&gt;     F = np.array([[1, dt, 0, 0],\n&gt;&gt;&gt;                   [0, 1, 0, 0],\n&gt;&gt;&gt;                   [0, 0, 1, dt],\n&gt;&gt;&gt;                   [0, 0, 0, 1]], dtype=float)\n&gt;&gt;&gt;     return np.dot(F, x)\n&gt;&gt;&gt;\n&gt;&gt;&gt; def hx(x):\n&gt;&gt;&gt;    # measurement function - convert state into a measurement\n&gt;&gt;&gt;    # where measurements are [x_pos, y_pos]\n&gt;&gt;&gt;    return np.array([x[0], x[2]])\n&gt;&gt;&gt;\n&gt;&gt;&gt; dt = 0.1\n&gt;&gt;&gt; # create sigma points to use in the filter. This is standard for Gaussian processes\n&gt;&gt;&gt; points = MerweScaledSigmaPoints(4, alpha=.1, beta=2., kappa=-1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; kf = UnscentedKalmanFilter(dim_x=4, dim_z=2, dt=dt, fx=fx, hx=hx, points=points)\n&gt;&gt;&gt; kf.x = np.array([-1., 1., -1., 1]) # initial state\n&gt;&gt;&gt; kf.P *= 0.2 # initial uncertainty\n&gt;&gt;&gt; z_std = 0.1\n&gt;&gt;&gt; kf.R = np.diag([z_std**2, z_std**2]) # 1 standard\n&gt;&gt;&gt; kf.Q = Q_discrete_white_noise(dim=2, dt=dt, var=0.01**2, block_size=2)\n&gt;&gt;&gt;\n&gt;&gt;&gt; zs = [[i+randn()*z_std, i+randn()*z_std] for i in range(50)] # measurements\n&gt;&gt;&gt; for z in zs:\n&gt;&gt;&gt;     kf.predict()\n&gt;&gt;&gt;     kf.update(z)\n&gt;&gt;&gt;     print(kf.x, 'log-likelihood', kf.log_likelihood)\n</code></pre> <p>For in depth explanations see my book Kalman and Bayesian Filters in Python https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> <p>Also see the filterpy/kalman/tests subdirectory for test code that may be illuminating.</p> References <p>.. [1] Julier, Simon J. \"The scaled unscented transformation,\"     American Control Converence, 2002, pp 4555-4559, vol 6.</p> <pre><code>Online copy:\nhttps://www.cs.unc.edu/~welch/kalman/media/pdf/ACC02-IEEE1357.PDF\n</code></pre> <p>.. [2] E. A. Wan and R. Van der Merwe, \u201cThe unscented Kalman filter for     nonlinear estimation,\u201d in Proc. Symp. Adaptive Syst. Signal     Process., Commun. Contr., Lake Louise, AB, Canada, Oct. 2000.</p> <pre><code>Online Copy:\nhttps://www.seas.harvard.edu/courses/cs281/papers/unscented.pdf\n</code></pre> <p>.. [3] S. Julier, J. Uhlmann, and H. Durrant-Whyte. \"A new method for        the nonlinear transformation of means and covariances in filters        and estimators,\" IEEE Transactions on Automatic Control, 45(3),        pp. 477-482 (March 2000).</p> <p>.. [4] E. A. Wan and R. Van der Merwe, \u201cThe Unscented Kalman filter for        Nonlinear Estimation,\u201d in Proc. Symp. Adaptive Syst. Signal        Process., Commun. Contr., Lake Louise, AB, Canada, Oct. 2000.</p> <pre><code>   https://www.seas.harvard.edu/courses/cs281/papers/unscented.pdf\n</code></pre> <p>.. [5] Wan, Merle \"The Unscented Kalman Filter,\" chapter in Kalman        Filtering and Neural Networks, John Wiley &amp; Sons, Inc., 2001.</p> <p>.. [6] R. Van der Merwe \"Sigma-Point Kalman Filters for Probabilitic        Inference in Dynamic State-Space Models\" (Doctoral dissertation)</p>"},{"location":"api/kalman/#bayesian_filters.kalman.UnscentedKalmanFilter.likelihood","title":"<code>likelihood</code>  <code>property</code>","text":"<p>Computed from the log-likelihood. The log-likelihood can be very small,  meaning a large negative value such as -28000. Taking the exp() of that results in 0.0, which can break typical algorithms which multiply by this value, so by default we always return a number &gt;= sys.float_info.min.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.UnscentedKalmanFilter.log_likelihood","title":"<code>log_likelihood</code>  <code>property</code>","text":"<p>log-likelihood of the last measurement.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.UnscentedKalmanFilter.mahalanobis","title":"<code>mahalanobis</code>  <code>property</code>","text":"<p>\" Mahalanobis distance of measurement. E.g. 3 means measurement was 3 standard deviations away from the predicted value.</p> <p>Returns:</p> Name Type Description <code>mahalanobis</code> <code>float</code>"},{"location":"api/kalman/#bayesian_filters.kalman.UnscentedKalmanFilter.__init__","title":"<code>__init__(dim_x, dim_z, dt, hx, fx, points, sqrt_fn=None, x_mean_fn=None, z_mean_fn=None, residual_x=None, residual_z=None, state_add=None)</code>","text":"<p>Create a Kalman filter. You are responsible for setting the various state variables to reasonable values; the defaults below will not give you a functional filter.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.UnscentedKalmanFilter.batch_filter","title":"<code>batch_filter(zs, Rs=None, dts=None, UT=None, saver=None)</code>","text":"<p>Performs the UKF filter over the list of measurement in <code>zs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>zs</code> <code>list - like</code> <p>list of measurements at each time step <code>self._dt</code> Missing measurements must be represented by 'None'.</p> required <code>Rs</code> <code>(None, array or list - like)</code> <p>optional list of values to use for the measurement error covariance R.</p> <p>If Rs is None then self.R is used for all epochs.</p> <p>If it is a list of matrices or a 3D array where len(Rs) == len(zs), then it is treated as a list of R values, one per epoch. This allows you to have varying R per epoch.</p> <code>None</code> <code>dts</code> <code>(None, scalar or list - like)</code> <p>optional value or list of delta time to be passed into predict.</p> <p>If dtss is None then self.dt is used for all epochs.</p> <p>If it is a list where len(dts) == len(zs), then it is treated as a list of dt values, one per epoch. This allows you to have varying epoch durations.</p> <code>None</code> <code>UT</code> <code>function(sigmas, Wm, Wc, noise_cov)</code> <p>Optional function to compute the unscented transform for the sigma points passed through hx. Typically the default function will work - you can use x_mean_fn and z_mean_fn to alter the behavior of the unscented transform.</p> <code>None</code> <code>saver</code> <code>Saver</code> <p>filterpy.common.Saver object. If provided, saver.save() will be called after every epoch</p> <code>None</code> <p>Returns:</p> Name Type Description <code>means</code> <code>ndarray((n, dim_x, 1))</code> <p>array of the state for each time step after the update. Each entry is an np.array. In other words <code>means[k,:]</code> is the state at step <code>k</code>.</p> <code>covariance</code> <code>ndarray((n, dim_x, dim_x))</code> <p>array of the covariances for each time step after the update. In other words <code>covariance[k,:,:]</code> is the covariance at step <code>k</code>.</p> <p>Examples:</p> <p>.. code-block:: Python</p> <pre><code># this example demonstrates tracking a measurement where the time\n# between measurement varies, as stored in dts The output is then smoothed\n# with an RTS smoother.\n\nzs = [t + random.randn()*4 for t in range (40)]\n\n(mu, cov, _, _) = ukf.batch_filter(zs, dts=dts)\n(xs, Ps, Ks) = ukf.rts_smoother(mu, cov)\n</code></pre>"},{"location":"api/kalman/#bayesian_filters.kalman.UnscentedKalmanFilter.compute_process_sigmas","title":"<code>compute_process_sigmas(dt, fx=None, **fx_args)</code>","text":"<p>computes the values of sigmas_f. Normally a user would not call this, but it is useful if you need to call update more than once between calls to predict (to update for multiple simultaneous measurements), so the sigmas correctly reflect the updated state x, P.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.UnscentedKalmanFilter.cross_variance","title":"<code>cross_variance(x, z, sigmas_f, sigmas_h)</code>","text":"<p>Compute cross variance of the state <code>x</code> and measurement <code>z</code>.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.UnscentedKalmanFilter.predict","title":"<code>predict(dt=None, UT=None, fx=None, **fx_args)</code>","text":"<p>Performs the predict step of the UKF. On return, self.x and self.P contain the predicted state (x) and covariance (P). '</p> <p>Important: this MUST be called before update() is called for the first time.</p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <code>double</code> <p>If specified, the time step to be used for this prediction. self._dt is used if this is not provided.</p> <code>None</code> <code>fx</code> <code>callable f(x, dt, **fx_args)</code> <p>State transition function. If not provided, the default function passed in during construction will be used.</p> <code>None</code> <code>UT</code> <code>function(sigmas, Wm, Wc, noise_cov)</code> <p>Optional function to compute the unscented transform for the sigma points passed through hx. Typically the default function will work - you can use x_mean_fn and z_mean_fn to alter the behavior of the unscented transform.</p> <code>None</code> <code>**fx_args</code> <code>keyword arguments</code> <p>optional keyword arguments to be passed into f(x).</p> <code>{}</code>"},{"location":"api/kalman/#bayesian_filters.kalman.UnscentedKalmanFilter.rts_smoother","title":"<code>rts_smoother(Xs, Ps, Qs=None, dts=None, UT=None)</code>","text":"<p>Runs the Rauch-Tung-Striebel Kalman smoother on a set of means and covariances computed by the UKF. The usual input would come from the output of <code>batch_filter()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>Xs</code> <code>array</code> <p>array of the means (state variable x) of the output of a Kalman filter.</p> required <code>Ps</code> <code>array</code> <p>array of the covariances of the output of a kalman filter.</p> required <code>Qs</code> <p>Process noise of the Kalman filter at each time step. Optional, if not provided the filter's self.Q will be used</p> <code>None</code> <code>dt</code> <code>optional, float or array-like of float</code> <p>If provided, specifies the time step of each step of the filter. If float, then the same time step is used for all steps. If an array, then each element k contains the time  at step k. Units are seconds.</p> required <code>UT</code> <code>function(sigmas, Wm, Wc, noise_cov)</code> <p>Optional function to compute the unscented transform for the sigma points passed through hx. Typically the default function will work - you can use x_mean_fn and z_mean_fn to alter the behavior of the unscented transform.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>smoothed means</p> <code>P</code> <code>ndarray</code> <p>smoothed state covariances</p> <code>K</code> <code>ndarray</code> <p>smoother gain at each step</p> <p>Examples:</p> <p>.. code-block:: Python</p> <pre><code>zs = [t + random.randn()*4 for t in range (40)]\n\n(mu, cov, _, _) = kalman.batch_filter(zs)\n(x, P, K) = rts_smoother(mu, cov, fk.F, fk.Q)\n</code></pre>"},{"location":"api/kalman/#bayesian_filters.kalman.UnscentedKalmanFilter.update","title":"<code>update(z, R=None, UT=None, hx=None, **hx_args)</code>","text":"<p>Update the UKF with the given measurements. On return, self.x and self.P contain the new mean and covariance of the filter.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>numpy.array of shape (dim_z)</code> <p>measurement vector</p> required <code>R</code> <code>array((dim_z, dim_z))</code> <p>Measurement noise. If provided, overrides self.R for this function call.</p> <code>None</code> <code>UT</code> <code>function(sigmas, Wm, Wc, noise_cov)</code> <p>Optional function to compute the unscented transform for the sigma points passed through hx. Typically the default function will work - you can use x_mean_fn and z_mean_fn to alter the behavior of the unscented transform.</p> <code>None</code> <code>hx</code> <code>callable h(x, **hx_args)</code> <p>Measurement function. If not provided, the default function passed in during construction will be used.</p> <code>None</code> <code>**hx_args</code> <code>keyword argument</code> <p>arguments to be passed into h(x) after x -&gt; h(x, **hx_args)</p> <code>{}</code>"},{"location":"api/kalman/#ensemblekalmanfilter","title":"EnsembleKalmanFilter","text":""},{"location":"api/kalman/#bayesian_filters.kalman.EnsembleKalmanFilter","title":"<code>EnsembleKalmanFilter</code>","text":"<p>               Bases: <code>object</code></p> <p>This implements the ensemble Kalman filter (EnKF). The EnKF uses an ensemble of hundreds to thousands of state vectors that are randomly sampled around the estimate, and adds perturbations at each update and predict step. It is useful for extremely large systems such as found in hydrophysics. As such, this class is admittedly a toy as it is far too slow with large N.</p> <p>There are many versions of this sort of this filter. This formulation is due to Crassidis and Junkins [1]. It works with both linear and nonlinear systems.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array(dim_x)</code> <p>state mean</p> required <code>P</code> <code>array((dim_x, dim_x))</code> <p>covariance of the state</p> required <code>dim_z</code> <code>int</code> <p>Number of of measurement inputs. For example, if the sensor provides you with position in (x,y), dim_z would be 2.</p> required <code>dt</code> <code>float</code> <p>time step in seconds</p> required <code>N</code> <code>int</code> <p>number of sigma points (ensembles). Must be greater than 1.</p> required <code>hx</code> <code>function hx(x)</code> <p>Measurement function. May be linear or nonlinear - converts state x into a measurement. Return must be an np.array of the same dimensionality as the measurement vector.</p> required <code>fx</code> <code>function fx(x, dt)</code> <p>State transition function. May be linear or nonlinear. Projects state x into the next time period. Returns the projected state x.</p> required <p>Attributes:</p> Name Type Description <code>x</code> <code>array(dim_x, 1)</code> <p>State estimate</p> <code>P</code> <code>array(dim_x, dim_x)</code> <p>State covariance matrix</p> <code>x_prior</code> <code>array(dim_x, 1)</code> <p>Prior (predicted) state estimate. The _prior and _post attributes are for convienence; they store the  prior and posterior of the current epoch. Read Only.</p> <code>P_prior</code> <code>array(dim_x, dim_x)</code> <p>Prior (predicted) state covariance matrix. Read Only.</p> <code>x_post</code> <code>array(dim_x, 1)</code> <p>Posterior (updated) state estimate. Read Only.</p> <code>P_post</code> <code>array(dim_x, dim_x)</code> <p>Posterior (updated) state covariance matrix. Read Only.</p> <code>z</code> <code>array</code> <p>Last measurement used in update(). Read only.</p> <code>R</code> <code>array(dim_z, dim_z)</code> <p>Measurement noise matrix</p> <code>Q</code> <code>array(dim_x, dim_x)</code> <p>Process noise matrix</p> <code>fx</code> <code>callable(x, dt)</code> <p>State transition function</p> <code>hx</code> <code>callable(x)</code> <p>Measurement function. Convert state <code>x</code> into a measurement</p> <code>inv</code> <code>function, default numpy.linalg.inv</code> <p>If you prefer another inverse function, such as the Moore-Penrose pseudo inverse, set it to that instead: kf.inv = np.linalg.pinv</p> <p>Examples:</p> <p>.. code-block:: Python</p> <pre><code>def hx(x):\n   return np.array([x[0]])\n\nF = np.array([[1., 1.],\n              [0., 1.]])\ndef fx(x, dt):\n    return np.dot(F, x)\n\nx = np.array([0., 1.])\nP = np.eye(2) * 100.\ndt = 0.1\nf = EnsembleKalmanFilter(x=x, P=P, dim_z=1, dt=dt,\n                         N=8, hx=hx, fx=fx)\n\nstd_noise = 3.\nf.R *= std_noise**2\nf.Q = Q_discrete_white_noise(2, dt, .01)\n\nwhile True:\n    z = read_sensor()\n    f.predict()\n    f.update(np.asarray([z]))\n</code></pre> <p>See my book Kalman and Bayesian Filters in Python https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> References <ul> <li>[1] John L Crassidis and John L. Junkins. \"Optimal Estimation of   Dynamic Systems. CRC Press, second edition. 2012. pp, 257-9.</li> </ul>"},{"location":"api/kalman/#bayesian_filters.kalman.EnsembleKalmanFilter.initialize","title":"<code>initialize(x, P)</code>","text":"<p>Initializes the filter with the specified mean and covariance. Only need to call this if you are using the filter to filter more than one set of data; this is called by init</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array(dim_z)</code> <p>state mean</p> required <code>P</code> <code>array((dim_x, dim_x))</code> <p>covariance of the state</p> required"},{"location":"api/kalman/#bayesian_filters.kalman.EnsembleKalmanFilter.predict","title":"<code>predict()</code>","text":"<p>Predict next position.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.EnsembleKalmanFilter.update","title":"<code>update(z, R=None)</code>","text":"<p>Add a new measurement (z) to the kalman filter. If z is None, nothing is changed.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>array</code> <p>measurement for this update.</p> required <code>R</code> <code>np.array, scalar, or None</code> <p>Optionally provide R to override the measurement noise for this one call, otherwise self.R will be used.</p> <code>None</code>"},{"location":"api/kalman/#informationfilter","title":"InformationFilter","text":""},{"location":"api/kalman/#bayesian_filters.kalman.InformationFilter","title":"<code>InformationFilter</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a linear Information filter. Information filters compute the inverse of the Kalman filter, allowing you to easily denote having no information at initialization.</p> <p>You are responsible for setting the various state variables to reasonable values; the defaults below will not give you a functional filter.</p> <p>Parameters:</p> Name Type Description Default <code>dim_x</code> <code>int</code> <p>Number of state variables for the  filter. For example, if you are tracking the position and velocity of an object in two dimensions, dim_x would be 4.</p> <p>This is used to set the default size of P, Q, and u</p> required <code>dim_z</code> <code>int</code> <p>Number of of measurement inputs. For example, if the sensor provides you with position in (x,y), dim_z would be 2.</p> required <code>dim_u</code> <code>int(optional)</code> <p>size of the control input, if it is being used. Default value of 0 indicates it is not used.</p> <code>0</code> <code>self</code> required <code>self</code> required <p>Attributes:</p> Name Type Description <code>x</code> <code>array(dim_x, 1)</code> <p>State estimate vector</p> <code>P_inv</code> <code>array(dim_x, dim_x)</code> <p>inverse state covariance matrix</p> <code>x_prior</code> <code>array(dim_x, 1)</code> <p>Prior (predicted) state estimate. The _prior and _post attributes are for convienence; they store the  prior and posterior of the current epoch. Read Only.</p> <code>P_inv_prior</code> <code>array(dim_x, dim_x)</code> <p>Inverse prior (predicted) state covariance matrix. Read Only.</p> <code>x_post</code> <code>array(dim_x, 1)</code> <p>Posterior (updated) state estimate. Read Only.</p> <code>P_inv_post</code> <code>array(dim_x, dim_x)</code> <p>Inverse posterior (updated) state covariance matrix. Read Only.</p> <code>z</code> <code>ndarray</code> <p>Last measurement used in update(). Read only.</p> <code>R_inv</code> <code>array(dim_z, dim_z)</code> <p>inverse of measurement noise matrix</p> <code>Q</code> <code>array(dim_x, dim_x)</code> <p>Process noise matrix</p> <code>H</code> <code>array(dim_z, dim_x)</code> <p>Measurement function</p> <code>y</code> <code>array</code> <p>Residual of the update step. Read only.</p> <code>K</code> <code>array(dim_x, dim_z)</code> <p>Kalman gain of the update step. Read only.</p> <code>S</code> <code>array</code> <p>Systen uncertaintly projected to measurement space. Read only.</p> <code>log_likelihood</code> <code>float</code> <p>log-likelihood of the last measurement. Read only.</p> <code>likelihood</code> <code>float</code> <p>likelihood of last measurment. Read only.</p> <p>Computed from the log-likelihood. The log-likelihood can be very small,  meaning a large negative value such as -28000. Taking the exp() of that results in 0.0, which can break typical algorithms which multiply by this value, so by default we always return a number &gt;= sys.float_info.min.</p> <code>inv</code> <code>function, default numpy.linalg.inv</code> <p>If you prefer another inverse function, such as the Moore-Penrose pseudo inverse, set it to that instead: kf.inv = np.linalg.pinv</p> <p>Examples:</p> <p>See my book Kalman and Bayesian Filters in Python https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p>"},{"location":"api/kalman/#bayesian_filters.kalman.InformationFilter.F","title":"<code>F</code>  <code>property</code> <code>writable</code>","text":"<p>State Transition matrix</p>"},{"location":"api/kalman/#bayesian_filters.kalman.InformationFilter.P","title":"<code>P</code>  <code>property</code>","text":"<p>State covariance matrix</p>"},{"location":"api/kalman/#bayesian_filters.kalman.InformationFilter.batch_filter","title":"<code>batch_filter(zs, Rs=None, update_first=False, saver=None)</code>","text":"<p>Batch processes a sequences of measurements.</p> <p>Parameters:</p> Name Type Description Default <code>zs</code> <code>list - like</code> <p>list of measurements at each time step <code>self.dt</code> Missing measurements must be represented by 'None'.</p> required <code>Rs</code> <code>list - like</code> <p>optional list of values to use for the measurement error covariance; a value of None in any position will cause the filter to use <code>self.R</code> for that time step.</p> <code>None</code> <code>update_first</code> <code>(bool, optional)</code> <p>controls whether the order of operations is update followed by predict, or predict followed by update. Default is predict-&gt;update.</p> <code>False</code> <code>saver</code> <code>Saver</code> <p>filterpy.common.Saver object. If provided, saver.save() will be called after every epoch</p> <code>None</code> <p>Returns:</p> Name Type Description <code>means</code> <code>array((n, dim_x, 1))</code> <p>array of the state for each time step. Each entry is an np.array. In other words <code>means[k,:]</code> is the state at step <code>k</code>.</p> <code>covariance</code> <code>array((n, dim_x, dim_x))</code> <p>array of the covariances for each time step. In other words <code>covariance[k,:,:]</code> is the covariance at step <code>k</code>.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.InformationFilter.predict","title":"<code>predict(u=0)</code>","text":"<p>Predict next position.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>ndarray</code> <p>Optional control vector. If non-zero, it is multiplied by B to create the control input into the system.</p> <code>0</code>"},{"location":"api/kalman/#bayesian_filters.kalman.InformationFilter.update","title":"<code>update(z, R_inv=None)</code>","text":"<p>Add a new measurement (z) to the kalman filter. If z is None, nothing is changed.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>array</code> <p>measurement for this update.</p> required <code>R</code> <code>np.array, scalar, or None</code> <p>Optionally provide R to override the measurement noise for this one call, otherwise  self.R will be used.</p> required"},{"location":"api/kalman/#immestimator","title":"IMMEstimator","text":""},{"location":"api/kalman/#bayesian_filters.kalman.IMMEstimator","title":"<code>IMMEstimator</code>","text":"<p>               Bases: <code>object</code></p> <p>Implements an Interacting Multiple-Model (IMM) estimator.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>(N,) array_like of KalmanFilter objects</code> <p>List of N filters. filters[i] is the ith Kalman filter in the IMM estimator.</p> <p>Each filter must have the same dimension for the state <code>x</code> and <code>P</code>, otherwise the states of each filter cannot be mixed with each other.</p> required <code>mu</code> <code>(N,) array_like of float</code> <p>mode probability: mu[i] is the probability that filter i is the correct one.</p> required <code>M</code> <code>(N, N) ndarray of float</code> <p>Markov chain transition matrix. M[i,j] is the probability of switching from filter j to filter i.</p> required <p>Attributes:</p> Name Type Description <code>x</code> <code>array(dim_x, 1)</code> <p>Current state estimate. Any call to update() or predict() updates this variable.</p> <code>P</code> <code>array(dim_x, dim_x)</code> <p>Current state covariance matrix. Any call to update() or predict() updates this variable.</p> <code>x_prior</code> <code>array(dim_x, 1)</code> <p>Prior (predicted) state estimate. The _prior and _post attributes are for convienence; they store the  prior and posterior of the current epoch. Read Only.</p> <code>P_prior</code> <code>array(dim_x, dim_x)</code> <p>Prior (predicted) state covariance matrix. Read Only.</p> <code>x_post</code> <code>array(dim_x, 1)</code> <p>Posterior (updated) state estimate. Read Only.</p> <code>P_post</code> <code>array(dim_x, dim_x)</code> <p>Posterior (updated) state covariance matrix. Read Only.</p> <code>N</code> <code>int</code> <p>number of filters in the filter bank</p> <code>mu</code> <code>(N,) ndarray of float</code> <p>mode probability: mu[i] is the probability that filter i is the correct one.</p> <code>M</code> <code>(N, N) ndarray of float</code> <p>Markov chain transition matrix. M[i,j] is the probability of switching from filter j to filter i.</p> <code>cbar</code> <code>(N,) ndarray of float</code> <p>Total probability, after interaction, that the target is in state j. We use it as the # normalization constant.</p> <code>likelihood</code> <code>(N,) ndarray of float</code> <p>Likelihood of each individual filter's last measurement.</p> <code>omega</code> <code>(N, N) ndarray of float</code> <p>Mixing probabilitity - omega[i, j] is the probabilility of mixing the state of filter i into filter j. Perhaps more understandably, it weights the states of each filter by:     x_j = sum(omega[i,j] * x_i)</p> <p>with a similar weighting for P_j</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from bayesian_filters.common import kinematic_kf\n&gt;&gt;&gt; from bayesian_filters.kalman import IMMEstimator\n&gt;&gt;&gt; kf1 = kinematic_kf(2, 2)\n&gt;&gt;&gt; kf2 = kinematic_kf(2, 2)\n&gt;&gt;&gt; # do some settings of x, R, P etc. here, I'll just use the defaults\n&gt;&gt;&gt; kf2.Q *= 0   # no prediction error in second filter\n&gt;&gt;&gt;\n&gt;&gt;&gt; filters = [kf1, kf2]\n&gt;&gt;&gt; mu = [0.5, 0.5]  # each filter is equally likely at the start\n&gt;&gt;&gt; trans = np.array([[0.97, 0.03], [0.03, 0.97]])\n&gt;&gt;&gt; imm = IMMEstimator(filters, mu, trans)\n&gt;&gt;&gt;\n&gt;&gt;&gt; for i in range(100):\n&gt;&gt;&gt;     # make some noisy data\n&gt;&gt;&gt;     x = i + np.random.randn()*np.sqrt(kf1.R[0, 0])\n&gt;&gt;&gt;     y = i + np.random.randn()*np.sqrt(kf1.R[1, 1])\n&gt;&gt;&gt;     z = np.array([[x], [y]])\n&gt;&gt;&gt;\n&gt;&gt;&gt;     # perform predict/update cycle\n&gt;&gt;&gt;     imm.predict()\n&gt;&gt;&gt;     imm.update(z)\n&gt;&gt;&gt;     print(imm.x.T)\n</code></pre> <p>For a full explanation and more examples see my book Kalman and Bayesian Filters in Python https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> References <p>Bar-Shalom, Y., Li, X-R., and Kirubarajan, T. \"Estimation with Application to Tracking and Navigation\". Wiley-Interscience, 2001.</p> <p>Crassidis, J and Junkins, J. \"Optimal Estimation of Dynamic Systems\". CRC Press, second edition. 2012.</p> <p>Labbe, R. \"Kalman and Bayesian Filters in Python\". https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p>"},{"location":"api/kalman/#bayesian_filters.kalman.IMMEstimator.predict","title":"<code>predict(u=None)</code>","text":"<p>Predict next state (prior) using the IMM state propagation equations.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>array</code> <p>Control vector. If not <code>None</code>, it is multiplied by B to create the control input into the system.</p> <code>None</code>"},{"location":"api/kalman/#bayesian_filters.kalman.IMMEstimator.update","title":"<code>update(z)</code>","text":"<p>Add a new measurement (z) to the Kalman filter. If z is None, nothing is changed.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>array</code> <p>measurement for this update.</p> required"},{"location":"api/kalman/#mmaefilterbank","title":"MMAEFilterBank","text":""},{"location":"api/kalman/#bayesian_filters.kalman.MMAEFilterBank","title":"<code>MMAEFilterBank</code>","text":"<p>               Bases: <code>object</code></p> <p>Implements the fixed Multiple Model Adaptive Estimator (MMAE). This is a bank of independent Kalman filters. This estimator computes the likelihood that each filter is the correct one, and blends their state estimates weighted by their likelihood to produce the state estimate.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>list of Kalman filters</code> <p>List of Kalman filters.</p> required <code>p</code> <code>list-like of floats</code> <p>Initial probability that each filter is the correct one. In general you'd probably set each element to 1./len(p).</p> required <code>dim_x</code> <code>float</code> <p>number of random variables in the state X</p> required <code>H</code> <code>Measurement matrix</code> <code>None</code> <p>Attributes:</p> Name Type Description <code>x</code> <code>array(dim_x, 1)</code> <p>Current state estimate. Any call to update() or predict() updates this variable.</p> <code>P</code> <code>array(dim_x, dim_x)</code> <p>Current state covariance matrix. Any call to update() or predict() updates this variable.</p> <code>x_prior</code> <code>array(dim_x, 1)</code> <p>Prior (predicted) state estimate. The _prior and _post attributes are for convienence; they store the  prior and posterior of the current epoch. Read Only.</p> <code>P_prior</code> <code>array(dim_x, dim_x)</code> <p>Prior (predicted) state covariance matrix. Read Only.</p> <code>x_post</code> <code>array(dim_x, 1)</code> <p>Posterior (updated) state estimate. Read Only.</p> <code>P_post</code> <code>array(dim_x, dim_x)</code> <p>Posterior (updated) state covariance matrix. Read Only.</p> <code>z</code> <code>ndarray</code> <p>Last measurement used in update(). Read only.</p> <code>filters</code> <code>list of Kalman filters</code> <p>List of Kalman filters.</p> <p>Examples:</p> <p>..code:     ca = make_ca_filter(dt, noise_factor=0.6)     cv = make_ca_filter(dt, noise_factor=0.6)     cv.F[:,2] = 0 # remove acceleration term     cv.P[2,2] = 0     cv.Q[2,2] = 0</p> <pre><code>filters = [cv, ca]\nbank = MMAEFilterBank(filters, p=(0.5, 0.5), dim_x=3)\n\nfor z in zs:\n    bank.predict()\n    bank.update(z)\n</code></pre> <p>Also, see my book Kalman and Bayesian Filters in Python https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> References <p>Zarchan and Musoff. \"Fundamentals of Kalman filtering: A Practical Approach.\" AIAA, third edition.</p>"},{"location":"api/kalman/#bayesian_filters.kalman.MMAEFilterBank.predict","title":"<code>predict(u=0)</code>","text":"<p>Predict next position using the Kalman filter state propagation equations for each filter in the bank.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>array</code> <p>Optional control vector. If non-zero, it is multiplied by B to create the control input into the system.</p> <code>0</code>"},{"location":"api/kalman/#bayesian_filters.kalman.MMAEFilterBank.update","title":"<code>update(z, R=None, H=None)</code>","text":"<p>Add a new measurement (z) to the Kalman filter. If z is None, nothing is changed.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>array</code> <p>measurement for this update.</p> required <code>R</code> <code>np.array, scalar, or None</code> <p>Optionally provide R to override the measurement noise for this one call, otherwise  self.R will be used.</p> <code>None</code> <code>H</code> <code>np.array,  or None</code> <p>Optionally provide H to override the measurement function for this one call, otherwise  self.H will be used.</p> <code>None</code>"},{"location":"api/leastsq/","title":"Least Squares Module","text":"<p>The leastsq module contains least squares filter implementations.</p>"},{"location":"api/leastsq/#bayesian_filters.leastsq","title":"<code>leastsq</code>","text":"<p>Copyright 2015 Roger R Labbe Jr.</p> <p>FilterPy library. http://github.com/rlabbe/filterpy</p> <p>Documentation at: https://filterpy.readthedocs.org</p> <p>Supporting book at: https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> <p>This is licensed under an MIT license. See the readme.MD file for more information.</p>"},{"location":"api/leastsq/#bayesian_filters.leastsq.__all__","title":"<code>__all__ = ['least_squares']</code>  <code>module-attribute</code>","text":""},{"location":"api/leastsq/#bayesian_filters.leastsq.LeastSquaresFilter","title":"<code>LeastSquaresFilter</code>","text":"<p>               Bases: <code>object</code></p> <p>Implements a Least Squares recursive filter. Formulation is per Zarchan [1]_.</p> <p>Filter may be of order 0 to 2. Order 0 assumes the value being tracked is a constant, order 1 assumes that it moves in a line, and order 2 assumes that it is tracking a second order polynomial.</p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <code>float</code> <p>time step per update</p> required <code>order</code> <code>int</code> <p>order of filter 0..2</p> required <code>noise_sigma</code> <code>float</code> <p>sigma (std dev) in x. This allows us to calculate the error of the filter, it does not influence the filter output.</p> <code>0.0</code> <p>Attributes:</p> Name Type Description <code>n</code> <code>int</code> <p>step in the recursion. 0 prior to first call, 1 after the first call, etc.</p> <code>K</code> <code>array</code> <p>Gains for the filter. K[0] for all orders, K[1] for orders 0 and 1, and K[2] for order 2</p> <code>x</code> <code>array(order + 1, 1)</code> <p>estimate(s) of the output. It is a vector containing the estimate x and the derivatives of x: [x x' x''].T. It contains as many derivatives as the order allows. That is, a zero order filter has no derivatives, a first order has one derivative, and a second order has two.</p> <code>y</code> <code>float</code> <p>residual (difference between measurement projection of previous estimate to current time).</p> <p>Examples:</p> <p>.. code-block:: Python</p> <pre><code>from bayesian_filters.leastsq import LeastSquaresFilter\n\nlsq = LeastSquaresFilter(dt=0.1, order=1, noise_sigma=2.3)\n\nwhile True:\n    z = sensor_reading()  # get a measurement\n    x = lsq.update(z)     # get the filtered estimate.\n    print('error: {}, velocity error: {}'.format(\n          lsq.error, lsq.derror))\n</code></pre> References <p>.. [1] Zarchan and Musoff. \"Fundamentals of Kalman Filtering: A Practical       Approach.\" Third Edition. AIAA, 2009.</p>"},{"location":"api/leastsq/#bayesian_filters.leastsq.LeastSquaresFilter.errors","title":"<code>errors()</code>","text":"<p>Computes and returns the error and standard deviation of the filter at this time step.</p> <p>Returns:</p> Name Type Description <code>error</code> <code>np.array size 1xorder+1</code> <code>std</code> <code>np.array size 1xorder+1</code>"},{"location":"api/leastsq/#bayesian_filters.leastsq.LeastSquaresFilter.reset","title":"<code>reset()</code>","text":"<p>reset filter back to state at time of construction</p>"},{"location":"api/leastsq/#bayesian_filters.leastsq.LeastSquaresFilter.update","title":"<code>update(z)</code>","text":"<p>Update filter with new measurement <code>z</code></p> <p>Returns:</p> Name Type Description <code>x</code> <code>array</code> <p>estimate for this time step (same as self.x)</p>"},{"location":"api/monte-carlo/","title":"Monte Carlo Module","text":"<p>The monte_carlo module contains Monte Carlo sampling and resampling functions.</p>"},{"location":"api/monte-carlo/#bayesian_filters.monte_carlo","title":"<code>monte_carlo</code>","text":"<p>Copyright 2015 Roger R Labbe Jr.</p> <p>filterpy library. http://github.com/rlabbe/filterpy</p> <p>Documentation at: https://filterpy.readthedocs.org</p> <p>Supporting book at: https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> <p>This is licensed under an MIT license. See the readme.MD file for more information.</p>"},{"location":"api/monte-carlo/#bayesian_filters.monte_carlo.__all__","title":"<code>__all__ = ['resampling']</code>  <code>module-attribute</code>","text":""},{"location":"api/monte-carlo/#bayesian_filters.monte_carlo.multinomial_resample","title":"<code>multinomial_resample(weights)</code>","text":"<p>This is the naive form of roulette sampling where we compute the  cumulative sum of the weights and then use binary search to select the  resampled point based on a uniformly distributed random number. Run time  is O(n log n). You do not want to use this algorithm in practice; for some  reason it is popular in blogs and online courses so I included it for  reference.</p>"},{"location":"api/monte-carlo/#bayesian_filters.monte_carlo.residual_resample","title":"<code>residual_resample(weights)</code>","text":"<p>Performs the residual resampling algorithm used by particle filters.</p> <p>Based on observation that we don't need to use random numbers to select most of the weights. Take int(N*w^i) samples of each particle i, and then resample any remaining using a standard resampling algorithm [1]</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>list-like of float</code> <p>list of weights as floats</p> required <p>Returns:</p> Name Type Description <code>indexes</code> <code>ndarray of ints</code> <p>array of indexes into the weights defining the resample. i.e. the index of the zeroth resample is indexes[0], etc.</p> References <p>.. [1] J. S. Liu and R. Chen. Sequential Monte Carlo methods for dynamic    systems. Journal of the American Statistical Association,    93(443):1032\u20131044, 1998.</p>"},{"location":"api/monte-carlo/#bayesian_filters.monte_carlo.stratified_resample","title":"<code>stratified_resample(weights)</code>","text":"<p>Performs the stratified resampling algorithm used by particle filters.</p> <p>This algorithms aims to make selections relatively uniformly across the particles. It divides the cumulative sum of the weights into N equal divisions, and then selects one particle randomly from each division. This guarantees that each sample is between 0 and 2/N apart.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>list-like of float</code> <p>list of weights as floats</p> required <p>Returns:</p> Name Type Description <code>indexes</code> <code>ndarray of ints</code> <p>array of indexes into the weights defining the resample. i.e. the index of the zeroth resample is indexes[0], etc.</p>"},{"location":"api/monte-carlo/#bayesian_filters.monte_carlo.systematic_resample","title":"<code>systematic_resample(weights)</code>","text":"<p>Performs the systemic resampling algorithm used by particle filters.</p> <p>This algorithm separates the sample space into N divisions. A single random offset is used to to choose where to sample from for all divisions. This guarantees that every sample is exactly 1/N apart.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>list-like of float</code> <p>list of weights as floats</p> required <p>Returns:</p> Name Type Description <code>indexes</code> <code>ndarray of ints</code> <p>array of indexes into the weights defining the resample. i.e. the index of the zeroth resample is indexes[0], etc.</p>"},{"location":"api/stats/","title":"Stats","text":""},{"location":"api/stats/#stats","title":"stats","text":"<p>A collection of functions used to compute and plot statistics relevant to Bayesian filters.</p>"},{"location":"api/stats/#gaussian","title":"Gaussian","text":""},{"location":"api/stats/#bayesian_filters.stats.stats.gaussian","title":"<code>gaussian(x, mean, var, normed=True)</code>","text":"<p>returns probability density function (pdf) for x given a Gaussian with the specified mean and variance. All must be scalars.</p> <p>gaussian (1,2,3) is equivalent to scipy.stats.norm(2, math.sqrt(3)).pdf(1) It is quite a bit faster albeit much less flexible than the latter.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>scalar or array - like</code> <p>The value(s) for which we compute the distribution</p> required <code>mean</code> <code>scalar</code> <p>Mean of the Gaussian</p> required <code>var</code> <code>scalar</code> <p>Variance of the Gaussian</p> required <code>normed</code> <code>bool</code> <p>Normalize the output if the input is an array of values.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>pdf</code> <code>float</code> <p>probability distribution of x for the Gaussian (mean, var). E.g. 0.101 denotes 10.1%.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gaussian(8, 1, 2)\n1.3498566943461957e-06\n</code></pre> <pre><code>&gt;&gt;&gt; gaussian([8, 7, 9], 1, 2)\narray([1.34985669e-06, 3.48132630e-05, 3.17455867e-08])\n</code></pre> Source code in <code>bayesian_filters/stats/stats.py</code> <pre><code>def gaussian(x, mean, var, normed=True):\n    \"\"\"\n    returns probability density function (pdf) for x given a Gaussian with the\n    specified mean and variance. All must be scalars.\n\n    gaussian (1,2,3) is equivalent to scipy.stats.norm(2, math.sqrt(3)).pdf(1)\n    It is quite a bit faster albeit much less flexible than the latter.\n\n    Parameters\n    ----------\n\n    x : scalar or array-like\n        The value(s) for which we compute the distribution\n\n    mean : scalar\n        Mean of the Gaussian\n\n    var : scalar\n        Variance of the Gaussian\n\n    normed : bool, default True\n        Normalize the output if the input is an array of values.\n\n    Returns\n    -------\n\n    pdf : float\n        probability distribution of x for the Gaussian (mean, var). E.g. 0.101 denotes\n        10.1%.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; gaussian(8, 1, 2)\n    1.3498566943461957e-06\n\n    &gt;&gt;&gt; gaussian([8, 7, 9], 1, 2)\n    array([1.34985669e-06, 3.48132630e-05, 3.17455867e-08])\n    \"\"\"\n\n    pdf = ((2 * math.pi * var) ** -0.5) * np.exp((-0.5 * (np.asarray(x) - mean) ** 2.0) / var)\n    if normed and len(np.shape(pdf)) &gt; 0:\n        pdf = pdf / sum(pdf)\n\n    return pdf\n</code></pre>"},{"location":"api/stats/#multiply-gaussians","title":"Multiply Gaussians","text":""},{"location":"api/stats/#bayesian_filters.stats.stats.mul","title":"<code>mul(mean1, var1, mean2, var2)</code>","text":"<p>Multiply Gaussian (mean1, var1) with (mean2, var2) and return the results as a tuple (mean, var).</p> <p>Strictly speaking the product of two Gaussian PDFs is a Gaussian function, not Gaussian PDF. It is, however, proportional to a Gaussian PDF, so it is safe to treat the output as a PDF for any filter using Bayes equation, which normalizes the result anyway.</p> <p>Parameters:</p> Name Type Description Default <code>mean1</code> <code>scalar</code> <p>mean of first Gaussian</p> required <code>var1</code> <code>scalar</code> <p>variance of first Gaussian</p> required <code>mean2</code> <code>scalar</code> <p>mean of second Gaussian</p> required <code>var2</code> <code>scalar</code> <p>variance of second Gaussian</p> required <p>Returns:</p> Name Type Description <code>mean</code> <code>scalar</code> <p>mean of product</p> <code>var</code> <code>scalar</code> <p>variance of product</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mul(1, 2, 3, 4)\n(1.6666666666666667, 1.3333333333333333)\n</code></pre> References <p>Bromily. \"Products and Convolutions of Gaussian Probability Functions\", Tina Memo No. 2003-003. http://www.tina-vision.net/docs/memos/2003-003.pdf</p> Source code in <code>bayesian_filters/stats/stats.py</code> <pre><code>def mul(mean1, var1, mean2, var2):\n    \"\"\"\n    Multiply Gaussian (mean1, var1) with (mean2, var2) and return the\n    results as a tuple (mean, var).\n\n    Strictly speaking the product of two Gaussian PDFs is a Gaussian\n    function, not Gaussian PDF. It is, however, proportional to a Gaussian\n    PDF, so it is safe to treat the output as a PDF for any filter using\n    Bayes equation, which normalizes the result anyway.\n\n    Parameters\n    ----------\n    mean1 : scalar\n         mean of first Gaussian\n\n    var1 : scalar\n         variance of first Gaussian\n\n    mean2 : scalar\n         mean of second Gaussian\n\n    var2 : scalar\n         variance of second Gaussian\n\n    Returns\n    -------\n    mean : scalar\n        mean of product\n\n    var : scalar\n        variance of product\n\n    Examples\n    --------\n    &gt;&gt;&gt; mul(1, 2, 3, 4)\n    (1.6666666666666667, 1.3333333333333333)\n\n    References\n    ----------\n    Bromily. \"Products and Convolutions of Gaussian Probability Functions\",\n    Tina Memo No. 2003-003.\n    http://www.tina-vision.net/docs/memos/2003-003.pdf\n    \"\"\"\n\n    mean = (var1 * mean2 + var2 * mean1) / (var1 + var2)\n    var = 1 / (1 / var1 + 1 / var2)\n    return (mean, var)\n</code></pre>"},{"location":"api/stats/#add-gaussians","title":"Add Gaussians","text":""},{"location":"api/stats/#bayesian_filters.stats.stats.add","title":"<code>add(mean1, var1, mean2, var2)</code>","text":"<p>Add the Gaussians (mean1, var1) with (mean2, var2) and return the results as a tuple (mean,var).</p> <p>var1 and var2 are variances - sigma squared in the usual parlance.</p> Source code in <code>bayesian_filters/stats/stats.py</code> <pre><code>def add(mean1, var1, mean2, var2):\n    \"\"\"\n    Add the Gaussians (mean1, var1) with (mean2, var2) and return the\n    results as a tuple (mean,var).\n\n    var1 and var2 are variances - sigma squared in the usual parlance.\n    \"\"\"\n\n    return (mean1 + mean2, var1 + var2)\n</code></pre>"},{"location":"api/stats/#log-likelihood","title":"Log Likelihood","text":""},{"location":"api/stats/#bayesian_filters.stats.stats.log_likelihood","title":"<code>log_likelihood(z, x, P, H, R)</code>","text":"<p>Returns log-likelihood of the measurement z given the Gaussian posterior (x, P) using measurement function H and measurement covariance error R</p> Source code in <code>bayesian_filters/stats/stats.py</code> <pre><code>def log_likelihood(z, x, P, H, R):\n    \"\"\"\n    Returns log-likelihood of the measurement z given the Gaussian\n    posterior (x, P) using measurement function H and measurement\n    covariance error R\n    \"\"\"\n    S = np.dot(H, np.dot(P, H.T)) + R\n    return logpdf(z, np.dot(H, x), S)\n</code></pre>"},{"location":"api/stats/#likelihood","title":"Likelihood","text":""},{"location":"api/stats/#bayesian_filters.stats.stats.likelihood","title":"<code>likelihood(z, x, P, H, R)</code>","text":"<p>Returns likelihood of the measurement z given the Gaussian posterior (x, P) using measurement function H and measurement covariance error R</p> Source code in <code>bayesian_filters/stats/stats.py</code> <pre><code>def likelihood(z, x, P, H, R):\n    \"\"\"\n    Returns likelihood of the measurement z given the Gaussian\n    posterior (x, P) using measurement function H and measurement\n    covariance error R\n    \"\"\"\n    return np.exp(log_likelihood(z, x, P, H, R))\n</code></pre>"},{"location":"api/stats/#log-pdf","title":"Log PDF","text":""},{"location":"api/stats/#bayesian_filters.stats.stats.logpdf","title":"<code>logpdf(x, mean=None, cov=1, allow_singular=True)</code>","text":"<p>Computes the log of the probability density function of the normal N(mean, cov) for the data x. The normal may be univariate or multivariate.</p> <p>Wrapper for older versions of scipy.multivariate_normal.logpdf which don't support support the allow_singular keyword prior to verion 0.15.0.</p> <p>If it is not supported, and cov is singular or not PSD you may get an exception.</p> <p><code>x</code> and <code>mean</code> may be column vectors, row vectors, or lists.</p> Source code in <code>bayesian_filters/stats/stats.py</code> <pre><code>def logpdf(x, mean=None, cov=1, allow_singular=True):\n    \"\"\"\n    Computes the log of the probability density function of the normal\n    N(mean, cov) for the data x. The normal may be univariate or multivariate.\n\n    Wrapper for older versions of scipy.multivariate_normal.logpdf which\n    don't support support the allow_singular keyword prior to verion 0.15.0.\n\n    If it is not supported, and cov is singular or not PSD you may get\n    an exception.\n\n    `x` and `mean` may be column vectors, row vectors, or lists.\n    \"\"\"\n\n    if mean is not None:\n        flat_mean = np.asarray(mean).flatten()\n    else:\n        flat_mean = None\n\n    flat_x = np.asarray(x).flatten()\n\n    if _support_singular:\n        return multivariate_normal.logpdf(flat_x, flat_mean, cov, allow_singular)\n    return multivariate_normal.logpdf(flat_x, flat_mean, cov)\n</code></pre>"},{"location":"api/stats/#multivariate-gaussian","title":"Multivariate Gaussian","text":""},{"location":"api/stats/#bayesian_filters.stats.stats.multivariate_gaussian","title":"<code>multivariate_gaussian(x, mu, cov)</code>","text":"<p>This is designed to replace scipy.stats.multivariate_normal which is not available before version 0.14. You may either pass in a multivariate set of data:</p> <p>.. code-block:: Python</p> <p>multivariate_gaussian (array([1,1]), array([3,4]), eye(2)*1.4)    multivariate_gaussian (array([1,1,1]), array([3,4,5]), 1.4)</p> <p>or unidimensional data:</p> <p>.. code-block:: Python</p> <p>multivariate_gaussian(1, 3, 1.4)</p> <p>In the multivariate case if cov is a scalar it is interpreted as eye(n)*cov</p> <p>The function gaussian() implements the 1D (univariate)case, and is much faster than this function.</p> <p>equivalent calls:</p> <p>.. code-block:: Python</p> <p>multivariate_gaussian(1, 2, 3)   scipy.stats.multivariate_normal(2,3).pdf(1)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float, or np.array-like</code> <p>Value to compute the probability for. May be a scalar if univariate, or any type that can be converted to an np.array (list, tuple, etc). np.array is best for speed.</p> required <code>mu</code> <code> float, or np.array-like</code> <p>mean for the Gaussian . May be a scalar if univariate,  or any type that can be converted to an np.array (list, tuple, etc).np.array is best for speed.</p> required <code>cov</code> <code> float, or np.array-like</code> <p>Covariance for the Gaussian . May be a scalar if univariate,  or any type that can be converted to an np.array (list, tuple, etc).np.array is best for speed.</p> required <p>Returns:</p> Name Type Description <code>probability</code> <code>float</code> <p>probability for x for the Gaussian (mu,cov)</p> Source code in <code>bayesian_filters/stats/stats.py</code> <pre><code>def multivariate_gaussian(x, mu, cov):\n    \"\"\"\n    This is designed to replace scipy.stats.multivariate_normal\n    which is not available before version 0.14. You may either pass in a\n    multivariate set of data:\n\n    .. code-block:: Python\n\n       multivariate_gaussian (array([1,1]), array([3,4]), eye(2)*1.4)\n       multivariate_gaussian (array([1,1,1]), array([3,4,5]), 1.4)\n\n    or unidimensional data:\n\n    .. code-block:: Python\n\n       multivariate_gaussian(1, 3, 1.4)\n\n    In the multivariate case if cov is a scalar it is interpreted as eye(n)*cov\n\n    The function gaussian() implements the 1D (univariate)case, and is much\n    faster than this function.\n\n    equivalent calls:\n\n    .. code-block:: Python\n\n      multivariate_gaussian(1, 2, 3)\n      scipy.stats.multivariate_normal(2,3).pdf(1)\n\n\n    Parameters\n    ----------\n\n    x : float, or np.array-like\n        Value to compute the probability for. May be a scalar if univariate,\n        or any type that can be converted to an np.array (list, tuple, etc).\n        np.array is best for speed.\n\n    mu :  float, or np.array-like\n        mean for the Gaussian . May be a scalar if univariate,  or any type\n        that can be converted to an np.array (list, tuple, etc).np.array is\n        best for speed.\n\n    cov :  float, or np.array-like\n        Covariance for the Gaussian . May be a scalar if univariate,  or any\n        type that can be converted to an np.array (list, tuple, etc).np.array is\n        best for speed.\n\n    Returns\n    -------\n\n    probability : float\n        probability for x for the Gaussian (mu,cov)\n    \"\"\"\n\n    warnings.warn(\n        (\n            \"This was implemented before SciPy version 0.14, which implemented \"\n            \"scipy.stats.multivariate_normal. This function will be removed in \"\n            \"a future release of FilterPy\"\n        ),\n        DeprecationWarning,\n    )\n\n    # force all to numpy.array type, and flatten in case they are vectors\n    x = np.asarray(x, dtype=None).flatten()\n    mu = np.asarray(mu, dtype=None).flatten()\n\n    nx = len(mu)\n    cov = _to_cov(cov, nx)\n\n    norm_coeff = nx * math.log(2 * math.pi) + np.linalg.slogdet(cov)[1]\n\n    err = x - mu\n    if sp.issparse(cov):\n        numerator = spln.spsolve(cov, err).T.dot(err)\n    else:\n        numerator = np.linalg.solve(cov, err).T.dot(err)\n\n    return math.exp(-0.5 * (norm_coeff + numerator))\n</code></pre>"},{"location":"api/stats/#multivariate-multiply","title":"Multivariate Multiply","text":""},{"location":"api/stats/#bayesian_filters.stats.stats.multivariate_multiply","title":"<code>multivariate_multiply(m1, c1, m2, c2)</code>","text":"<p>Multiplies the two multivariate Gaussians together and returns the results as the tuple (mean, covariance).</p> <p>Examples:</p> <p>.. code-block:: Python</p> <pre><code>m, c = multivariate_multiply([7.0, 2], [[1.0, 2.0], [2.0, 1.0]],\n                             [3.2, 0], [[8.0, 1.1], [1.1,8.0]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>m1</code> <code>array - like</code> <p>Mean of first Gaussian. Must be convertable to an 1D array via numpy.asarray(), For example 6, [6], [6, 5], np.array([3, 4, 5, 6]) are all valid.</p> required <code>c1</code> <code>matrix - like</code> <p>Covariance of first Gaussian. Must be convertable to an 2D array via numpy.asarray().</p> <p>m2 : array-like Mean of second Gaussian. Must be convertable to an 1D array via numpy.asarray(), For example 6, [6], [6, 5], np.array([3, 4, 5, 6]) are all valid.</p> required <code>c2</code> <code>matrix - like</code> <p>Covariance of second Gaussian. Must be convertable to an 2D array via numpy.asarray().</p> required <p>Returns:</p> Name Type Description <code>m</code> <code>ndarray</code> <p>mean of the result</p> <code>c</code> <code>ndarray</code> <p>covariance of the result</p> Source code in <code>bayesian_filters/stats/stats.py</code> <pre><code>def multivariate_multiply(m1, c1, m2, c2):\n    \"\"\"\n    Multiplies the two multivariate Gaussians together and returns the\n    results as the tuple (mean, covariance).\n\n    Examples\n    --------\n\n    .. code-block:: Python\n\n        m, c = multivariate_multiply([7.0, 2], [[1.0, 2.0], [2.0, 1.0]],\n                                     [3.2, 0], [[8.0, 1.1], [1.1,8.0]])\n\n    Parameters\n    ----------\n\n    m1 : array-like\n        Mean of first Gaussian. Must be convertable to an 1D array via\n        numpy.asarray(), For example 6, [6], [6, 5], np.array([3, 4, 5, 6])\n        are all valid.\n\n    c1 : matrix-like\n        Covariance of first Gaussian. Must be convertable to an 2D array via\n        numpy.asarray().\n\n     m2 : array-like\n        Mean of second Gaussian. Must be convertable to an 1D array via\n        numpy.asarray(), For example 6, [6], [6, 5], np.array([3, 4, 5, 6])\n        are all valid.\n\n    c2 : matrix-like\n        Covariance of second Gaussian. Must be convertable to an 2D array via\n        numpy.asarray().\n\n    Returns\n    -------\n\n    m : ndarray\n        mean of the result\n\n    c : ndarray\n        covariance of the result\n    \"\"\"\n\n    C1 = np.asarray(c1)\n    C2 = np.asarray(c2)\n    M1 = np.asarray(m1)\n    M2 = np.asarray(m2)\n\n    sum_inv = np.linalg.inv(C1 + C2)\n    C3 = np.dot(C1, sum_inv).dot(C2)\n\n    M3 = np.dot(C2, sum_inv).dot(M1) + np.dot(C1, sum_inv).dot(M2)\n\n    return M3, C3\n</code></pre>"},{"location":"api/stats/#plot-gaussian-cdf","title":"Plot Gaussian CDF","text":""},{"location":"api/stats/#bayesian_filters.stats.stats.plot_gaussian_cdf","title":"<code>plot_gaussian_cdf(mean=0.0, variance=1.0, ax=None, xlim=None, ylim=(0.0, 1.0), xlabel=None, ylabel=None, label=None)</code>","text":"<p>Plots a normal distribution CDF with the given mean and variance. x-axis contains the mean, the y-axis shows the cumulative probability.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>scalar</code> <p>mean for the normal distribution.</p> <code>0.</code> <code>variance</code> <code>scalar</code> <p>variance for the normal distribution.</p> <code>0.</code> <code>ax</code> <code>matplotlib axes object</code> <p>If provided, the axes to draw on, otherwise plt.gca() is used.</p> <code>None</code> <code>xlim</code> <p>specify the limits for the x or y axis as tuple (low,high). If not specified, limits will be automatically chosen to be 'nice'</p> <code>None</code> <code>ylim</code> <p>specify the limits for the x or y axis as tuple (low,high). If not specified, limits will be automatically chosen to be 'nice'</p> <code>None</code> <code>xlabel</code> <code>(str, optional)</code> <p>label for the x-axis</p> <code>None</code> <code>ylabel</code> <code>str</code> <p>label for the y-axis</p> <code>None</code> <code>label</code> <code>str</code> <p>label for the legend</p> <code>None</code> <p>Returns:</p> Type Description <code>    axis of plot</code> Source code in <code>bayesian_filters/stats/stats.py</code> <pre><code>def plot_gaussian_cdf(\n    mean=0.0,\n    variance=1.0,\n    ax=None,\n    xlim=None,\n    ylim=(0.0, 1.0),\n    xlabel=None,\n    ylabel=None,\n    label=None,\n):\n    \"\"\"\n    Plots a normal distribution CDF with the given mean and variance.\n    x-axis contains the mean, the y-axis shows the cumulative probability.\n\n    Parameters\n    ----------\n\n    mean : scalar, default 0.\n        mean for the normal distribution.\n\n    variance : scalar, default 0.\n        variance for the normal distribution.\n\n    ax : matplotlib axes object, optional\n        If provided, the axes to draw on, otherwise plt.gca() is used.\n\n    xlim, ylim: (float,float), optional\n        specify the limits for the x or y axis as tuple (low,high).\n        If not specified, limits will be automatically chosen to be 'nice'\n\n    xlabel : str,optional\n        label for the x-axis\n\n    ylabel : str, optional\n        label for the y-axis\n\n    label : str, optional\n        label for the legend\n\n    Returns\n    -------\n        axis of plot\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    if ax is None:\n        ax = plt.gca()\n\n    sigma = math.sqrt(variance)\n    n = norm(mean, sigma)\n    if xlim is None:\n        xlim = [n.ppf(0.001), n.ppf(0.999)]\n\n    xs = np.arange(xlim[0], xlim[1], (xlim[1] - xlim[0]) / 1000.0)\n    cdf = n.cdf(xs)\n    ax.plot(xs, cdf, label=label)\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    return ax\n</code></pre>"},{"location":"api/stats/#plot-gaussian-pdf","title":"Plot Gaussian PDF","text":""},{"location":"api/stats/#bayesian_filters.stats.stats.plot_gaussian_pdf","title":"<code>plot_gaussian_pdf(mean=0.0, variance=1.0, std=None, ax=None, mean_line=False, xlim=None, ylim=None, xlabel=None, ylabel=None, label=None)</code>","text":"<p>Plots a normal distribution PDF with the given mean and variance. x-axis contains the mean, the y-axis shows the probability density.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>scalar</code> <p>mean for the normal distribution.</p> <code>0.</code> <code>variance</code> <code>scalar</code> <p>variance for the normal distribution.</p> <code>1., optional</code> <code>std</code> <p>standard deviation of the normal distribution. Use instead of <code>variance</code> if desired</p> <code>None</code> <code>ax</code> <code>matplotlib axes object</code> <p>If provided, the axes to draw on, otherwise plt.gca() is used.</p> <code>None</code> <code>mean_line</code> <code>boolean</code> <p>draws a line at x=mean</p> <code>False</code> <code>xlim</code> <p>specify the limits for the x or y axis as tuple (low,high). If not specified, limits will be automatically chosen to be 'nice'</p> <code>None</code> <code>ylim</code> <p>specify the limits for the x or y axis as tuple (low,high). If not specified, limits will be automatically chosen to be 'nice'</p> <code>None</code> <code>xlabel</code> <code>(str, optional)</code> <p>label for the x-axis</p> <code>None</code> <code>ylabel</code> <code>str</code> <p>label for the y-axis</p> <code>None</code> <code>label</code> <code>str</code> <p>label for the legend</p> <code>None</code> <p>Returns:</p> Type Description <code>    axis of plot</code> Source code in <code>bayesian_filters/stats/stats.py</code> <pre><code>def plot_gaussian_pdf(\n    mean=0.0,\n    variance=1.0,\n    std=None,\n    ax=None,\n    mean_line=False,\n    xlim=None,\n    ylim=None,\n    xlabel=None,\n    ylabel=None,\n    label=None,\n):\n    \"\"\"\n    Plots a normal distribution PDF with the given mean and variance.\n    x-axis contains the mean, the y-axis shows the probability density.\n\n    Parameters\n    ----------\n\n    mean : scalar, default 0.\n        mean for the normal distribution.\n\n    variance : scalar, default 1., optional\n        variance for the normal distribution.\n\n    std: scalar, default=None, optional\n        standard deviation of the normal distribution. Use instead of\n        `variance` if desired\n\n    ax : matplotlib axes object, optional\n        If provided, the axes to draw on, otherwise plt.gca() is used.\n\n    mean_line : boolean\n        draws a line at x=mean\n\n    xlim, ylim: (float,float), optional\n        specify the limits for the x or y axis as tuple (low,high).\n        If not specified, limits will be automatically chosen to be 'nice'\n\n    xlabel : str,optional\n        label for the x-axis\n\n    ylabel : str, optional\n        label for the y-axis\n\n    label : str, optional\n        label for the legend\n\n    Returns\n    -------\n        axis of plot\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    if ax is None:\n        ax = plt.gca()\n\n    if variance is not None and std is not None:\n        raise ValueError(\"Specify only one of variance and std\")\n\n    if variance is None and std is None:\n        raise ValueError(\"Specify variance or std\")\n\n    if variance is not None:\n        std = math.sqrt(variance)\n\n    n = norm(mean, std)\n\n    if xlim is None:\n        xlim = [n.ppf(0.001), n.ppf(0.999)]\n\n    xs = np.arange(xlim[0], xlim[1], (xlim[1] - xlim[0]) / 1000.0)\n    ax.plot(xs, n.pdf(xs), label=label)\n    ax.set_xlim(xlim)\n\n    if ylim is not None:\n        ax.set_ylim(ylim)\n\n    if mean_line:\n        plt.axvline(mean)\n\n    if xlabel is not None:\n        ax.set_xlabel(xlabel)\n    if ylabel is not None:\n        ax.set_ylabel(ylabel)\n    return ax\n</code></pre>"},{"location":"api/stats/#plot-discrete-cdf","title":"Plot Discrete CDF","text":""},{"location":"api/stats/#bayesian_filters.stats.stats.plot_discrete_cdf","title":"<code>plot_discrete_cdf(xs, ys, ax=None, xlabel=None, ylabel=None, label=None)</code>","text":"<p>Plots a normal distribution CDF with the given mean and variance. x-axis contains the mean, the y-axis shows the cumulative probability.</p> <p>Parameters:</p> Name Type Description Default <code>xs</code> <code>list-like of scalars</code> <p>x values corresponding to the values in <code>y</code>s. Can be <code>None</code>, in which case range(len(ys)) will be used.</p> required <code>ys</code> <code>list-like of scalars</code> <p>list of probabilities to be plotted which should sum to 1.</p> required <code>ax</code> <code>matplotlib axes object</code> <p>If provided, the axes to draw on, otherwise plt.gca() is used.</p> <code>None</code> <code>xlim</code> <p>specify the limits for the x or y axis as tuple (low,high). If not specified, limits will be automatically chosen to be 'nice'</p> required <code>ylim</code> <p>specify the limits for the x or y axis as tuple (low,high). If not specified, limits will be automatically chosen to be 'nice'</p> required <code>xlabel</code> <code>(str, optional)</code> <p>label for the x-axis</p> <code>None</code> <code>ylabel</code> <code>str</code> <p>label for the y-axis</p> <code>None</code> <code>label</code> <code>str</code> <p>label for the legend</p> <code>None</code> <p>Returns:</p> Type Description <code>    axis of plot</code> Source code in <code>bayesian_filters/stats/stats.py</code> <pre><code>def plot_discrete_cdf(xs, ys, ax=None, xlabel=None, ylabel=None, label=None):\n    \"\"\"\n    Plots a normal distribution CDF with the given mean and variance.\n    x-axis contains the mean, the y-axis shows the cumulative probability.\n\n    Parameters\n    ----------\n\n    xs : list-like of scalars\n        x values corresponding to the values in `y`s. Can be `None`, in which\n        case range(len(ys)) will be used.\n\n    ys : list-like of scalars\n        list of probabilities to be plotted which should sum to 1.\n\n    ax : matplotlib axes object, optional\n        If provided, the axes to draw on, otherwise plt.gca() is used.\n\n    xlim, ylim: (float,float), optional\n        specify the limits for the x or y axis as tuple (low,high).\n        If not specified, limits will be automatically chosen to be 'nice'\n\n    xlabel : str,optional\n        label for the x-axis\n\n    ylabel : str, optional\n        label for the y-axis\n\n    label : str, optional\n        label for the legend\n\n    Returns\n    -------\n        axis of plot\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    if ax is None:\n        ax = plt.gca()\n\n    if xs is None:\n        xs = range(len(ys))\n    ys = np.cumsum(ys)\n    ax.plot(xs, ys, label=label)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    return ax\n</code></pre>"},{"location":"api/stats/#plot-gaussian","title":"Plot Gaussian","text":""},{"location":"api/stats/#bayesian_filters.stats.stats.plot_gaussian","title":"<code>plot_gaussian(mean=0.0, variance=1.0, ax=None, mean_line=False, xlim=None, ylim=None, xlabel=None, ylabel=None, label=None)</code>","text":"<p>DEPRECATED. Use plot_gaussian_pdf() instead. This is poorly named, as there are multiple ways to plot a Gaussian.</p> Source code in <code>bayesian_filters/stats/stats.py</code> <pre><code>def plot_gaussian(\n    mean=0.0,\n    variance=1.0,\n    ax=None,\n    mean_line=False,\n    xlim=None,\n    ylim=None,\n    xlabel=None,\n    ylabel=None,\n    label=None,\n):\n    \"\"\"\n    DEPRECATED. Use plot_gaussian_pdf() instead. This is poorly named, as\n    there are multiple ways to plot a Gaussian.\n    \"\"\"\n\n    warnings.warn(\n        \"This function is deprecated. It is poorly named. \"\n        \"A Gaussian can be plotted as a PDF or CDF. This \"\n        \"plots a PDF. Use plot_gaussian_pdf() instead,\",\n        DeprecationWarning,\n    )\n    return plot_gaussian_pdf(mean, variance, ax, mean_line, xlim, ylim, xlabel, ylabel, label)\n</code></pre>"},{"location":"api/stats/#covariance-ellipse","title":"Covariance Ellipse","text":""},{"location":"api/stats/#bayesian_filters.stats.stats.covariance_ellipse","title":"<code>covariance_ellipse(P, deviations=1)</code>","text":"<p>Returns a tuple defining the ellipse representing the 2 dimensional covariance matrix P.</p> <p>Parameters:</p> Name Type Description Default <code>P</code> <code>nd.array shape (2,2)</code> <p>covariance matrix</p> required <code>deviations</code> <code>int (optional</code> <code>= 1)</code> <code>Returns</code> required Source code in <code>bayesian_filters/stats/stats.py</code> <pre><code>def covariance_ellipse(P, deviations=1):\n    \"\"\"\n    Returns a tuple defining the ellipse representing the 2 dimensional\n    covariance matrix P.\n\n    Parameters\n    ----------\n\n    P : nd.array shape (2,2)\n        covariance matrix\n\n    deviations : int (optional, default = 1)\n        # of standard deviations. Default is 1.\n\n    Returns (angle_radians, width_radius, height_radius)\n    \"\"\"\n\n    U, s, _ = linalg.svd(P)\n    orientation = math.atan2(U[1, 0], U[0, 0])\n    width = deviations * math.sqrt(s[0])\n    height = deviations * math.sqrt(s[1])\n\n    if height &gt; width:\n        raise ValueError(\"width must be greater than height\")\n\n    return (orientation, width, height)\n</code></pre>"},{"location":"api/stats/#bayesian_filters.stats.stats.covariance_ellipse--of-standard-deviations-default-is-1","title":"of standard deviations. Default is 1.","text":""},{"location":"api/stats/#plot-covariance-ellipse","title":"Plot Covariance Ellipse","text":""},{"location":"api/stats/#bayesian_filters.stats.stats.plot_covariance_ellipse","title":"<code>plot_covariance_ellipse(mean, cov=None, variance=1.0, std=None, ellipse=None, title=None, axis_equal=True, show_semiaxis=False, facecolor=None, edgecolor=None, fc='none', ec='#004080', alpha=1.0, xlim=None, ylim=None, ls='solid')</code>","text":"<p>Deprecated function to plot a covariance ellipse. Use plot_covariance instead.</p> See Also <p>plot_covariance</p> Source code in <code>bayesian_filters/stats/stats.py</code> <pre><code>def plot_covariance_ellipse(\n    mean,\n    cov=None,\n    variance=1.0,\n    std=None,\n    ellipse=None,\n    title=None,\n    axis_equal=True,\n    show_semiaxis=False,\n    facecolor=None,\n    edgecolor=None,\n    fc=\"none\",\n    ec=\"#004080\",\n    alpha=1.0,\n    xlim=None,\n    ylim=None,\n    ls=\"solid\",\n):\n    \"\"\"\n    Deprecated function to plot a covariance ellipse. Use plot_covariance\n    instead.\n\n    See Also\n    --------\n\n    plot_covariance\n    \"\"\"\n\n    warnings.warn(\"deprecated, use plot_covariance instead\", DeprecationWarning)\n    plot_covariance(\n        mean=mean,\n        cov=cov,\n        variance=variance,\n        std=std,\n        ellipse=ellipse,\n        title=title,\n        axis_equal=axis_equal,\n        show_semiaxis=show_semiaxis,\n        facecolor=facecolor,\n        edgecolor=edgecolor,\n        fc=fc,\n        ec=ec,\n        alpha=alpha,\n        xlim=xlim,\n        ylim=ylim,\n        ls=ls,\n    )\n</code></pre>"},{"location":"api/stats/#normal-cdf","title":"Normal CDF","text":""},{"location":"api/stats/#bayesian_filters.stats.stats.norm_cdf","title":"<code>norm_cdf(x_range, mu, var=1, std=None)</code>","text":"<p>Computes the probability that a Gaussian distribution lies within a range of values.</p> <p>Parameters:</p> Name Type Description Default <code>x_range</code> <code>(float, float)</code> <p>tuple of range to compute probability for</p> required <code>mu</code> <code>float</code> <p>mean of the Gaussian</p> required <code>var</code> <code>float</code> <p>variance of the Gaussian. Ignored if <code>std</code> is provided</p> <code>1</code> <code>std</code> <code>float</code> <p>standard deviation of the Gaussian. This overrides the <code>var</code> parameter</p> <code>None</code> <p>Returns:</p> Name Type Description <code>probability</code> <code>float</code> <p>probability that Gaussian is within x_range. E.g. .1 means 10%.</p> Source code in <code>bayesian_filters/stats/stats.py</code> <pre><code>def norm_cdf(x_range, mu, var=1, std=None):\n    \"\"\"\n    Computes the probability that a Gaussian distribution lies\n    within a range of values.\n\n    Parameters\n    ----------\n\n    x_range : (float, float)\n        tuple of range to compute probability for\n\n    mu : float\n        mean of the Gaussian\n\n    var : float, optional\n        variance of the Gaussian. Ignored if `std` is provided\n\n    std : float, optional\n        standard deviation of the Gaussian. This overrides the `var` parameter\n\n    Returns\n    -------\n\n    probability : float\n        probability that Gaussian is within x_range. E.g. .1 means 10%.\n    \"\"\"\n\n    if std is None:\n        std = math.sqrt(var)\n    return abs(norm.cdf(x_range[0], loc=mu, scale=std) - norm.cdf(x_range[1], loc=mu, scale=std))\n</code></pre>"},{"location":"api/stats/#random-student-t","title":"Random Student-t","text":""},{"location":"api/stats/#bayesian_filters.stats.stats.rand_student_t","title":"<code>rand_student_t(df, mu=0, std=1)</code>","text":"<p>return random number distributed by student's t distribution with <code>df</code> degrees of freedom with the specified mean and standard deviation.</p> Source code in <code>bayesian_filters/stats/stats.py</code> <pre><code>def rand_student_t(df, mu=0, std=1):\n    \"\"\"\n    return random number distributed by student's t distribution with\n    `df` degrees of freedom with the specified mean and standard deviation.\n    \"\"\"\n\n    x = random.gauss(0, std)\n    y = 2.0 * random.gammavariate(0.5 * df, 2.0)\n    return x / (math.sqrt(y / df)) + mu\n</code></pre>"},{"location":"api/stats/#nees","title":"NEES","text":""},{"location":"api/stats/#bayesian_filters.stats.stats.NEES","title":"<code>NEES(xs, est_xs, ps)</code>","text":"<p>Computes the normalized estimated error squared (NEES) test on a sequence of estimates. The estimates are optimal if the mean error is zero and the covariance matches the Kalman filter's covariance. If this holds, then the mean of the NEES should be equal to or less than the dimension of x.</p> <p>Examples:</p> <p>.. code-block: Python</p> <pre><code>xs = ground_truth()\nest_xs, ps, _, _ = kf.batch_filter(zs)\nNEES(xs, est_xs, ps)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>xs</code> <code>list - like</code> <p>sequence of true values for the state x</p> required <code>est_xs</code> <code>list - like</code> <p>sequence of estimates from an estimator (such as Kalman filter)</p> required <code>ps</code> <code>list - like</code> <p>sequence of covariance matrices from the estimator</p> required <p>Returns:</p> Name Type Description <code>errs</code> <code>list of floats</code> <p>list of NEES computed for each estimate</p> Source code in <code>bayesian_filters/stats/stats.py</code> <pre><code>def NEES(xs, est_xs, ps):\n    \"\"\"\n    Computes the normalized estimated error squared (NEES) test on a sequence\n    of estimates. The estimates are optimal if the mean error is zero and\n    the covariance matches the Kalman filter's covariance. If this holds,\n    then the mean of the NEES should be equal to or less than the dimension\n    of x.\n\n    Examples\n    --------\n\n    .. code-block: Python\n\n        xs = ground_truth()\n        est_xs, ps, _, _ = kf.batch_filter(zs)\n        NEES(xs, est_xs, ps)\n\n    Parameters\n    ----------\n\n    xs : list-like\n        sequence of true values for the state x\n\n    est_xs : list-like\n        sequence of estimates from an estimator (such as Kalman filter)\n\n    ps : list-like\n        sequence of covariance matrices from the estimator\n\n    Returns\n    -------\n\n    errs : list of floats\n        list of NEES computed for each estimate\n\n    \"\"\"\n\n    est_err = xs - est_xs\n    errs = []\n    for x, p in zip(est_err, ps):\n        errs.append(np.dot(x.T, linalg.inv(p)).dot(x))\n    return errs\n</code></pre>"},{"location":"api/stats/#mahalanobis-distance","title":"Mahalanobis Distance","text":""},{"location":"api/stats/#bayesian_filters.stats.stats.mahalanobis","title":"<code>mahalanobis(x, mean, cov)</code>","text":"<p>Computes the Mahalanobis distance between the state vector x from the Gaussian <code>mean</code> with covariance <code>cov</code>. This can be thought as the number of standard deviations x is from the mean, i.e. a return value of 3 means x is 3 std from mean.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>(N,) array_like, or float</code> <p>Input state vector</p> required <code>mean</code> <code>(N,) array_like, or float</code> <p>mean of multivariate Gaussian</p> required <code>cov</code> <code>(N, N) array_like  or float</code> <p>covariance of the multivariate Gaussian</p> required <p>Returns:</p> Name Type Description <code>mahalanobis</code> <code>double</code> <p>The Mahalanobis distance between vectors <code>x</code> and <code>mean</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mahalanobis(x=3., mean=3.5, cov=4.**2) # univariate case\n0.125\n</code></pre> <pre><code>&gt;&gt;&gt; mahalanobis(x=3., mean=6, cov=1) # univariate, 3 std away\n3.0\n</code></pre> <pre><code>&gt;&gt;&gt; mahalanobis([1., 2], [1.1, 3.5], [[1., .1],[.1, 13]])\n0.42533327058913922\n</code></pre> Source code in <code>bayesian_filters/stats/stats.py</code> <pre><code>def mahalanobis(x, mean, cov):\n    \"\"\"\n    Computes the Mahalanobis distance between the state vector x from the\n    Gaussian `mean` with covariance `cov`. This can be thought as the number\n    of standard deviations x is from the mean, i.e. a return value of 3 means\n    x is 3 std from mean.\n\n    Parameters\n    ----------\n    x : (N,) array_like, or float\n        Input state vector\n\n    mean : (N,) array_like, or float\n        mean of multivariate Gaussian\n\n    cov : (N, N) array_like  or float\n        covariance of the multivariate Gaussian\n\n    Returns\n    -------\n    mahalanobis : double\n        The Mahalanobis distance between vectors `x` and `mean`\n\n    Examples\n    --------\n    &gt;&gt;&gt; mahalanobis(x=3., mean=3.5, cov=4.**2) # univariate case\n    0.125\n\n    &gt;&gt;&gt; mahalanobis(x=3., mean=6, cov=1) # univariate, 3 std away\n    3.0\n\n    &gt;&gt;&gt; mahalanobis([1., 2], [1.1, 3.5], [[1., .1],[.1, 13]])\n    0.42533327058913922\n    \"\"\"\n\n    x = _validate_vector(x)\n    mean = _validate_vector(mean)\n\n    if x.shape != mean.shape:\n        raise ValueError(\"length of input vectors must be the same\")\n\n    y = x - mean\n    S = np.atleast_2d(cov)\n\n    dist = float(np.dot(np.dot(y.T, inv(S)), y))\n    return math.sqrt(dist)\n</code></pre>"},{"location":"filters/ensemble-kalman-filter/","title":"Ensemble Kalman Filter","text":""},{"location":"filters/ensemble-kalman-filter/#ensemblekalmanfilter","title":"EnsembleKalmanFilter","text":""},{"location":"filters/ensemble-kalman-filter/#introduction-and-overview","title":"Introduction and Overview","text":"<p>This implements the Ensemble Kalman filter.</p>"},{"location":"filters/ensemble-kalman-filter/#bayesian_filters.kalman.ensemble_kalman_filter.EnsembleKalmanFilter","title":"<code>EnsembleKalmanFilter</code>","text":"<p>               Bases: <code>object</code></p> <p>This implements the ensemble Kalman filter (EnKF). The EnKF uses an ensemble of hundreds to thousands of state vectors that are randomly sampled around the estimate, and adds perturbations at each update and predict step. It is useful for extremely large systems such as found in hydrophysics. As such, this class is admittedly a toy as it is far too slow with large N.</p> <p>There are many versions of this sort of this filter. This formulation is due to Crassidis and Junkins [1]. It works with both linear and nonlinear systems.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array(dim_x)</code> <p>state mean</p> required <code>P</code> <code>array((dim_x, dim_x))</code> <p>covariance of the state</p> required <code>dim_z</code> <code>int</code> <p>Number of of measurement inputs. For example, if the sensor provides you with position in (x,y), dim_z would be 2.</p> required <code>dt</code> <code>float</code> <p>time step in seconds</p> required <code>N</code> <code>int</code> <p>number of sigma points (ensembles). Must be greater than 1.</p> required <code>hx</code> <code>function hx(x)</code> <p>Measurement function. May be linear or nonlinear - converts state x into a measurement. Return must be an np.array of the same dimensionality as the measurement vector.</p> required <code>fx</code> <code>function fx(x, dt)</code> <p>State transition function. May be linear or nonlinear. Projects state x into the next time period. Returns the projected state x.</p> required <p>Attributes:</p> Name Type Description <code>x</code> <code>array(dim_x, 1)</code> <p>State estimate</p> <code>P</code> <code>array(dim_x, dim_x)</code> <p>State covariance matrix</p> <code>x_prior</code> <code>array(dim_x, 1)</code> <p>Prior (predicted) state estimate. The _prior and _post attributes are for convienence; they store the  prior and posterior of the current epoch. Read Only.</p> <code>P_prior</code> <code>array(dim_x, dim_x)</code> <p>Prior (predicted) state covariance matrix. Read Only.</p> <code>x_post</code> <code>array(dim_x, 1)</code> <p>Posterior (updated) state estimate. Read Only.</p> <code>P_post</code> <code>array(dim_x, dim_x)</code> <p>Posterior (updated) state covariance matrix. Read Only.</p> <code>z</code> <code>array</code> <p>Last measurement used in update(). Read only.</p> <code>R</code> <code>array(dim_z, dim_z)</code> <p>Measurement noise matrix</p> <code>Q</code> <code>array(dim_x, dim_x)</code> <p>Process noise matrix</p> <code>fx</code> <code>callable(x, dt)</code> <p>State transition function</p> <code>hx</code> <code>callable(x)</code> <p>Measurement function. Convert state <code>x</code> into a measurement</p> <code>inv</code> <code>function, default numpy.linalg.inv</code> <p>If you prefer another inverse function, such as the Moore-Penrose pseudo inverse, set it to that instead: kf.inv = np.linalg.pinv</p> <p>Examples:</p> <p>.. code-block:: Python</p> <pre><code>def hx(x):\n   return np.array([x[0]])\n\nF = np.array([[1., 1.],\n              [0., 1.]])\ndef fx(x, dt):\n    return np.dot(F, x)\n\nx = np.array([0., 1.])\nP = np.eye(2) * 100.\ndt = 0.1\nf = EnsembleKalmanFilter(x=x, P=P, dim_z=1, dt=dt,\n                         N=8, hx=hx, fx=fx)\n\nstd_noise = 3.\nf.R *= std_noise**2\nf.Q = Q_discrete_white_noise(2, dt, .01)\n\nwhile True:\n    z = read_sensor()\n    f.predict()\n    f.update(np.asarray([z]))\n</code></pre> <p>See my book Kalman and Bayesian Filters in Python https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> References <ul> <li>[1] John L Crassidis and John L. Junkins. \"Optimal Estimation of   Dynamic Systems. CRC Press, second edition. 2012. pp, 257-9.</li> </ul> Source code in <code>bayesian_filters/kalman/ensemble_kalman_filter.py</code> <pre><code>class EnsembleKalmanFilter(object):\n    \"\"\"\n    This implements the ensemble Kalman filter (EnKF). The EnKF uses\n    an ensemble of hundreds to thousands of state vectors that are randomly\n    sampled around the estimate, and adds perturbations at each update and\n    predict step. It is useful for extremely large systems such as found\n    in hydrophysics. As such, this class is admittedly a toy as it is far\n    too slow with large N.\n\n    There are many versions of this sort of this filter. This formulation is\n    due to Crassidis and Junkins [1]. It works with both linear and nonlinear\n    systems.\n\n    Parameters\n    ----------\n\n    x : np.array(dim_x)\n        state mean\n\n    P : np.array((dim_x, dim_x))\n        covariance of the state\n\n    dim_z : int\n        Number of of measurement inputs. For example, if the sensor\n        provides you with position in (x,y), dim_z would be 2.\n\n    dt : float\n        time step in seconds\n\n    N : int\n        number of sigma points (ensembles). Must be greater than 1.\n\n\n    hx : function hx(x)\n        Measurement function. May be linear or nonlinear - converts state\n        x into a measurement. Return must be an np.array of the same\n        dimensionality as the measurement vector.\n\n    fx : function fx(x, dt)\n        State transition function. May be linear or nonlinear. Projects\n        state x into the next time period. Returns the projected state x.\n\n\n    Attributes\n    ----------\n    x : numpy.array(dim_x, 1)\n        State estimate\n\n    P : numpy.array(dim_x, dim_x)\n        State covariance matrix\n\n    x_prior : numpy.array(dim_x, 1)\n        Prior (predicted) state estimate. The *_prior and *_post attributes\n        are for convienence; they store the  prior and posterior of the\n        current epoch. Read Only.\n\n    P_prior : numpy.array(dim_x, dim_x)\n        Prior (predicted) state covariance matrix. Read Only.\n\n    x_post : numpy.array(dim_x, 1)\n        Posterior (updated) state estimate. Read Only.\n\n    P_post : numpy.array(dim_x, dim_x)\n        Posterior (updated) state covariance matrix. Read Only.\n\n    z : numpy.array\n        Last measurement used in update(). Read only.\n\n    R : numpy.array(dim_z, dim_z)\n        Measurement noise matrix\n\n    Q : numpy.array(dim_x, dim_x)\n        Process noise matrix\n\n    fx : callable (x, dt)\n        State transition function\n\n    hx : callable (x)\n        Measurement function. Convert state `x` into a measurement\n\n\n    inv : function, default numpy.linalg.inv\n        If you prefer another inverse function, such as the Moore-Penrose\n        pseudo inverse, set it to that instead: kf.inv = np.linalg.pinv\n\n    Examples\n    --------\n\n    .. code-block:: Python\n\n        def hx(x):\n           return np.array([x[0]])\n\n        F = np.array([[1., 1.],\n                      [0., 1.]])\n        def fx(x, dt):\n            return np.dot(F, x)\n\n        x = np.array([0., 1.])\n        P = np.eye(2) * 100.\n        dt = 0.1\n        f = EnsembleKalmanFilter(x=x, P=P, dim_z=1, dt=dt,\n                                 N=8, hx=hx, fx=fx)\n\n        std_noise = 3.\n        f.R *= std_noise**2\n        f.Q = Q_discrete_white_noise(2, dt, .01)\n\n        while True:\n            z = read_sensor()\n            f.predict()\n            f.update(np.asarray([z]))\n\n    See my book Kalman and Bayesian Filters in Python\n    https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n\n    References\n    ----------\n\n    - [1] John L Crassidis and John L. Junkins. \"Optimal Estimation of\n      Dynamic Systems. CRC Press, second edition. 2012. pp, 257-9.\n    \"\"\"\n\n    def __init__(self, x, P, dim_z, dt, N, hx, fx):\n        if dim_z &lt;= 0:\n            raise ValueError(\"dim_z must be greater than zero\")\n\n        if N &lt;= 0:\n            raise ValueError(\"N must be greater than zero\")\n\n        dim_x = len(x)\n        self.dim_x = dim_x\n        self.dim_z = dim_z\n        self.dt = dt\n        self.N = N\n        self.hx = hx\n        self.fx = fx\n        self.K = zeros((dim_x, dim_z))\n        self.z = array([[None] * self.dim_z]).T\n        self.S = zeros((dim_z, dim_z))  # system uncertainty\n        self.SI = zeros((dim_z, dim_z))  # inverse system uncertainty\n\n        self.initialize(x, P)\n        self.Q = eye(dim_x)  # process uncertainty\n        self.R = eye(dim_z)  # state uncertainty\n        self.inv = np.linalg.inv\n\n        # used to create error terms centered at 0 mean for\n        # state and measurement\n        self._mean = zeros(dim_x)\n        self._mean_z = zeros(dim_z)\n\n    def initialize(self, x, P):\n        \"\"\"\n        Initializes the filter with the specified mean and\n        covariance. Only need to call this if you are using the filter\n        to filter more than one set of data; this is called by __init__\n\n        Parameters\n        ----------\n\n        x : np.array(dim_z)\n            state mean\n\n        P : np.array((dim_x, dim_x))\n            covariance of the state\n        \"\"\"\n\n        if x.ndim != 1:\n            raise ValueError(\"x must be a 1D array\")\n\n        self.sigmas = multivariate_normal(mean=x, cov=P, size=self.N)\n        self.x = x\n        self.P = P\n\n        # these will always be a copy of x,P after predict() is called\n        self.x_prior = self.x.copy()\n        self.P_prior = self.P.copy()\n\n        # these will always be a copy of x,P after update() is called\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n\n    def update(self, z, R=None):\n        \"\"\"\n        Add a new measurement (z) to the kalman filter. If z is None, nothing\n        is changed.\n\n        Parameters\n        ----------\n\n        z : np.array\n            measurement for this update.\n\n        R : np.array, scalar, or None\n            Optionally provide R to override the measurement noise for this\n            one call, otherwise self.R will be used.\n        \"\"\"\n\n        if z is None:\n            self.z = array([[None] * self.dim_z]).T\n            self.x_post = self.x.copy()\n            self.P_post = self.P.copy()\n            return\n\n        if R is None:\n            R = self.R\n        if np.isscalar(R):\n            R = eye(self.dim_z) * R\n\n        N = self.N\n        dim_z = len(z)\n        sigmas_h = zeros((N, dim_z))\n\n        # transform sigma points into measurement space\n        for i in range(N):\n            sigmas_h[i] = self.hx(self.sigmas[i])\n\n        z_mean = np.mean(sigmas_h, axis=0)\n\n        P_zz = (outer_product_sum(sigmas_h - z_mean) / (N - 1)) + R\n        P_xz = outer_product_sum(self.sigmas - self.x, sigmas_h - z_mean) / (N - 1)\n\n        self.S = P_zz\n        self.SI = self.inv(self.S)\n        self.K = dot(P_xz, self.SI)\n\n        e_r = multivariate_normal(self._mean_z, R, N)\n        for i in range(N):\n            self.sigmas[i] += dot(self.K, z + e_r[i] - sigmas_h[i])\n\n        self.x = np.mean(self.sigmas, axis=0)\n        self.P = self.P - dot(dot(self.K, self.S), self.K.T)\n\n        # save measurement and posterior state\n        self.z = deepcopy(z)\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n\n    def predict(self):\n        \"\"\"Predict next position.\"\"\"\n\n        N = self.N\n        for i, s in enumerate(self.sigmas):\n            self.sigmas[i] = self.fx(s, self.dt)\n\n        e = multivariate_normal(self._mean, self.Q, N)\n        self.sigmas += e\n\n        self.x = np.mean(self.sigmas, axis=0)\n        self.P = outer_product_sum(self.sigmas - self.x) / (N - 1)\n\n        # save prior\n        self.x_prior = np.copy(self.x)\n        self.P_prior = np.copy(self.P)\n\n    def __repr__(self):\n        return \"\\n\".join(\n            [\n                \"EnsembleKalmanFilter object\",\n                pretty_str(\"dim_x\", self.dim_x),\n                pretty_str(\"dim_z\", self.dim_z),\n                pretty_str(\"dt\", self.dt),\n                pretty_str(\"x\", self.x),\n                pretty_str(\"P\", self.P),\n                pretty_str(\"x_prior\", self.x_prior),\n                pretty_str(\"P_prior\", self.P_prior),\n                pretty_str(\"Q\", self.Q),\n                pretty_str(\"R\", self.R),\n                pretty_str(\"K\", self.K),\n                pretty_str(\"S\", self.S),\n                pretty_str(\"sigmas\", self.sigmas),\n                pretty_str(\"hx\", self.hx),\n                pretty_str(\"fx\", self.fx),\n            ]\n        )\n</code></pre>"},{"location":"filters/ensemble-kalman-filter/#bayesian_filters.kalman.ensemble_kalman_filter.EnsembleKalmanFilter.initialize","title":"<code>initialize(x, P)</code>","text":"<p>Initializes the filter with the specified mean and covariance. Only need to call this if you are using the filter to filter more than one set of data; this is called by init</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array(dim_z)</code> <p>state mean</p> required <code>P</code> <code>array((dim_x, dim_x))</code> <p>covariance of the state</p> required Source code in <code>bayesian_filters/kalman/ensemble_kalman_filter.py</code> <pre><code>def initialize(self, x, P):\n    \"\"\"\n    Initializes the filter with the specified mean and\n    covariance. Only need to call this if you are using the filter\n    to filter more than one set of data; this is called by __init__\n\n    Parameters\n    ----------\n\n    x : np.array(dim_z)\n        state mean\n\n    P : np.array((dim_x, dim_x))\n        covariance of the state\n    \"\"\"\n\n    if x.ndim != 1:\n        raise ValueError(\"x must be a 1D array\")\n\n    self.sigmas = multivariate_normal(mean=x, cov=P, size=self.N)\n    self.x = x\n    self.P = P\n\n    # these will always be a copy of x,P after predict() is called\n    self.x_prior = self.x.copy()\n    self.P_prior = self.P.copy()\n\n    # these will always be a copy of x,P after update() is called\n    self.x_post = self.x.copy()\n    self.P_post = self.P.copy()\n</code></pre>"},{"location":"filters/ensemble-kalman-filter/#bayesian_filters.kalman.ensemble_kalman_filter.EnsembleKalmanFilter.predict","title":"<code>predict()</code>","text":"<p>Predict next position.</p> Source code in <code>bayesian_filters/kalman/ensemble_kalman_filter.py</code> <pre><code>def predict(self):\n    \"\"\"Predict next position.\"\"\"\n\n    N = self.N\n    for i, s in enumerate(self.sigmas):\n        self.sigmas[i] = self.fx(s, self.dt)\n\n    e = multivariate_normal(self._mean, self.Q, N)\n    self.sigmas += e\n\n    self.x = np.mean(self.sigmas, axis=0)\n    self.P = outer_product_sum(self.sigmas - self.x) / (N - 1)\n\n    # save prior\n    self.x_prior = np.copy(self.x)\n    self.P_prior = np.copy(self.P)\n</code></pre>"},{"location":"filters/ensemble-kalman-filter/#bayesian_filters.kalman.ensemble_kalman_filter.EnsembleKalmanFilter.update","title":"<code>update(z, R=None)</code>","text":"<p>Add a new measurement (z) to the kalman filter. If z is None, nothing is changed.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>array</code> <p>measurement for this update.</p> required <code>R</code> <code>np.array, scalar, or None</code> <p>Optionally provide R to override the measurement noise for this one call, otherwise self.R will be used.</p> <code>None</code> Source code in <code>bayesian_filters/kalman/ensemble_kalman_filter.py</code> <pre><code>def update(self, z, R=None):\n    \"\"\"\n    Add a new measurement (z) to the kalman filter. If z is None, nothing\n    is changed.\n\n    Parameters\n    ----------\n\n    z : np.array\n        measurement for this update.\n\n    R : np.array, scalar, or None\n        Optionally provide R to override the measurement noise for this\n        one call, otherwise self.R will be used.\n    \"\"\"\n\n    if z is None:\n        self.z = array([[None] * self.dim_z]).T\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n        return\n\n    if R is None:\n        R = self.R\n    if np.isscalar(R):\n        R = eye(self.dim_z) * R\n\n    N = self.N\n    dim_z = len(z)\n    sigmas_h = zeros((N, dim_z))\n\n    # transform sigma points into measurement space\n    for i in range(N):\n        sigmas_h[i] = self.hx(self.sigmas[i])\n\n    z_mean = np.mean(sigmas_h, axis=0)\n\n    P_zz = (outer_product_sum(sigmas_h - z_mean) / (N - 1)) + R\n    P_xz = outer_product_sum(self.sigmas - self.x, sigmas_h - z_mean) / (N - 1)\n\n    self.S = P_zz\n    self.SI = self.inv(self.S)\n    self.K = dot(P_xz, self.SI)\n\n    e_r = multivariate_normal(self._mean_z, R, N)\n    for i in range(N):\n        self.sigmas[i] += dot(self.K, z + e_r[i] - sigmas_h[i])\n\n    self.x = np.mean(self.sigmas, axis=0)\n    self.P = self.P - dot(dot(self.K, self.S), self.K.T)\n\n    # save measurement and posterior state\n    self.z = deepcopy(z)\n    self.x_post = self.x.copy()\n    self.P_post = self.P.copy()\n</code></pre>"},{"location":"filters/extended-kalman-filter/","title":"Extended Kalman Filter","text":""},{"location":"filters/extended-kalman-filter/#extendedkalmanfilter","title":"ExtendedKalmanFilter","text":""},{"location":"filters/extended-kalman-filter/#introduction-and-overview","title":"Introduction and Overview","text":"<p>Implements a extended Kalman filter. For now the best documentation is my free book Kalman and Bayesian Filters in Python [1]</p> <p>The test files in this directory also give you a basic idea of use, albeit without much description.</p> <p>In brief, you will first construct this object, specifying the size of the state vector with <code>dim_x</code> and the size of the measurement vector that you will be using with <code>dim_z</code>. These are mostly used to perform size checks when you assign values to the various matrices. For example, if you specified <code>dim_z=2</code> and then try to assign a 3x3 matrix to R (the measurement noise matrix you will get an assert exception because R should be 2x2. (If for whatever reason you need to alter the size of things midstream just use the underscore version of the matrices to assign directly: your_filter._R = a_3x3_matrix.)</p> <p>After construction the filter will have default matrices created for you, but you must specify the values for each. It's usually easiest to just overwrite them rather than assign to each element yourself. This will be clearer in the example below. All are of type numpy.array.</p> <p>References</p> <p>.. [1] Labbe, Roger. \"Kalman and Bayesian Filters in Python\".</p> <p>github repo:     https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> <p>read online:     http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb</p>"},{"location":"filters/extended-kalman-filter/#api-reference","title":"API Reference","text":""},{"location":"filters/extended-kalman-filter/#bayesian_filters.kalman.EKF.ExtendedKalmanFilter","title":"<code>ExtendedKalmanFilter</code>","text":"<p>               Bases: <code>object</code></p> <p>Implements an extended Kalman filter (EKF). You are responsible for setting the various state variables to reasonable values; the defaults will  not give you a functional filter.</p> <p>You will have to set the following attributes after constructing this object for the filter to perform properly. Please note that there are various checks in place to ensure that you have made everything the 'correct' size. However, it is possible to provide incorrectly sized arrays such that the linear algebra can not perform an operation. It can also fail silently - you can end up with matrices of a size that allows the linear algebra to work, but are the wrong shape for the problem you are trying to solve.</p> <p>Parameters:</p> Name Type Description Default <code>dim_x</code> <code>int</code> <p>Number of state variables for the Kalman filter. For example, if you are tracking the position and velocity of an object in two dimensions, dim_x would be 4.</p> <p>This is used to set the default size of P, Q, and u</p> required <code>dim_z</code> <code>int</code> <p>Number of measurement inputs. For example, if the sensor provides you with position in (x,y), dim_z would be 2.</p> required <p>Attributes:</p> Name Type Description <code>x</code> <code>array(dim_x, 1)</code> <p>State estimate vector</p> <code>P</code> <code>array(dim_x, dim_x)</code> <p>Covariance matrix</p> <code>x_prior</code> <code>array(dim_x, 1)</code> <p>Prior (predicted) state estimate. The _prior and _post attributes are for convienence; they store the  prior and posterior of the current epoch. Read Only.</p> <code>P_prior</code> <code>array(dim_x, dim_x)</code> <p>Prior (predicted) state covariance matrix. Read Only.</p> <code>x_post</code> <code>array(dim_x, 1)</code> <p>Posterior (updated) state estimate. Read Only.</p> <code>P_post</code> <code>array(dim_x, dim_x)</code> <p>Posterior (updated) state covariance matrix. Read Only.</p> <code>R</code> <code>array(dim_z, dim_z)</code> <p>Measurement noise matrix</p> <code>Q</code> <code>array(dim_x, dim_x)</code> <p>Process noise matrix</p> <code>F</code> <code>array()</code> <p>State Transition matrix</p> <code>H</code> <code>array(dim_x, dim_x)</code> <p>Measurement function</p> <code>y</code> <code>array</code> <p>Residual of the update step. Read only.</p> <code>K</code> <code>array(dim_x, dim_z)</code> <p>Kalman gain of the update step. Read only.</p> <code>S</code> <code>array</code> <p>Systen uncertaintly projected to measurement space. Read only.</p> <code>z</code> <code>ndarray</code> <p>Last measurement used in update(). Read only.</p> <code>log_likelihood</code> <code>float</code> <p>log-likelihood of the last measurement. Read only.</p> <code>likelihood</code> <code>float</code> <p>likelihood of last measurment. Read only.</p> <p>Computed from the log-likelihood. The log-likelihood can be very small,  meaning a large negative value such as -28000. Taking the exp() of that results in 0.0, which can break typical algorithms which multiply by this value, so by default we always return a number &gt;= sys.float_info.min.</p> <code>mahalanobis</code> <code>float</code> <p>mahalanobis distance of the innovation. E.g. 3 means measurement was 3 standard deviations away from the predicted value.</p> <p>Read only.</p> <p>Examples:</p> <p>See my book Kalman and Bayesian Filters in Python https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> Source code in <code>bayesian_filters/kalman/EKF.py</code> <pre><code>class ExtendedKalmanFilter(object):\n    \"\"\"Implements an extended Kalman filter (EKF). You are responsible for\n    setting the various state variables to reasonable values; the defaults\n    will  not give you a functional filter.\n\n    You will have to set the following attributes after constructing this\n    object for the filter to perform properly. Please note that there are\n    various checks in place to ensure that you have made everything the\n    'correct' size. However, it is possible to provide incorrectly sized\n    arrays such that the linear algebra can not perform an operation.\n    It can also fail silently - you can end up with matrices of a size that\n    allows the linear algebra to work, but are the wrong shape for the problem\n    you are trying to solve.\n\n    Parameters\n    ----------\n\n    dim_x : int\n        Number of state variables for the Kalman filter. For example, if\n        you are tracking the position and velocity of an object in two\n        dimensions, dim_x would be 4.\n\n        This is used to set the default size of P, Q, and u\n\n    dim_z : int\n        Number of measurement inputs. For example, if the sensor\n        provides you with position in (x,y), dim_z would be 2.\n\n    Attributes\n    ----------\n    x : numpy.array(dim_x, 1)\n        State estimate vector\n\n    P : numpy.array(dim_x, dim_x)\n        Covariance matrix\n\n    x_prior : numpy.array(dim_x, 1)\n        Prior (predicted) state estimate. The *_prior and *_post attributes\n        are for convienence; they store the  prior and posterior of the\n        current epoch. Read Only.\n\n    P_prior : numpy.array(dim_x, dim_x)\n        Prior (predicted) state covariance matrix. Read Only.\n\n    x_post : numpy.array(dim_x, 1)\n        Posterior (updated) state estimate. Read Only.\n\n    P_post : numpy.array(dim_x, dim_x)\n        Posterior (updated) state covariance matrix. Read Only.\n\n    R : numpy.array(dim_z, dim_z)\n        Measurement noise matrix\n\n    Q : numpy.array(dim_x, dim_x)\n        Process noise matrix\n\n    F : numpy.array()\n        State Transition matrix\n\n    H : numpy.array(dim_x, dim_x)\n        Measurement function\n\n    y : numpy.array\n        Residual of the update step. Read only.\n\n    K : numpy.array(dim_x, dim_z)\n        Kalman gain of the update step. Read only.\n\n    S :  numpy.array\n        Systen uncertaintly projected to measurement space. Read only.\n\n    z : ndarray\n        Last measurement used in update(). Read only.\n\n    log_likelihood : float\n        log-likelihood of the last measurement. Read only.\n\n    likelihood : float\n        likelihood of last measurment. Read only.\n\n        Computed from the log-likelihood. The log-likelihood can be very\n        small,  meaning a large negative value such as -28000. Taking the\n        exp() of that results in 0.0, which can break typical algorithms\n        which multiply by this value, so by default we always return a\n        number &gt;= sys.float_info.min.\n\n    mahalanobis : float\n        mahalanobis distance of the innovation. E.g. 3 means measurement\n        was 3 standard deviations away from the predicted value.\n\n        Read only.\n\n    Examples\n    --------\n\n    See my book Kalman and Bayesian Filters in Python\n    https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n    \"\"\"\n\n    def __init__(self, dim_x, dim_z, dim_u=0):\n        self.dim_x = dim_x\n        self.dim_z = dim_z\n        self.dim_u = dim_u\n\n        self.x = zeros((dim_x, 1))  # state\n        self.P = eye(dim_x)  # uncertainty covariance\n        self.B = 0  # control transition matrix\n        self.F = np.eye(dim_x)  # state transition matrix\n        self.R = eye(dim_z)  # state uncertainty\n        self.Q = eye(dim_x)  # process uncertainty\n        self.y = zeros((dim_z, 1))  # residual\n\n        z = np.array([None] * self.dim_z)\n        self.z = reshape_z(z, self.dim_z, self.x.ndim)\n\n        # gain and residual are computed during the innovation step. We\n        # save them so that in case you want to inspect them for various\n        # purposes\n        self.K = np.zeros(self.x.shape)  # kalman gain\n        self.y = zeros((dim_z, 1))\n        self.S = np.zeros((dim_z, dim_z))  # system uncertainty\n        self.SI = np.zeros((dim_z, dim_z))  # inverse system uncertainty\n\n        # identity matrix. Do not alter this.\n        self._I = np.eye(dim_x)\n\n        self._log_likelihood = log(sys.float_info.min)\n        self._likelihood = sys.float_info.min\n        self._mahalanobis = None\n\n        # these will always be a copy of x,P after predict() is called\n        self.x_prior = self.x.copy()\n        self.P_prior = self.P.copy()\n\n        # these will always be a copy of x,P after update() is called\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n\n    def predict_update(self, z, HJacobian, Hx, args=(), hx_args=(), u=0):\n        \"\"\"Performs the predict/update innovation of the extended Kalman\n        filter.\n\n        Parameters\n        ----------\n\n        z : np.array\n            measurement for this step.\n            If `None`, only predict step is perfomed.\n\n        HJacobian : function\n           function which computes the Jacobian of the H matrix (measurement\n            function). Takes state variable (self.x) as input, along with the\n            optional arguments in args, and returns H.\n\n        Hx : function\n            function which takes as input the state variable (self.x) along\n            with the optional arguments in hx_args, and returns the measurement\n            that would correspond to that state.\n\n        args : tuple, optional, default (,)\n            arguments to be passed into HJacobian after the required state\n            variable.\n\n        hx_args : tuple, optional, default (,)\n            arguments to be passed into Hx after the required state\n            variable.\n\n        u : np.array or scalar\n            optional control vector input to the filter.\n        \"\"\"\n        # pylint: disable=too-many-locals\n\n        if not isinstance(args, tuple):\n            args = (args,)\n\n        if not isinstance(hx_args, tuple):\n            hx_args = (hx_args,)\n\n        if np.isscalar(z) and self.dim_z == 1:\n            z = np.asarray([z], float)\n\n        F = self.F\n        B = self.B\n        P = self.P\n        Q = self.Q\n        R = self.R\n        x = self.x\n\n        H = HJacobian(x, *args)\n\n        # predict step\n        x = dot(F, x) + dot(B, u)\n        P = dot(F, P).dot(F.T) + Q\n\n        # save prior\n        self.x_prior = np.copy(self.x)\n        self.P_prior = np.copy(self.P)\n\n        # update step\n        PHT = dot(P, H.T)\n        self.S = dot(H, PHT) + R\n        self.SI = linalg.inv(self.S)\n        self.K = dot(PHT, self.SI)\n\n        self.y = z - Hx(x, *hx_args)\n        self.x = x + dot(self.K, self.y)\n\n        I_KH = self._I - dot(self.K, H)\n        self.P = dot(I_KH, P).dot(I_KH.T) + dot(self.K, R).dot(self.K.T)\n\n        # save measurement and posterior state\n        self.z = deepcopy(z)\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n\n        # set to None to force recompute\n        self._log_likelihood = None\n        self._likelihood = None\n        self._mahalanobis = None\n\n    def update(self, z, HJacobian, Hx, R=None, args=(), hx_args=(), residual=np.subtract):\n        \"\"\"Performs the update innovation of the extended Kalman filter.\n\n        Parameters\n        ----------\n\n        z : np.array\n            measurement for this step.\n            If `None`, posterior is not computed\n\n        HJacobian : function\n           function which computes the Jacobian of the H matrix (measurement\n            function). Takes state variable (self.x) as input, returns H.\n\n        Hx : function\n            function which takes as input the state variable (self.x) along\n            with the optional arguments in hx_args, and returns the measurement\n            that would correspond to that state.\n\n        R : np.array, scalar, or None\n            Optionally provide R to override the measurement noise for this\n            one call, otherwise  self.R will be used.\n\n        args : tuple, optional, default (,)\n            arguments to be passed into HJacobian after the required state\n            variable. for robot localization you might need to pass in\n            information about the map and time of day, so you might have\n            `args=(map_data, time)`, where the signature of HCacobian will\n            be `def HJacobian(x, map, t)`\n\n        hx_args : tuple, optional, default (,)\n            arguments to be passed into Hx function after the required state\n            variable.\n\n        residual : function (z, z2), optional\n            Optional function that computes the residual (difference) between\n            the two measurement vectors. If you do not provide this, then the\n            built in minus operator will be used. You will normally want to use\n            the built in unless your residual computation is nonlinear (for\n            example, if they are angles)\n        \"\"\"\n\n        if z is None:\n            self.z = np.array([[None] * self.dim_z]).T\n            self.x_post = self.x.copy()\n            self.P_post = self.P.copy()\n            return\n\n        if not isinstance(args, tuple):\n            args = (args,)\n\n        if not isinstance(hx_args, tuple):\n            hx_args = (hx_args,)\n\n        if R is None:\n            R = self.R\n        elif np.isscalar(R):\n            R = eye(self.dim_z) * R\n\n        if np.isscalar(z) and self.dim_z == 1:\n            z = np.asarray([z], float)\n\n        H = HJacobian(self.x, *args)\n\n        PHT = dot(self.P, H.T)\n        self.S = dot(H, PHT) + R\n        self.SI = linalg.inv(self.S)\n        self.K = PHT.dot(self.SI)\n\n        hx = Hx(self.x, *hx_args)\n        self.y = residual(z, hx)\n        self.x = self.x + dot(self.K, self.y)\n\n        # P = (I-KH)P(I-KH)' + KRK' is more numerically stable\n        # and works for non-optimal K vs the equation\n        # P = (I-KH)P usually seen in the literature.\n        I_KH = self._I - dot(self.K, H)\n        self.P = dot(I_KH, self.P).dot(I_KH.T) + dot(self.K, R).dot(self.K.T)\n\n        # set to None to force recompute\n        self._log_likelihood = None\n        self._likelihood = None\n        self._mahalanobis = None\n\n        # save measurement and posterior state\n        self.z = deepcopy(z)\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n\n    def predict_x(self, u=0):\n        \"\"\"\n        Predicts the next state of X. If you need to\n        compute the next state yourself, override this function. You would\n        need to do this, for example, if the usual Taylor expansion to\n        generate F is not providing accurate results for you.\n        \"\"\"\n        self.x = dot(self.F, self.x) + dot(self.B, u)\n\n    def predict(self, u=0):\n        \"\"\"\n        Predict next state (prior) using the Kalman filter state propagation\n        equations.\n\n        Parameters\n        ----------\n\n        u : np.array\n            Optional control vector. If non-zero, it is multiplied by B\n            to create the control input into the system.\n        \"\"\"\n\n        self.predict_x(u)\n        self.P = dot(self.F, self.P).dot(self.F.T) + self.Q\n\n        # save prior\n        self.x_prior = np.copy(self.x)\n        self.P_prior = np.copy(self.P)\n\n    @property\n    def log_likelihood(self):\n        \"\"\"\n        log-likelihood of the last measurement.\n        \"\"\"\n\n        if self._log_likelihood is None:\n            self._log_likelihood = logpdf(x=self.y, cov=self.S)\n        return self._log_likelihood\n\n    @property\n    def likelihood(self):\n        \"\"\"\n        Computed from the log-likelihood. The log-likelihood can be very\n        small,  meaning a large negative value such as -28000. Taking the\n        exp() of that results in 0.0, which can break typical algorithms\n        which multiply by this value, so by default we always return a\n        number &gt;= sys.float_info.min.\n        \"\"\"\n        if self._likelihood is None:\n            self._likelihood = exp(self.log_likelihood)\n            if self._likelihood == 0:\n                self._likelihood = sys.float_info.min\n        return self._likelihood\n\n    @property\n    def mahalanobis(self):\n        \"\"\"\n        Mahalanobis distance of innovation. E.g. 3 means measurement\n        was 3 standard deviations away from the predicted value.\n\n        Returns\n        -------\n        mahalanobis : float\n        \"\"\"\n        if self._mahalanobis is None:\n            self._mahalanobis = sqrt(float(dot(dot(self.y.T, self.SI), self.y)))\n        return self._mahalanobis\n\n    def __repr__(self):\n        return \"\\n\".join(\n            [\n                \"KalmanFilter object\",\n                pretty_str(\"x\", self.x),\n                pretty_str(\"P\", self.P),\n                pretty_str(\"x_prior\", self.x_prior),\n                pretty_str(\"P_prior\", self.P_prior),\n                pretty_str(\"F\", self.F),\n                pretty_str(\"Q\", self.Q),\n                pretty_str(\"R\", self.R),\n                pretty_str(\"K\", self.K),\n                pretty_str(\"y\", self.y),\n                pretty_str(\"S\", self.S),\n                pretty_str(\"likelihood\", self.likelihood),\n                pretty_str(\"log-likelihood\", self.log_likelihood),\n                pretty_str(\"mahalanobis\", self.mahalanobis),\n            ]\n        )\n</code></pre>"},{"location":"filters/extended-kalman-filter/#bayesian_filters.kalman.EKF.ExtendedKalmanFilter.likelihood","title":"<code>likelihood</code>  <code>property</code>","text":"<p>Computed from the log-likelihood. The log-likelihood can be very small,  meaning a large negative value such as -28000. Taking the exp() of that results in 0.0, which can break typical algorithms which multiply by this value, so by default we always return a number &gt;= sys.float_info.min.</p>"},{"location":"filters/extended-kalman-filter/#bayesian_filters.kalman.EKF.ExtendedKalmanFilter.log_likelihood","title":"<code>log_likelihood</code>  <code>property</code>","text":"<p>log-likelihood of the last measurement.</p>"},{"location":"filters/extended-kalman-filter/#bayesian_filters.kalman.EKF.ExtendedKalmanFilter.mahalanobis","title":"<code>mahalanobis</code>  <code>property</code>","text":"<p>Mahalanobis distance of innovation. E.g. 3 means measurement was 3 standard deviations away from the predicted value.</p> <p>Returns:</p> Name Type Description <code>mahalanobis</code> <code>float</code>"},{"location":"filters/extended-kalman-filter/#bayesian_filters.kalman.EKF.ExtendedKalmanFilter.predict","title":"<code>predict(u=0)</code>","text":"<p>Predict next state (prior) using the Kalman filter state propagation equations.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>array</code> <p>Optional control vector. If non-zero, it is multiplied by B to create the control input into the system.</p> <code>0</code> Source code in <code>bayesian_filters/kalman/EKF.py</code> <pre><code>def predict(self, u=0):\n    \"\"\"\n    Predict next state (prior) using the Kalman filter state propagation\n    equations.\n\n    Parameters\n    ----------\n\n    u : np.array\n        Optional control vector. If non-zero, it is multiplied by B\n        to create the control input into the system.\n    \"\"\"\n\n    self.predict_x(u)\n    self.P = dot(self.F, self.P).dot(self.F.T) + self.Q\n\n    # save prior\n    self.x_prior = np.copy(self.x)\n    self.P_prior = np.copy(self.P)\n</code></pre>"},{"location":"filters/extended-kalman-filter/#bayesian_filters.kalman.EKF.ExtendedKalmanFilter.predict_update","title":"<code>predict_update(z, HJacobian, Hx, args=(), hx_args=(), u=0)</code>","text":"<p>Performs the predict/update innovation of the extended Kalman filter.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>array</code> <p>measurement for this step. If <code>None</code>, only predict step is perfomed.</p> required <code>HJacobian</code> <code>function</code> <p>function which computes the Jacobian of the H matrix (measurement function). Takes state variable (self.x) as input, along with the optional arguments in args, and returns H.</p> required <code>Hx</code> <code>function</code> <p>function which takes as input the state variable (self.x) along with the optional arguments in hx_args, and returns the measurement that would correspond to that state.</p> required <code>args</code> <code>tuple</code> <p>arguments to be passed into HJacobian after the required state variable.</p> <code>(,)</code> <code>hx_args</code> <code>tuple</code> <p>arguments to be passed into Hx after the required state variable.</p> <code>(,)</code> <code>u</code> <code>array or scalar</code> <p>optional control vector input to the filter.</p> <code>0</code> Source code in <code>bayesian_filters/kalman/EKF.py</code> <pre><code>def predict_update(self, z, HJacobian, Hx, args=(), hx_args=(), u=0):\n    \"\"\"Performs the predict/update innovation of the extended Kalman\n    filter.\n\n    Parameters\n    ----------\n\n    z : np.array\n        measurement for this step.\n        If `None`, only predict step is perfomed.\n\n    HJacobian : function\n       function which computes the Jacobian of the H matrix (measurement\n        function). Takes state variable (self.x) as input, along with the\n        optional arguments in args, and returns H.\n\n    Hx : function\n        function which takes as input the state variable (self.x) along\n        with the optional arguments in hx_args, and returns the measurement\n        that would correspond to that state.\n\n    args : tuple, optional, default (,)\n        arguments to be passed into HJacobian after the required state\n        variable.\n\n    hx_args : tuple, optional, default (,)\n        arguments to be passed into Hx after the required state\n        variable.\n\n    u : np.array or scalar\n        optional control vector input to the filter.\n    \"\"\"\n    # pylint: disable=too-many-locals\n\n    if not isinstance(args, tuple):\n        args = (args,)\n\n    if not isinstance(hx_args, tuple):\n        hx_args = (hx_args,)\n\n    if np.isscalar(z) and self.dim_z == 1:\n        z = np.asarray([z], float)\n\n    F = self.F\n    B = self.B\n    P = self.P\n    Q = self.Q\n    R = self.R\n    x = self.x\n\n    H = HJacobian(x, *args)\n\n    # predict step\n    x = dot(F, x) + dot(B, u)\n    P = dot(F, P).dot(F.T) + Q\n\n    # save prior\n    self.x_prior = np.copy(self.x)\n    self.P_prior = np.copy(self.P)\n\n    # update step\n    PHT = dot(P, H.T)\n    self.S = dot(H, PHT) + R\n    self.SI = linalg.inv(self.S)\n    self.K = dot(PHT, self.SI)\n\n    self.y = z - Hx(x, *hx_args)\n    self.x = x + dot(self.K, self.y)\n\n    I_KH = self._I - dot(self.K, H)\n    self.P = dot(I_KH, P).dot(I_KH.T) + dot(self.K, R).dot(self.K.T)\n\n    # save measurement and posterior state\n    self.z = deepcopy(z)\n    self.x_post = self.x.copy()\n    self.P_post = self.P.copy()\n\n    # set to None to force recompute\n    self._log_likelihood = None\n    self._likelihood = None\n    self._mahalanobis = None\n</code></pre>"},{"location":"filters/extended-kalman-filter/#bayesian_filters.kalman.EKF.ExtendedKalmanFilter.predict_x","title":"<code>predict_x(u=0)</code>","text":"<p>Predicts the next state of X. If you need to compute the next state yourself, override this function. You would need to do this, for example, if the usual Taylor expansion to generate F is not providing accurate results for you.</p> Source code in <code>bayesian_filters/kalman/EKF.py</code> <pre><code>def predict_x(self, u=0):\n    \"\"\"\n    Predicts the next state of X. If you need to\n    compute the next state yourself, override this function. You would\n    need to do this, for example, if the usual Taylor expansion to\n    generate F is not providing accurate results for you.\n    \"\"\"\n    self.x = dot(self.F, self.x) + dot(self.B, u)\n</code></pre>"},{"location":"filters/extended-kalman-filter/#bayesian_filters.kalman.EKF.ExtendedKalmanFilter.update","title":"<code>update(z, HJacobian, Hx, R=None, args=(), hx_args=(), residual=np.subtract)</code>","text":"<p>Performs the update innovation of the extended Kalman filter.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>array</code> <p>measurement for this step. If <code>None</code>, posterior is not computed</p> required <code>HJacobian</code> <code>function</code> <p>function which computes the Jacobian of the H matrix (measurement function). Takes state variable (self.x) as input, returns H.</p> required <code>Hx</code> <code>function</code> <p>function which takes as input the state variable (self.x) along with the optional arguments in hx_args, and returns the measurement that would correspond to that state.</p> required <code>R</code> <code>np.array, scalar, or None</code> <p>Optionally provide R to override the measurement noise for this one call, otherwise  self.R will be used.</p> <code>None</code> <code>args</code> <code>tuple</code> <p>arguments to be passed into HJacobian after the required state variable. for robot localization you might need to pass in information about the map and time of day, so you might have <code>args=(map_data, time)</code>, where the signature of HCacobian will be <code>def HJacobian(x, map, t)</code></p> <code>(,)</code> <code>hx_args</code> <code>tuple</code> <p>arguments to be passed into Hx function after the required state variable.</p> <code>(,)</code> <code>residual</code> <code>function(z, z2)</code> <p>Optional function that computes the residual (difference) between the two measurement vectors. If you do not provide this, then the built in minus operator will be used. You will normally want to use the built in unless your residual computation is nonlinear (for example, if they are angles)</p> <code>subtract</code> Source code in <code>bayesian_filters/kalman/EKF.py</code> <pre><code>def update(self, z, HJacobian, Hx, R=None, args=(), hx_args=(), residual=np.subtract):\n    \"\"\"Performs the update innovation of the extended Kalman filter.\n\n    Parameters\n    ----------\n\n    z : np.array\n        measurement for this step.\n        If `None`, posterior is not computed\n\n    HJacobian : function\n       function which computes the Jacobian of the H matrix (measurement\n        function). Takes state variable (self.x) as input, returns H.\n\n    Hx : function\n        function which takes as input the state variable (self.x) along\n        with the optional arguments in hx_args, and returns the measurement\n        that would correspond to that state.\n\n    R : np.array, scalar, or None\n        Optionally provide R to override the measurement noise for this\n        one call, otherwise  self.R will be used.\n\n    args : tuple, optional, default (,)\n        arguments to be passed into HJacobian after the required state\n        variable. for robot localization you might need to pass in\n        information about the map and time of day, so you might have\n        `args=(map_data, time)`, where the signature of HCacobian will\n        be `def HJacobian(x, map, t)`\n\n    hx_args : tuple, optional, default (,)\n        arguments to be passed into Hx function after the required state\n        variable.\n\n    residual : function (z, z2), optional\n        Optional function that computes the residual (difference) between\n        the two measurement vectors. If you do not provide this, then the\n        built in minus operator will be used. You will normally want to use\n        the built in unless your residual computation is nonlinear (for\n        example, if they are angles)\n    \"\"\"\n\n    if z is None:\n        self.z = np.array([[None] * self.dim_z]).T\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n        return\n\n    if not isinstance(args, tuple):\n        args = (args,)\n\n    if not isinstance(hx_args, tuple):\n        hx_args = (hx_args,)\n\n    if R is None:\n        R = self.R\n    elif np.isscalar(R):\n        R = eye(self.dim_z) * R\n\n    if np.isscalar(z) and self.dim_z == 1:\n        z = np.asarray([z], float)\n\n    H = HJacobian(self.x, *args)\n\n    PHT = dot(self.P, H.T)\n    self.S = dot(H, PHT) + R\n    self.SI = linalg.inv(self.S)\n    self.K = PHT.dot(self.SI)\n\n    hx = Hx(self.x, *hx_args)\n    self.y = residual(z, hx)\n    self.x = self.x + dot(self.K, self.y)\n\n    # P = (I-KH)P(I-KH)' + KRK' is more numerically stable\n    # and works for non-optimal K vs the equation\n    # P = (I-KH)P usually seen in the literature.\n    I_KH = self._I - dot(self.K, H)\n    self.P = dot(I_KH, self.P).dot(I_KH.T) + dot(self.K, R).dot(self.K.T)\n\n    # set to None to force recompute\n    self._log_likelihood = None\n    self._likelihood = None\n    self._mahalanobis = None\n\n    # save measurement and posterior state\n    self.z = deepcopy(z)\n    self.x_post = self.x.copy()\n    self.P_post = self.P.copy()\n</code></pre>"},{"location":"filters/fading-kalman-filter/","title":"Fading Kalman Filter","text":""},{"location":"filters/fading-kalman-filter/#fadingkalmanfilter","title":"FadingKalmanFilter","text":"<p>Implements a fading memory Kalman filter.</p>"},{"location":"filters/fading-kalman-filter/#api-reference","title":"API Reference","text":""},{"location":"filters/fading-kalman-filter/#bayesian_filters.kalman.fading_memory.FadingKalmanFilter","title":"<code>FadingKalmanFilter</code>","text":"<p>               Bases: <code>object</code></p> <p>Fading memory Kalman filter. This implements a linear Kalman filter with a fading memory effect controlled by <code>alpha</code>. This is obsolete. The class KalmanFilter now incorporates the <code>alpha</code> attribute, and should be used instead.</p> <p>You are responsible for setting the various state variables to reasonable values; the defaults below will not give you a functional filter.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float, &gt;= 1</code> <p>alpha controls how much you want the filter to forget past measurements. alpha==1 yields identical performance to the Kalman filter. A typical application might use 1.01</p> required <code>dim_x</code> <code>int</code> <p>Number of state variables for the Kalman filter. For example, if you are tracking the position and velocity of an object in two dimensions, dim_x would be 4.</p> <p>This is used to set the default size of P, Q, and u</p> required <code>dim_z</code> <code>int</code> <p>Number of of measurement inputs. For example, if the sensor provides you with position in (x,y), dim_z would be 2.</p> required <code>dim_u</code> <code>int(optional)</code> <p>size of the control input, if it is being used. Default value of 0 indicates it is not used.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>You will have to assign reasonable values to all of these before</code> <code>running the filter. All must have dtype of float</code> <code>x</code> <code>ndarray (dim_x, 1), default = [0,0,0...0]</code> <p>state of the filter</p> <code>P</code> <code>ndarray (dim_x, dim_x), default identity matrix</code> <p>covariance matrix</p> <code>x_prior</code> <code>array(dim_x, 1)</code> <p>Prior (predicted) state estimate. The _prior and _post attributes are for convienence; they store the  prior and posterior of the current epoch. Read Only.</p> <code>P_prior</code> <code>array(dim_x, dim_x)</code> <p>Prior (predicted) state covariance matrix. Read Only.</p> <code>x_post</code> <code>array(dim_x, 1)</code> <p>Posterior (updated) state estimate. Read Only.</p> <code>P_post</code> <code>array(dim_x, dim_x)</code> <p>Posterior (updated) state covariance matrix. Read Only.</p> <code>z</code> <code>ndarray</code> <p>Last measurement used in update(). Read only.</p> <code>Q</code> <code>ndarray (dim_x, dim_x), default identity matrix</code> <p>Process uncertainty matrix</p> <code>R</code> <code>ndarray (dim_z, dim_z), default identity matrix</code> <p>measurement uncertainty</p> <code>H</code> <code>ndarray(dim_z, dim_x)</code> <p>measurement function</p> <code>F</code> <code>ndarray(dim_x, dim_x)</code> <p>state transition matrix</p> <code>B</code> <code>ndarray (dim_x, dim_u), default 0</code> <p>control transition matrix</p> <code>y</code> <code>array</code> <p>Residual of the update step. Read only.</p> <code>K</code> <code>array(dim_x, dim_z)</code> <p>Kalman gain of the update step. Read only.</p> <code>S</code> <code>array</code> <p>System uncertainty (P projected to measurement space). Read only.</p> <code>S</code> <code>array</code> <p>Inverse system uncertainty. Read only.</p> <code>log_likelihood</code> <code>float</code> <p>log-likelihood of the last measurement. Read only.</p> <code>likelihood</code> <code>float</code> <p>likelihood of last measurement. Read only.</p> <p>Computed from the log-likelihood. The log-likelihood can be very small,  meaning a large negative value such as -28000. Taking the exp() of that results in 0.0, which can break typical algorithms which multiply by this value, so by default we always return a number &gt;= sys.float_info.min.</p> <code>mahalanobis</code> <code>float</code> <p>mahalanobis distance of the innovation. Read only.</p> <p>Examples:</p> <p>See my book Kalman and Bayesian Filters in Python https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> Source code in <code>bayesian_filters/kalman/fading_memory.py</code> <pre><code>class FadingKalmanFilter(object):\n    \"\"\"\n    Fading memory Kalman filter. This implements a linear Kalman filter with\n    a fading memory effect controlled by `alpha`. This is obsolete. The\n    class KalmanFilter now incorporates the `alpha` attribute, and should\n    be used instead.\n\n    You are responsible for setting the\n    various state variables to reasonable values; the defaults below\n    will not give you a functional filter.\n\n    Parameters\n    ----------\n\n    alpha : float, &gt;= 1\n        alpha controls how much you want the filter to forget past\n        measurements. alpha==1 yields identical performance to the\n        Kalman filter. A typical application might use 1.01\n\n    dim_x : int\n        Number of state variables for the Kalman filter. For example, if\n        you are tracking the position and velocity of an object in two\n        dimensions, dim_x would be 4.\n\n        This is used to set the default size of P, Q, and u\n\n    dim_z : int\n        Number of of measurement inputs. For example, if the sensor\n        provides you with position in (x,y), dim_z would be 2.\n\n    dim_u : int (optional)\n        size of the control input, if it is being used.\n        Default value of 0 indicates it is not used.\n\n    Attributes\n    ----------\n\n    You will have to assign reasonable values to all of these before\n    running the filter. All must have dtype of float\n\n    x : ndarray (dim_x, 1), default = [0,0,0...0]\n        state of the filter\n\n    P : ndarray (dim_x, dim_x), default identity matrix\n        covariance matrix\n\n    x_prior : numpy.array(dim_x, 1)\n        Prior (predicted) state estimate. The *_prior and *_post attributes\n        are for convienence; they store the  prior and posterior of the\n        current epoch. Read Only.\n\n    P_prior : numpy.array(dim_x, dim_x)\n        Prior (predicted) state covariance matrix. Read Only.\n\n    x_post : numpy.array(dim_x, 1)\n        Posterior (updated) state estimate. Read Only.\n\n    P_post : numpy.array(dim_x, dim_x)\n        Posterior (updated) state covariance matrix. Read Only.\n\n    z : ndarray\n        Last measurement used in update(). Read only.\n\n    Q : ndarray (dim_x, dim_x), default identity matrix\n        Process uncertainty matrix\n\n    R : ndarray (dim_z, dim_z), default identity matrix\n        measurement uncertainty\n\n    H : ndarray (dim_z, dim_x)\n        measurement function\n\n    F : ndarray (dim_x, dim_x)\n        state transition matrix\n\n    B : ndarray (dim_x, dim_u), default 0\n        control transition matrix\n\n    y : numpy.array\n        Residual of the update step. Read only.\n\n    K : numpy.array(dim_x, dim_z)\n        Kalman gain of the update step. Read only.\n\n    S :  numpy.array\n        System uncertainty (P projected to measurement space). Read only.\n\n    S :  numpy.array\n        Inverse system uncertainty. Read only.\n\n    log_likelihood : float\n        log-likelihood of the last measurement. Read only.\n\n    likelihood : float\n        likelihood of last measurement. Read only.\n\n        Computed from the log-likelihood. The log-likelihood can be very\n        small,  meaning a large negative value such as -28000. Taking the\n        exp() of that results in 0.0, which can break typical algorithms\n        which multiply by this value, so by default we always return a\n        number &gt;= sys.float_info.min.\n\n    mahalanobis : float\n        mahalanobis distance of the innovation. Read only.\n\n\n    Examples\n    --------\n\n    See my book Kalman and Bayesian Filters in Python\n    https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n    \"\"\"\n\n    def __init__(self, alpha, dim_x, dim_z, dim_u=0):\n        warnings.warn(\n            \"Use KalmanFilter class instead; it also provides the alpha attribute\",\n            DeprecationWarning,\n        )\n\n        assert alpha &gt;= 1\n        assert dim_x &gt; 0\n        assert dim_z &gt; 0\n        assert dim_u &gt;= 0\n\n        self.alpha_sq = alpha**2\n        self.dim_x = dim_x\n        self.dim_z = dim_z\n        self.dim_u = dim_u\n\n        self.x = zeros((dim_x, 1))  # state\n        self.P = eye(dim_x)  # uncertainty covariance\n        self.Q = eye(dim_x)  # process uncertainty\n        self.B = 0.0  # control transition matrix\n        self.F = np.eye(dim_x)  # state transition matrix\n        self.H = zeros((dim_z, dim_x))  # Measurement function\n        self.R = eye(dim_z)  # state uncertainty\n        self.z = np.array([[None] * dim_z]).T\n\n        # gain and residual are computed during the innovation step. We\n        # save them so that in case you want to inspect them for various\n        # purposes\n        self.K = 0  # kalman gain\n        self.y = zeros((dim_z, 1))\n        self.S = np.zeros((dim_z, dim_z))  # system uncertainty (measurement space)\n        self.SI = np.zeros((dim_z, dim_z))  # inverse system uncertainty\n\n        # identity matrix. Do not alter this.\n        self.I = np.eye(dim_x)\n\n        # Only computed only if requested via property\n        self._log_likelihood = log(sys.float_info.min)\n        self._likelihood = sys.float_info.min\n        self._mahalanobis = None\n\n        # these will always be a copy of x,P after predict() is called\n        self.x_prior = self.x.copy()\n        self.P_prior = self.P.copy()\n\n        # these will always be a copy of x,P after update() is called\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n\n    def update(self, z, R=None):\n        \"\"\"\n        Add a new measurement (z) to the kalman filter. If z is None, nothing\n        is changed.\n\n        Parameters\n        ----------\n\n        z : np.array\n            measurement for this update.\n\n        R : np.array, scalar, or None\n            Optionally provide R to override the measurement noise for this\n            one call, otherwise  self.R will be used.\n        \"\"\"\n\n        if z is None:\n            self.z = np.array([[None] * self.dim_z]).T\n            self.x_post = self.x.copy()\n            self.P_post = self.P.copy()\n            return\n\n        if R is None:\n            R = self.R\n        elif np.isscalar(R):\n            R = eye(self.dim_z) * R\n\n        # y = z - Hx\n        # error (residual) between measurement and prediction\n        self.y = z - dot(self.H, self.x)\n\n        PHT = dot(self.P, self.H.T)\n\n        # S = HPH' + R\n        # project system uncertainty into measurement space\n        self.S = dot(self.H, PHT) + R\n        self.SI = linalg.inv(self.S)\n\n        # K = PH'inv(S)\n        # map system uncertainty into kalman gain\n        self.K = PHT.dot(self.SI)\n\n        # x = x + Ky\n        # predict new x with residual scaled by the kalman gain\n        self.x = self.x + dot(self.K, self.y)\n\n        # P = (I-KH)P(I-KH)' + KRK'\n        I_KH = self.I - dot(self.K, self.H)\n        self.P = dot(I_KH, self.P).dot(I_KH.T) + dot(self.K, R).dot(self.K.T)\n\n        # save measurement and posterior state\n        self.z = deepcopy(z)\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n\n        # set to None to force recompute\n        self._log_likelihood = None\n        self._likelihood = None\n        self._mahalanobis = None\n\n    def predict(self, u=0):\n        \"\"\"Predict next position.\n\n        Parameters\n        ----------\n\n        u : np.array\n            Optional control vector. If non-zero, it is multiplied by B\n            to create the control input into the system.\n        \"\"\"\n\n        # x = Fx + Bu\n        self.x = dot(self.F, self.x) + dot(self.B, u)\n\n        # P = FPF' + Q\n        self.P = self.alpha_sq * dot(self.F, self.P).dot(self.F.T) + self.Q\n\n        # save prior\n        self.x_prior = self.x.copy()\n        self.P_prior = self.P.copy()\n\n    def batch_filter(self, zs, Rs=None, update_first=False):\n        \"\"\"Batch processes a sequences of measurements.\n\n        Parameters\n        ----------\n\n        zs : list-like\n            list of measurements at each time step `self.dt` Missing\n            measurements must be represented by 'None'.\n\n        Rs : list-like, optional\n            optional list of values to use for the measurement error\n            covariance; a value of None in any position will cause the filter\n            to use `self.R` for that time step.\n\n        update_first : bool, optional,\n            controls whether the order of operations is update followed by\n            predict, or predict followed by update. Default is predict-&gt;update.\n\n        Returns\n        -------\n\n        means: np.array((n,dim_x,1))\n            array of the state for each time step after the update. Each entry\n            is an np.array. In other words `means[k,:]` is the state at step\n            `k`.\n\n        covariance: np.array((n,dim_x,dim_x))\n            array of the covariances for each time step after the update.\n            In other words `covariance[k,:,:]` is the covariance at step `k`.\n\n        means_predictions: np.array((n,dim_x,1))\n            array of the state for each time step after the predictions. Each\n            entry is an np.array. In other words `means[k,:]` is the state at\n            step `k`.\n\n        covariance_predictions: np.array((n,dim_x,dim_x))\n            array of the covariances for each time step after the prediction.\n            In other words `covariance[k,:,:]` is the covariance at step `k`.\n        \"\"\"\n\n        n = np.size(zs, 0)\n        if Rs is None:\n            Rs = [None] * n\n\n        # pylint: disable=bad-whitespace\n\n        # mean estimates from Kalman Filter\n        means = zeros((n, self.dim_x, 1))\n        means_p = zeros((n, self.dim_x, 1))\n\n        # state covariances from Kalman Filter\n        covariances = zeros((n, self.dim_x, self.dim_x))\n        covariances_p = zeros((n, self.dim_x, self.dim_x))\n\n        if update_first:\n            for i, (z, r) in enumerate(zip(zs, Rs)):\n                self.update(z, r)\n                means[i, :] = self.x\n                covariances[i, :, :] = self.P\n\n                self.predict()\n                means_p[i, :] = self.x\n                covariances_p[i, :, :] = self.P\n        else:\n            for i, (z, r) in enumerate(zip(zs, Rs)):\n                self.predict()\n                means_p[i, :] = self.x\n                covariances_p[i, :, :] = self.P\n\n                self.update(z, r)\n                means[i, :] = self.x\n                covariances[i, :, :] = self.P\n\n        return (means, covariances, means_p, covariances_p)\n\n    def get_prediction(self, u=0):\n        \"\"\"Predicts the next state of the filter and returns it. Does not\n        alter the state of the filter.\n\n        Parameters\n        ----------\n\n        u : np.array\n            optional control input\n\n        Returns\n        -------\n\n        (x, P)\n            State vector and covariance array of the prediction.\n        \"\"\"\n\n        x = dot(self.F, self.x) + dot(self.B, u)\n        P = self.alpha_sq * dot(self.F, self.P).dot(self.F.T) + self.Q\n        return (x, P)\n\n    def residual_of(self, z):\n        \"\"\"returns the residual for the given measurement (z). Does not alter\n        the state of the filter.\n        \"\"\"\n        return z - dot(self.H, self.x)\n\n    def measurement_of_state(self, x):\n        \"\"\"Helper function that converts a state into a measurement.\n\n        Parameters\n        ----------\n\n        x : np.array\n            kalman state vector\n\n        Returns\n        -------\n\n        z : np.array\n            measurement corresponding to the given state\n        \"\"\"\n        return dot(self.H, x)\n\n    @property\n    def alpha(self):\n        \"\"\"scaling factor for fading memory\"\"\"\n\n        return sqrt(self.alpha_sq)\n\n    @property\n    def log_likelihood(self):\n        \"\"\"\n        log-likelihood of the last measurement.\n        \"\"\"\n        if self._log_likelihood is None:\n            self._log_likelihood = logpdf(x=self.y, cov=self.S)\n        return self._log_likelihood\n\n    @property\n    def likelihood(self):\n        \"\"\"\n        Computed from the log-likelihood. The log-likelihood can be very\n        small,  meaning a large negative value such as -28000. Taking the\n        exp() of that results in 0.0, which can break typical algorithms\n        which multiply by this value, so by default we always return a\n        number &gt;= sys.float_info.min.\n        \"\"\"\n        if self._likelihood is None:\n            self._likelihood = exp(self.log_likelihood)\n            if self._likelihood == 0:\n                self._likelihood = sys.float_info.min\n        return self._likelihood\n\n    @property\n    def mahalanobis(self):\n        \"\"\" \"\n        Mahalanobis distance of innovation. E.g. 3 means measurement\n        was 3 standard deviations away from the predicted value.\n\n        Returns\n        -------\n        mahalanobis : float\n        \"\"\"\n        if self._mahalanobis is None:\n            self._mahalanobis = sqrt(float(dot(dot(self.y.T, self.SI), self.y)))\n        return self._mahalanobis\n\n    def __repr__(self):\n        return \"\\n\".join(\n            [\n                \"FadingKalmanFilter object\",\n                pretty_str(\"dim_x\", self.x),\n                pretty_str(\"dim_z\", self.x),\n                pretty_str(\"dim_u\", self.dim_u),\n                pretty_str(\"x\", self.x),\n                pretty_str(\"P\", self.P),\n                pretty_str(\"F\", self.F),\n                pretty_str(\"Q\", self.Q),\n                pretty_str(\"R\", self.R),\n                pretty_str(\"H\", self.H),\n                pretty_str(\"K\", self.K),\n                pretty_str(\"y\", self.y),\n                pretty_str(\"S\", self.S),\n                pretty_str(\"B\", self.B),\n                pretty_str(\"likelihood\", self.likelihood),\n                pretty_str(\"log-likelihood\", self.log_likelihood),\n                pretty_str(\"mahalanobis\", self.mahalanobis),\n                pretty_str(\"alpha\", self.alpha),\n            ]\n        )\n</code></pre>"},{"location":"filters/fading-kalman-filter/#bayesian_filters.kalman.fading_memory.FadingKalmanFilter.alpha","title":"<code>alpha</code>  <code>property</code>","text":"<p>scaling factor for fading memory</p>"},{"location":"filters/fading-kalman-filter/#bayesian_filters.kalman.fading_memory.FadingKalmanFilter.likelihood","title":"<code>likelihood</code>  <code>property</code>","text":"<p>Computed from the log-likelihood. The log-likelihood can be very small,  meaning a large negative value such as -28000. Taking the exp() of that results in 0.0, which can break typical algorithms which multiply by this value, so by default we always return a number &gt;= sys.float_info.min.</p>"},{"location":"filters/fading-kalman-filter/#bayesian_filters.kalman.fading_memory.FadingKalmanFilter.log_likelihood","title":"<code>log_likelihood</code>  <code>property</code>","text":"<p>log-likelihood of the last measurement.</p>"},{"location":"filters/fading-kalman-filter/#bayesian_filters.kalman.fading_memory.FadingKalmanFilter.mahalanobis","title":"<code>mahalanobis</code>  <code>property</code>","text":"<p>\" Mahalanobis distance of innovation. E.g. 3 means measurement was 3 standard deviations away from the predicted value.</p> <p>Returns:</p> Name Type Description <code>mahalanobis</code> <code>float</code>"},{"location":"filters/fading-kalman-filter/#bayesian_filters.kalman.fading_memory.FadingKalmanFilter.batch_filter","title":"<code>batch_filter(zs, Rs=None, update_first=False)</code>","text":"<p>Batch processes a sequences of measurements.</p> <p>Parameters:</p> Name Type Description Default <code>zs</code> <code>list - like</code> <p>list of measurements at each time step <code>self.dt</code> Missing measurements must be represented by 'None'.</p> required <code>Rs</code> <code>list - like</code> <p>optional list of values to use for the measurement error covariance; a value of None in any position will cause the filter to use <code>self.R</code> for that time step.</p> <code>None</code> <code>update_first</code> <code>(bool, optional)</code> <p>controls whether the order of operations is update followed by predict, or predict followed by update. Default is predict-&gt;update.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>means</code> <code>array((n, dim_x, 1))</code> <p>array of the state for each time step after the update. Each entry is an np.array. In other words <code>means[k,:]</code> is the state at step <code>k</code>.</p> <code>covariance</code> <code>array((n, dim_x, dim_x))</code> <p>array of the covariances for each time step after the update. In other words <code>covariance[k,:,:]</code> is the covariance at step <code>k</code>.</p> <code>means_predictions</code> <code>array((n, dim_x, 1))</code> <p>array of the state for each time step after the predictions. Each entry is an np.array. In other words <code>means[k,:]</code> is the state at step <code>k</code>.</p> <code>covariance_predictions</code> <code>array((n, dim_x, dim_x))</code> <p>array of the covariances for each time step after the prediction. In other words <code>covariance[k,:,:]</code> is the covariance at step <code>k</code>.</p> Source code in <code>bayesian_filters/kalman/fading_memory.py</code> <pre><code>def batch_filter(self, zs, Rs=None, update_first=False):\n    \"\"\"Batch processes a sequences of measurements.\n\n    Parameters\n    ----------\n\n    zs : list-like\n        list of measurements at each time step `self.dt` Missing\n        measurements must be represented by 'None'.\n\n    Rs : list-like, optional\n        optional list of values to use for the measurement error\n        covariance; a value of None in any position will cause the filter\n        to use `self.R` for that time step.\n\n    update_first : bool, optional,\n        controls whether the order of operations is update followed by\n        predict, or predict followed by update. Default is predict-&gt;update.\n\n    Returns\n    -------\n\n    means: np.array((n,dim_x,1))\n        array of the state for each time step after the update. Each entry\n        is an np.array. In other words `means[k,:]` is the state at step\n        `k`.\n\n    covariance: np.array((n,dim_x,dim_x))\n        array of the covariances for each time step after the update.\n        In other words `covariance[k,:,:]` is the covariance at step `k`.\n\n    means_predictions: np.array((n,dim_x,1))\n        array of the state for each time step after the predictions. Each\n        entry is an np.array. In other words `means[k,:]` is the state at\n        step `k`.\n\n    covariance_predictions: np.array((n,dim_x,dim_x))\n        array of the covariances for each time step after the prediction.\n        In other words `covariance[k,:,:]` is the covariance at step `k`.\n    \"\"\"\n\n    n = np.size(zs, 0)\n    if Rs is None:\n        Rs = [None] * n\n\n    # pylint: disable=bad-whitespace\n\n    # mean estimates from Kalman Filter\n    means = zeros((n, self.dim_x, 1))\n    means_p = zeros((n, self.dim_x, 1))\n\n    # state covariances from Kalman Filter\n    covariances = zeros((n, self.dim_x, self.dim_x))\n    covariances_p = zeros((n, self.dim_x, self.dim_x))\n\n    if update_first:\n        for i, (z, r) in enumerate(zip(zs, Rs)):\n            self.update(z, r)\n            means[i, :] = self.x\n            covariances[i, :, :] = self.P\n\n            self.predict()\n            means_p[i, :] = self.x\n            covariances_p[i, :, :] = self.P\n    else:\n        for i, (z, r) in enumerate(zip(zs, Rs)):\n            self.predict()\n            means_p[i, :] = self.x\n            covariances_p[i, :, :] = self.P\n\n            self.update(z, r)\n            means[i, :] = self.x\n            covariances[i, :, :] = self.P\n\n    return (means, covariances, means_p, covariances_p)\n</code></pre>"},{"location":"filters/fading-kalman-filter/#bayesian_filters.kalman.fading_memory.FadingKalmanFilter.get_prediction","title":"<code>get_prediction(u=0)</code>","text":"<p>Predicts the next state of the filter and returns it. Does not alter the state of the filter.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>array</code> <p>optional control input</p> <code>0</code> <p>Returns:</p> Type Description <code>(x, P)</code> <p>State vector and covariance array of the prediction.</p> Source code in <code>bayesian_filters/kalman/fading_memory.py</code> <pre><code>def get_prediction(self, u=0):\n    \"\"\"Predicts the next state of the filter and returns it. Does not\n    alter the state of the filter.\n\n    Parameters\n    ----------\n\n    u : np.array\n        optional control input\n\n    Returns\n    -------\n\n    (x, P)\n        State vector and covariance array of the prediction.\n    \"\"\"\n\n    x = dot(self.F, self.x) + dot(self.B, u)\n    P = self.alpha_sq * dot(self.F, self.P).dot(self.F.T) + self.Q\n    return (x, P)\n</code></pre>"},{"location":"filters/fading-kalman-filter/#bayesian_filters.kalman.fading_memory.FadingKalmanFilter.measurement_of_state","title":"<code>measurement_of_state(x)</code>","text":"<p>Helper function that converts a state into a measurement.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>kalman state vector</p> required <p>Returns:</p> Name Type Description <code>z</code> <code>array</code> <p>measurement corresponding to the given state</p> Source code in <code>bayesian_filters/kalman/fading_memory.py</code> <pre><code>def measurement_of_state(self, x):\n    \"\"\"Helper function that converts a state into a measurement.\n\n    Parameters\n    ----------\n\n    x : np.array\n        kalman state vector\n\n    Returns\n    -------\n\n    z : np.array\n        measurement corresponding to the given state\n    \"\"\"\n    return dot(self.H, x)\n</code></pre>"},{"location":"filters/fading-kalman-filter/#bayesian_filters.kalman.fading_memory.FadingKalmanFilter.predict","title":"<code>predict(u=0)</code>","text":"<p>Predict next position.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>array</code> <p>Optional control vector. If non-zero, it is multiplied by B to create the control input into the system.</p> <code>0</code> Source code in <code>bayesian_filters/kalman/fading_memory.py</code> <pre><code>def predict(self, u=0):\n    \"\"\"Predict next position.\n\n    Parameters\n    ----------\n\n    u : np.array\n        Optional control vector. If non-zero, it is multiplied by B\n        to create the control input into the system.\n    \"\"\"\n\n    # x = Fx + Bu\n    self.x = dot(self.F, self.x) + dot(self.B, u)\n\n    # P = FPF' + Q\n    self.P = self.alpha_sq * dot(self.F, self.P).dot(self.F.T) + self.Q\n\n    # save prior\n    self.x_prior = self.x.copy()\n    self.P_prior = self.P.copy()\n</code></pre>"},{"location":"filters/fading-kalman-filter/#bayesian_filters.kalman.fading_memory.FadingKalmanFilter.residual_of","title":"<code>residual_of(z)</code>","text":"<p>returns the residual for the given measurement (z). Does not alter the state of the filter.</p> Source code in <code>bayesian_filters/kalman/fading_memory.py</code> <pre><code>def residual_of(self, z):\n    \"\"\"returns the residual for the given measurement (z). Does not alter\n    the state of the filter.\n    \"\"\"\n    return z - dot(self.H, self.x)\n</code></pre>"},{"location":"filters/fading-kalman-filter/#bayesian_filters.kalman.fading_memory.FadingKalmanFilter.update","title":"<code>update(z, R=None)</code>","text":"<p>Add a new measurement (z) to the kalman filter. If z is None, nothing is changed.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>array</code> <p>measurement for this update.</p> required <code>R</code> <code>np.array, scalar, or None</code> <p>Optionally provide R to override the measurement noise for this one call, otherwise  self.R will be used.</p> <code>None</code> Source code in <code>bayesian_filters/kalman/fading_memory.py</code> <pre><code>def update(self, z, R=None):\n    \"\"\"\n    Add a new measurement (z) to the kalman filter. If z is None, nothing\n    is changed.\n\n    Parameters\n    ----------\n\n    z : np.array\n        measurement for this update.\n\n    R : np.array, scalar, or None\n        Optionally provide R to override the measurement noise for this\n        one call, otherwise  self.R will be used.\n    \"\"\"\n\n    if z is None:\n        self.z = np.array([[None] * self.dim_z]).T\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n        return\n\n    if R is None:\n        R = self.R\n    elif np.isscalar(R):\n        R = eye(self.dim_z) * R\n\n    # y = z - Hx\n    # error (residual) between measurement and prediction\n    self.y = z - dot(self.H, self.x)\n\n    PHT = dot(self.P, self.H.T)\n\n    # S = HPH' + R\n    # project system uncertainty into measurement space\n    self.S = dot(self.H, PHT) + R\n    self.SI = linalg.inv(self.S)\n\n    # K = PH'inv(S)\n    # map system uncertainty into kalman gain\n    self.K = PHT.dot(self.SI)\n\n    # x = x + Ky\n    # predict new x with residual scaled by the kalman gain\n    self.x = self.x + dot(self.K, self.y)\n\n    # P = (I-KH)P(I-KH)' + KRK'\n    I_KH = self.I - dot(self.K, self.H)\n    self.P = dot(I_KH, self.P).dot(I_KH.T) + dot(self.K, R).dot(self.K.T)\n\n    # save measurement and posterior state\n    self.z = deepcopy(z)\n    self.x_post = self.x.copy()\n    self.P_post = self.P.copy()\n\n    # set to None to force recompute\n    self._log_likelihood = None\n    self._likelihood = None\n    self._mahalanobis = None\n</code></pre>"},{"location":"filters/h-infinity-filter/","title":"H-Infinity Filter","text":""},{"location":"filters/h-infinity-filter/#hinfinityfilter","title":"HInfinityFilter","text":"<p>H-Infinity filter implementation for robust state estimation.</p>"},{"location":"filters/h-infinity-filter/#api-reference","title":"API Reference","text":""},{"location":"filters/h-infinity-filter/#bayesian_filters.hinfinity.hinfinity_filter.HInfinityFilter","title":"<code>HInfinityFilter</code>","text":"<p>               Bases: <code>object</code></p> <p>H-Infinity filter. You are responsible for setting the various state variables to reasonable values; the defaults below will not give you a functional filter.</p> <p>Parameters:</p> Name Type Description Default <code>dim_x</code> <code>int</code> <p>Number of state variables for the Kalman filter. For example, if you are tracking the position and velocity of an object in two dimensions, dim_x would be 4.</p> <p>This is used to set the default size of <code>P</code>, <code>Q</code>, and <code>u</code></p> required <code>dim_z</code> <code>int</code> <p>Number of of measurement inputs. For example, if the sensor provides you with position in (x, y), <code>dim_z</code> would be 2.</p> required <code>dim_u</code> <code>int</code> <p>Number of control inputs for the Gu part of the prediction step.</p> required <code>gamma</code> <code>float</code> required <code>Warning</code> <p>I do not believe this code is correct. DO NOT USE THIS. In particular, note that predict does not update the covariance matrix.</p> required Source code in <code>bayesian_filters/hinfinity/hinfinity_filter.py</code> <pre><code>class HInfinityFilter(object):\n    \"\"\"\n    H-Infinity filter. You are responsible for setting the\n    various state variables to reasonable values; the defaults below will\n    not give you a functional filter.\n\n    Parameters\n    ----------\n\n    dim_x : int\n        Number of state variables for the Kalman filter. For example, if\n        you are tracking the position and velocity of an object in two\n        dimensions, dim_x would be 4.\n\n        This is used to set the default size of `P`, `Q`, and `u`\n\n    dim_z : int\n        Number of of measurement inputs. For example, if the sensor\n        provides you with position in (x, y), `dim_z` would be 2.\n\n    dim_u : int\n        Number of control inputs for the Gu part of the prediction step.\n\n    gamma : float\n\n    Warning:\n        I do not believe this code is correct. DO NOT USE THIS.\n        In particular, note that predict does not update the covariance\n        matrix.\n    \"\"\"\n\n    def __init__(self, dim_x, dim_z, dim_u, gamma):\n        warnings.warn(\"This code is likely incorrect. DO NOT USE.\", DeprecationWarning)\n\n        self.dim_x = dim_x\n        self.dim_z = dim_z\n        self.dim_u = dim_u\n        self.gamma = gamma\n\n        self.x = zeros((dim_x, 1))  # state\n\n        self.B = 0  # control transition matrix\n        self.F = eye(dim_x)  # state transition matrix\n        self.H = zeros((dim_z, dim_x))  # Measurement function\n        self.P = eye(dim_x)  # Uncertainty covariance.\n        self.Q = eye(dim_x)\n\n        self._V_inv = zeros((dim_z, dim_z))  # inverse measurement noise\n        self._V = zeros((dim_z, dim_z))  #  measurement noise\n        self.W = zeros((dim_x, dim_x))  # process uncertainty\n\n        # gain and residual are computed during the innovation step. We\n        # save them so that in case you want to inspect them for various\n        # purposes\n\n        self.K = 0  # H-infinity gain\n        self.y = zeros((dim_z, 1))\n        self.z = zeros((dim_z, 1))\n\n        # identity matrix. Do not alter this.\n        self._I = np.eye(dim_x)\n\n    def update(self, z):\n        \"\"\"\n        Add a new measurement `z` to the H-Infinity filter. If `z` is None,\n        nothing is changed.\n\n        Parameters\n        ----------\n        z : ndarray\n            measurement for this update.\n        \"\"\"\n\n        if z is None:\n            return\n\n        # rename for readability and a tiny extra bit of speed\n        I = self._I\n        gamma = self.gamma\n        Q = self.Q\n        H = self.H\n        P = self.P\n        x = self.x\n        V_inv = self._V_inv\n        F = self.F\n        W = self.W\n\n        # common subexpression H.T * V^-1\n        HTVI = dot(H.T, V_inv)\n\n        L = linalg.inv(I - gamma * dot(Q, P) + dot(HTVI, H).dot(P))\n\n        # common subexpression P*L\n        PL = dot(P, L)\n\n        K = dot(F, PL).dot(HTVI)\n\n        self.y = z - dot(H, x)\n\n        # x = x + Ky\n        # predict new x with residual scaled by the H-Infinity gain\n        self.x = self.x + dot(K, self.y)\n        self.P = dot(F, PL).dot(F.T) + W\n\n        # force P to be symmetric\n        self.P = (self.P + self.P.T) / 2\n\n        # pylint: disable=bare-except\n        try:\n            self.z = np.copy(z)\n        except:\n            self.z = copy.deepcopy(z)\n\n    def predict(self, u=0):\n        \"\"\"\n        Predict next position.\n\n        Parameters\n        ----------\n        u : ndarray\n            Optional control vector. If non-zero, it is multiplied by `B`\n            to create the control input into the system.\n        \"\"\"\n\n        # x = Fx + Bu\n        self.x = dot(self.F, self.x) + dot(self.B, u)\n\n    def batch_filter(self, Zs, update_first=False, saver=False):\n        \"\"\"Batch processes a sequences of measurements.\n\n        Parameters\n        ----------\n        Zs : list-like\n            list of measurements at each time step `self.dt` Missing\n            measurements must be represented by 'None'.\n\n        update_first : bool, default=False, optional,\n            controls whether the order of operations is update followed by\n            predict, or predict followed by update.\n\n        saver : filterpy.common.Saver, optional\n            filterpy.common.Saver object. If provided, saver.save() will be\n            called after every epoch\n\n        Returns\n        -------\n        means: ndarray ((n, dim_x, 1))\n            array of the state for each time step. Each entry is an np.array.\n            In other words `means[k,:]` is the state at step `k`.\n\n        covariance: ndarray((n, dim_x, dim_x))\n            array of the covariances for each time step. In other words\n            `covariance[k, :, :]` is the covariance at step `k`.\n        \"\"\"\n\n        n = np.size(Zs, 0)\n\n        # mean estimates from H-Infinity Filter\n        means = zeros((n, self.dim_x, 1))\n\n        # state covariances from H-Infinity Filter\n        covariances = zeros((n, self.dim_x, self.dim_x))\n\n        if update_first:\n            for i, z in enumerate(Zs):\n                self.update(z)\n                means[i, :] = self.x\n                covariances[i, :, :] = self.P\n                self.predict()\n\n                if saver is not None:\n                    saver.save()\n        else:\n            for i, z in enumerate(Zs):\n                self.predict()\n                self.update(z)\n\n                means[i, :] = self.x\n                covariances[i, :, :] = self.P\n\n                if saver is not None:\n                    saver.save()\n\n        return (means, covariances)\n\n    def get_prediction(self, u=0):\n        \"\"\"Predicts the next state of the filter and returns it. Does not\n        alter the state of the filter.\n\n        Parameters\n        ----------\n        u : ndarray\n            optional control input\n\n        Returns\n        -------\n        x : ndarray\n            State vector of the prediction.\n        \"\"\"\n        return dot(self.F, self.x) + dot(self.B, u)\n\n    def residual_of(self, z):\n        \"\"\"returns the residual for the given measurement (z). Does not alter\n        the state of the filter.\n        \"\"\"\n        return z - dot(self.H, self.x)\n\n    def measurement_of_state(self, x):\n        \"\"\"Helper function that converts a state into a measurement.\n\n        Parameters\n        ----------\n        x : ndarray\n            H-Infinity state vector\n\n        Returns\n        -------\n        z : ndarray\n            measurement corresponding to the given state\n        \"\"\"\n        return dot(self.H, x)\n\n    @property\n    def V(self):\n        \"\"\"measurement noise matrix\"\"\"\n        return self._V\n\n    @V.setter\n    def V(self, value):\n        \"\"\"measurement noise matrix\"\"\"\n\n        if np.isscalar(value):\n            self._V = np.array([[value]], dtype=float)\n        else:\n            self._V = value\n        self._V_inv = linalg.inv(self._V)\n\n    def __repr__(self):\n        return \"\\n\".join(\n            [\n                \"HInfinityFilter object\",\n                pretty_str(\"dim_x\", self.dim_x),\n                pretty_str(\"dim_z\", self.dim_z),\n                pretty_str(\"dim_u\", self.dim_u),\n                pretty_str(\"gamma\", self.dim_u),\n                pretty_str(\"x\", self.x),\n                pretty_str(\"P\", self.P),\n                pretty_str(\"F\", self.F),\n                pretty_str(\"Q\", self.Q),\n                pretty_str(\"V\", self.V),\n                pretty_str(\"W\", self.W),\n                pretty_str(\"K\", self.K),\n                pretty_str(\"y\", self.y),\n            ]\n        )\n</code></pre>"},{"location":"filters/h-infinity-filter/#bayesian_filters.hinfinity.hinfinity_filter.HInfinityFilter.V","title":"<code>V</code>  <code>property</code> <code>writable</code>","text":"<p>measurement noise matrix</p>"},{"location":"filters/h-infinity-filter/#bayesian_filters.hinfinity.hinfinity_filter.HInfinityFilter.batch_filter","title":"<code>batch_filter(Zs, update_first=False, saver=False)</code>","text":"<p>Batch processes a sequences of measurements.</p> <p>Parameters:</p> Name Type Description Default <code>Zs</code> <code>list - like</code> <p>list of measurements at each time step <code>self.dt</code> Missing measurements must be represented by 'None'.</p> required <code>update_first</code> <code>bool</code> <p>controls whether the order of operations is update followed by predict, or predict followed by update.</p> <code>False, optional,</code> <code>saver</code> <code>Saver</code> <p>filterpy.common.Saver object. If provided, saver.save() will be called after every epoch</p> <code>False</code> <p>Returns:</p> Name Type Description <code>means</code> <code>ndarray((n, dim_x, 1))</code> <p>array of the state for each time step. Each entry is an np.array. In other words <code>means[k,:]</code> is the state at step <code>k</code>.</p> <code>covariance</code> <code>ndarray((n, dim_x, dim_x))</code> <p>array of the covariances for each time step. In other words <code>covariance[k, :, :]</code> is the covariance at step <code>k</code>.</p> Source code in <code>bayesian_filters/hinfinity/hinfinity_filter.py</code> <pre><code>def batch_filter(self, Zs, update_first=False, saver=False):\n    \"\"\"Batch processes a sequences of measurements.\n\n    Parameters\n    ----------\n    Zs : list-like\n        list of measurements at each time step `self.dt` Missing\n        measurements must be represented by 'None'.\n\n    update_first : bool, default=False, optional,\n        controls whether the order of operations is update followed by\n        predict, or predict followed by update.\n\n    saver : filterpy.common.Saver, optional\n        filterpy.common.Saver object. If provided, saver.save() will be\n        called after every epoch\n\n    Returns\n    -------\n    means: ndarray ((n, dim_x, 1))\n        array of the state for each time step. Each entry is an np.array.\n        In other words `means[k,:]` is the state at step `k`.\n\n    covariance: ndarray((n, dim_x, dim_x))\n        array of the covariances for each time step. In other words\n        `covariance[k, :, :]` is the covariance at step `k`.\n    \"\"\"\n\n    n = np.size(Zs, 0)\n\n    # mean estimates from H-Infinity Filter\n    means = zeros((n, self.dim_x, 1))\n\n    # state covariances from H-Infinity Filter\n    covariances = zeros((n, self.dim_x, self.dim_x))\n\n    if update_first:\n        for i, z in enumerate(Zs):\n            self.update(z)\n            means[i, :] = self.x\n            covariances[i, :, :] = self.P\n            self.predict()\n\n            if saver is not None:\n                saver.save()\n    else:\n        for i, z in enumerate(Zs):\n            self.predict()\n            self.update(z)\n\n            means[i, :] = self.x\n            covariances[i, :, :] = self.P\n\n            if saver is not None:\n                saver.save()\n\n    return (means, covariances)\n</code></pre>"},{"location":"filters/h-infinity-filter/#bayesian_filters.hinfinity.hinfinity_filter.HInfinityFilter.get_prediction","title":"<code>get_prediction(u=0)</code>","text":"<p>Predicts the next state of the filter and returns it. Does not alter the state of the filter.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>ndarray</code> <p>optional control input</p> <code>0</code> <p>Returns:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>State vector of the prediction.</p> Source code in <code>bayesian_filters/hinfinity/hinfinity_filter.py</code> <pre><code>def get_prediction(self, u=0):\n    \"\"\"Predicts the next state of the filter and returns it. Does not\n    alter the state of the filter.\n\n    Parameters\n    ----------\n    u : ndarray\n        optional control input\n\n    Returns\n    -------\n    x : ndarray\n        State vector of the prediction.\n    \"\"\"\n    return dot(self.F, self.x) + dot(self.B, u)\n</code></pre>"},{"location":"filters/h-infinity-filter/#bayesian_filters.hinfinity.hinfinity_filter.HInfinityFilter.measurement_of_state","title":"<code>measurement_of_state(x)</code>","text":"<p>Helper function that converts a state into a measurement.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>H-Infinity state vector</p> required <p>Returns:</p> Name Type Description <code>z</code> <code>ndarray</code> <p>measurement corresponding to the given state</p> Source code in <code>bayesian_filters/hinfinity/hinfinity_filter.py</code> <pre><code>def measurement_of_state(self, x):\n    \"\"\"Helper function that converts a state into a measurement.\n\n    Parameters\n    ----------\n    x : ndarray\n        H-Infinity state vector\n\n    Returns\n    -------\n    z : ndarray\n        measurement corresponding to the given state\n    \"\"\"\n    return dot(self.H, x)\n</code></pre>"},{"location":"filters/h-infinity-filter/#bayesian_filters.hinfinity.hinfinity_filter.HInfinityFilter.predict","title":"<code>predict(u=0)</code>","text":"<p>Predict next position.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>ndarray</code> <p>Optional control vector. If non-zero, it is multiplied by <code>B</code> to create the control input into the system.</p> <code>0</code> Source code in <code>bayesian_filters/hinfinity/hinfinity_filter.py</code> <pre><code>def predict(self, u=0):\n    \"\"\"\n    Predict next position.\n\n    Parameters\n    ----------\n    u : ndarray\n        Optional control vector. If non-zero, it is multiplied by `B`\n        to create the control input into the system.\n    \"\"\"\n\n    # x = Fx + Bu\n    self.x = dot(self.F, self.x) + dot(self.B, u)\n</code></pre>"},{"location":"filters/h-infinity-filter/#bayesian_filters.hinfinity.hinfinity_filter.HInfinityFilter.residual_of","title":"<code>residual_of(z)</code>","text":"<p>returns the residual for the given measurement (z). Does not alter the state of the filter.</p> Source code in <code>bayesian_filters/hinfinity/hinfinity_filter.py</code> <pre><code>def residual_of(self, z):\n    \"\"\"returns the residual for the given measurement (z). Does not alter\n    the state of the filter.\n    \"\"\"\n    return z - dot(self.H, self.x)\n</code></pre>"},{"location":"filters/h-infinity-filter/#bayesian_filters.hinfinity.hinfinity_filter.HInfinityFilter.update","title":"<code>update(z)</code>","text":"<p>Add a new measurement <code>z</code> to the H-Infinity filter. If <code>z</code> is None, nothing is changed.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>ndarray</code> <p>measurement for this update.</p> required Source code in <code>bayesian_filters/hinfinity/hinfinity_filter.py</code> <pre><code>def update(self, z):\n    \"\"\"\n    Add a new measurement `z` to the H-Infinity filter. If `z` is None,\n    nothing is changed.\n\n    Parameters\n    ----------\n    z : ndarray\n        measurement for this update.\n    \"\"\"\n\n    if z is None:\n        return\n\n    # rename for readability and a tiny extra bit of speed\n    I = self._I\n    gamma = self.gamma\n    Q = self.Q\n    H = self.H\n    P = self.P\n    x = self.x\n    V_inv = self._V_inv\n    F = self.F\n    W = self.W\n\n    # common subexpression H.T * V^-1\n    HTVI = dot(H.T, V_inv)\n\n    L = linalg.inv(I - gamma * dot(Q, P) + dot(HTVI, H).dot(P))\n\n    # common subexpression P*L\n    PL = dot(P, L)\n\n    K = dot(F, PL).dot(HTVI)\n\n    self.y = z - dot(H, x)\n\n    # x = x + Ky\n    # predict new x with residual scaled by the H-Infinity gain\n    self.x = self.x + dot(K, self.y)\n    self.P = dot(F, PL).dot(F.T) + W\n\n    # force P to be symmetric\n    self.P = (self.P + self.P.T) / 2\n\n    # pylint: disable=bare-except\n    try:\n        self.z = np.copy(z)\n    except:\n        self.z = copy.deepcopy(z)\n</code></pre>"},{"location":"filters/imm-estimator/","title":"IMM Estimator","text":""},{"location":"filters/imm-estimator/#imm-estimator","title":"IMM Estimator","text":"<p>needs documentation....</p>"},{"location":"filters/imm-estimator/#api-reference","title":"API Reference","text":""},{"location":"filters/imm-estimator/#bayesian_filters.kalman.IMM.IMMEstimator","title":"<code>IMMEstimator</code>","text":"<p>               Bases: <code>object</code></p> <p>Implements an Interacting Multiple-Model (IMM) estimator.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>(N,) array_like of KalmanFilter objects</code> <p>List of N filters. filters[i] is the ith Kalman filter in the IMM estimator.</p> <p>Each filter must have the same dimension for the state <code>x</code> and <code>P</code>, otherwise the states of each filter cannot be mixed with each other.</p> required <code>mu</code> <code>(N,) array_like of float</code> <p>mode probability: mu[i] is the probability that filter i is the correct one.</p> required <code>M</code> <code>(N, N) ndarray of float</code> <p>Markov chain transition matrix. M[i,j] is the probability of switching from filter j to filter i.</p> required <p>Attributes:</p> Name Type Description <code>x</code> <code>array(dim_x, 1)</code> <p>Current state estimate. Any call to update() or predict() updates this variable.</p> <code>P</code> <code>array(dim_x, dim_x)</code> <p>Current state covariance matrix. Any call to update() or predict() updates this variable.</p> <code>x_prior</code> <code>array(dim_x, 1)</code> <p>Prior (predicted) state estimate. The _prior and _post attributes are for convienence; they store the  prior and posterior of the current epoch. Read Only.</p> <code>P_prior</code> <code>array(dim_x, dim_x)</code> <p>Prior (predicted) state covariance matrix. Read Only.</p> <code>x_post</code> <code>array(dim_x, 1)</code> <p>Posterior (updated) state estimate. Read Only.</p> <code>P_post</code> <code>array(dim_x, dim_x)</code> <p>Posterior (updated) state covariance matrix. Read Only.</p> <code>N</code> <code>int</code> <p>number of filters in the filter bank</p> <code>mu</code> <code>(N,) ndarray of float</code> <p>mode probability: mu[i] is the probability that filter i is the correct one.</p> <code>M</code> <code>(N, N) ndarray of float</code> <p>Markov chain transition matrix. M[i,j] is the probability of switching from filter j to filter i.</p> <code>cbar</code> <code>(N,) ndarray of float</code> <p>Total probability, after interaction, that the target is in state j. We use it as the # normalization constant.</p> <code>likelihood</code> <code>(N,) ndarray of float</code> <p>Likelihood of each individual filter's last measurement.</p> <code>omega</code> <code>(N, N) ndarray of float</code> <p>Mixing probabilitity - omega[i, j] is the probabilility of mixing the state of filter i into filter j. Perhaps more understandably, it weights the states of each filter by:     x_j = sum(omega[i,j] * x_i)</p> <p>with a similar weighting for P_j</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from bayesian_filters.common import kinematic_kf\n&gt;&gt;&gt; from bayesian_filters.kalman import IMMEstimator\n&gt;&gt;&gt; kf1 = kinematic_kf(2, 2)\n&gt;&gt;&gt; kf2 = kinematic_kf(2, 2)\n&gt;&gt;&gt; # do some settings of x, R, P etc. here, I'll just use the defaults\n&gt;&gt;&gt; kf2.Q *= 0   # no prediction error in second filter\n&gt;&gt;&gt;\n&gt;&gt;&gt; filters = [kf1, kf2]\n&gt;&gt;&gt; mu = [0.5, 0.5]  # each filter is equally likely at the start\n&gt;&gt;&gt; trans = np.array([[0.97, 0.03], [0.03, 0.97]])\n&gt;&gt;&gt; imm = IMMEstimator(filters, mu, trans)\n&gt;&gt;&gt;\n&gt;&gt;&gt; for i in range(100):\n&gt;&gt;&gt;     # make some noisy data\n&gt;&gt;&gt;     x = i + np.random.randn()*np.sqrt(kf1.R[0, 0])\n&gt;&gt;&gt;     y = i + np.random.randn()*np.sqrt(kf1.R[1, 1])\n&gt;&gt;&gt;     z = np.array([[x], [y]])\n&gt;&gt;&gt;\n&gt;&gt;&gt;     # perform predict/update cycle\n&gt;&gt;&gt;     imm.predict()\n&gt;&gt;&gt;     imm.update(z)\n&gt;&gt;&gt;     print(imm.x.T)\n</code></pre> <p>For a full explanation and more examples see my book Kalman and Bayesian Filters in Python https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> References <p>Bar-Shalom, Y., Li, X-R., and Kirubarajan, T. \"Estimation with Application to Tracking and Navigation\". Wiley-Interscience, 2001.</p> <p>Crassidis, J and Junkins, J. \"Optimal Estimation of Dynamic Systems\". CRC Press, second edition. 2012.</p> <p>Labbe, R. \"Kalman and Bayesian Filters in Python\". https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> Source code in <code>bayesian_filters/kalman/IMM.py</code> <pre><code>class IMMEstimator(object):\n    \"\"\"Implements an Interacting Multiple-Model (IMM) estimator.\n\n    Parameters\n    ----------\n\n    filters : (N,) array_like of KalmanFilter objects\n        List of N filters. filters[i] is the ith Kalman filter in the\n        IMM estimator.\n\n        Each filter must have the same dimension for the state `x` and `P`,\n        otherwise the states of each filter cannot be mixed with each other.\n\n    mu : (N,) array_like of float\n        mode probability: mu[i] is the probability that\n        filter i is the correct one.\n\n    M : (N, N) ndarray of float\n        Markov chain transition matrix. M[i,j] is the probability of\n        switching from filter j to filter i.\n\n\n    Attributes\n    ----------\n    x : numpy.array(dim_x, 1)\n        Current state estimate. Any call to update() or predict() updates\n        this variable.\n\n    P : numpy.array(dim_x, dim_x)\n        Current state covariance matrix. Any call to update() or predict()\n        updates this variable.\n\n    x_prior : numpy.array(dim_x, 1)\n        Prior (predicted) state estimate. The *_prior and *_post attributes\n        are for convienence; they store the  prior and posterior of the\n        current epoch. Read Only.\n\n    P_prior : numpy.array(dim_x, dim_x)\n        Prior (predicted) state covariance matrix. Read Only.\n\n    x_post : numpy.array(dim_x, 1)\n        Posterior (updated) state estimate. Read Only.\n\n    P_post : numpy.array(dim_x, dim_x)\n        Posterior (updated) state covariance matrix. Read Only.\n\n    N : int\n        number of filters in the filter bank\n\n    mu : (N,) ndarray of float\n        mode probability: mu[i] is the probability that\n        filter i is the correct one.\n\n    M : (N, N) ndarray of float\n        Markov chain transition matrix. M[i,j] is the probability of\n        switching from filter j to filter i.\n\n    cbar : (N,) ndarray of float\n        Total probability, after interaction, that the target is in state j.\n        We use it as the # normalization constant.\n\n    likelihood: (N,) ndarray of float\n        Likelihood of each individual filter's last measurement.\n\n    omega : (N, N) ndarray of float\n        Mixing probabilitity - omega[i, j] is the probabilility of mixing\n        the state of filter i into filter j. Perhaps more understandably,\n        it weights the states of each filter by:\n            x_j = sum(omega[i,j] * x_i)\n\n        with a similar weighting for P_j\n\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from bayesian_filters.common import kinematic_kf\n    &gt;&gt;&gt; from bayesian_filters.kalman import IMMEstimator\n    &gt;&gt;&gt; kf1 = kinematic_kf(2, 2)\n    &gt;&gt;&gt; kf2 = kinematic_kf(2, 2)\n    &gt;&gt;&gt; # do some settings of x, R, P etc. here, I'll just use the defaults\n    &gt;&gt;&gt; kf2.Q *= 0   # no prediction error in second filter\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; filters = [kf1, kf2]\n    &gt;&gt;&gt; mu = [0.5, 0.5]  # each filter is equally likely at the start\n    &gt;&gt;&gt; trans = np.array([[0.97, 0.03], [0.03, 0.97]])\n    &gt;&gt;&gt; imm = IMMEstimator(filters, mu, trans)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; for i in range(100):\n    &gt;&gt;&gt;     # make some noisy data\n    &gt;&gt;&gt;     x = i + np.random.randn()*np.sqrt(kf1.R[0, 0])\n    &gt;&gt;&gt;     y = i + np.random.randn()*np.sqrt(kf1.R[1, 1])\n    &gt;&gt;&gt;     z = np.array([[x], [y]])\n    &gt;&gt;&gt;\n    &gt;&gt;&gt;     # perform predict/update cycle\n    &gt;&gt;&gt;     imm.predict()\n    &gt;&gt;&gt;     imm.update(z)\n    &gt;&gt;&gt;     print(imm.x.T)\n\n    For a full explanation and more examples see my book\n    Kalman and Bayesian Filters in Python\n    https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n\n\n    References\n    ----------\n\n    Bar-Shalom, Y., Li, X-R., and Kirubarajan, T. \"Estimation with\n    Application to Tracking and Navigation\". Wiley-Interscience, 2001.\n\n    Crassidis, J and Junkins, J. \"Optimal Estimation of\n    Dynamic Systems\". CRC Press, second edition. 2012.\n\n    Labbe, R. \"Kalman and Bayesian Filters in Python\".\n    https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n    \"\"\"\n\n    def __init__(self, filters, mu, M):\n        if len(filters) &lt; 2:\n            raise ValueError(\"filters must contain at least two filters\")\n\n        self.filters = filters\n        self.mu = asarray(mu) / np.sum(mu)\n        self.M = M\n\n        x_shape = filters[0].x.shape\n        for f in filters:\n            if x_shape != f.x.shape:\n                raise ValueError(\"All filters must have the same state dimension\")\n\n        self.x = zeros(filters[0].x.shape)\n        self.P = zeros(filters[0].P.shape)\n        self.N = len(filters)  # number of filters\n        self.likelihood = zeros(self.N)\n        self.omega = zeros((self.N, self.N))\n        self._compute_mixing_probabilities()\n\n        # initialize imm state estimate based on current filters\n        self._compute_state_estimate()\n        self.x_prior = self.x.copy()\n        self.P_prior = self.P.copy()\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n\n    def update(self, z):\n        \"\"\"\n        Add a new measurement (z) to the Kalman filter. If z is None, nothing\n        is changed.\n\n        Parameters\n        ----------\n\n        z : np.array\n            measurement for this update.\n        \"\"\"\n\n        # run update on each filter, and save the likelihood\n        for i, f in enumerate(self.filters):\n            f.update(z)\n            self.likelihood[i] = f.likelihood\n\n        # update mode probabilities from total probability * likelihood\n        self.mu = self.cbar * self.likelihood\n        self.mu /= np.sum(self.mu)  # normalize\n\n        self._compute_mixing_probabilities()\n\n        # compute mixed IMM state and covariance and save posterior estimate\n        self._compute_state_estimate()\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n\n    def predict(self, u=None):\n        \"\"\"\n        Predict next state (prior) using the IMM state propagation\n        equations.\n\n        Parameters\n        ----------\n\n        u : np.array, optional\n            Control vector. If not `None`, it is multiplied by B\n            to create the control input into the system.\n        \"\"\"\n\n        # compute mixed initial conditions\n        xs, Ps = [], []\n        for i, (f, w) in enumerate(zip(self.filters, self.omega.T)):\n            x = zeros(self.x.shape)\n            for kf, wj in zip(self.filters, w):\n                x += kf.x * wj\n            xs.append(x)\n\n            P = zeros(self.P.shape)\n            for kf, wj in zip(self.filters, w):\n                y = kf.x - x\n                P += wj * (outer(y, y) + kf.P)\n            Ps.append(P)\n\n        #  compute each filter's prior using the mixed initial conditions\n        for i, f in enumerate(self.filters):\n            # propagate using the mixed state estimate and covariance\n            f.x = xs[i].copy()\n            f.P = Ps[i].copy()\n            f.predict(u)\n\n        # compute mixed IMM state and covariance and save posterior estimate\n        self._compute_state_estimate()\n        self.x_prior = self.x.copy()\n        self.P_prior = self.P.copy()\n\n    def _compute_state_estimate(self):\n        \"\"\"\n        Computes the IMM's mixed state estimate from each filter using\n        the the mode probability self.mu to weight the estimates.\n        \"\"\"\n        self.x.fill(0)\n        for f, mu in zip(self.filters, self.mu):\n            self.x += f.x * mu\n\n        self.P.fill(0)\n        for f, mu in zip(self.filters, self.mu):\n            y = f.x - self.x\n            self.P += mu * (outer(y, y) + f.P)\n\n    def _compute_mixing_probabilities(self):\n        \"\"\"\n        Compute the mixing probability for each filter.\n        \"\"\"\n\n        self.cbar = dot(self.mu, self.M)\n        for i in range(self.N):\n            for j in range(self.N):\n                self.omega[i, j] = (self.M[i, j] * self.mu[i]) / self.cbar[j]\n\n    def __repr__(self):\n        return \"\\n\".join(\n            [\n                \"IMMEstimator object\",\n                pretty_str(\"x\", self.x),\n                pretty_str(\"P\", self.P),\n                pretty_str(\"x_prior\", self.x_prior),\n                pretty_str(\"P_prior\", self.P_prior),\n                pretty_str(\"x_post\", self.x_post),\n                pretty_str(\"P_post\", self.P_post),\n                pretty_str(\"N\", self.N),\n                pretty_str(\"mu\", self.mu),\n                pretty_str(\"M\", self.M),\n                pretty_str(\"cbar\", self.cbar),\n                pretty_str(\"likelihood\", self.likelihood),\n                pretty_str(\"omega\", self.omega),\n            ]\n        )\n</code></pre>"},{"location":"filters/imm-estimator/#bayesian_filters.kalman.IMM.IMMEstimator.predict","title":"<code>predict(u=None)</code>","text":"<p>Predict next state (prior) using the IMM state propagation equations.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>array</code> <p>Control vector. If not <code>None</code>, it is multiplied by B to create the control input into the system.</p> <code>None</code> Source code in <code>bayesian_filters/kalman/IMM.py</code> <pre><code>def predict(self, u=None):\n    \"\"\"\n    Predict next state (prior) using the IMM state propagation\n    equations.\n\n    Parameters\n    ----------\n\n    u : np.array, optional\n        Control vector. If not `None`, it is multiplied by B\n        to create the control input into the system.\n    \"\"\"\n\n    # compute mixed initial conditions\n    xs, Ps = [], []\n    for i, (f, w) in enumerate(zip(self.filters, self.omega.T)):\n        x = zeros(self.x.shape)\n        for kf, wj in zip(self.filters, w):\n            x += kf.x * wj\n        xs.append(x)\n\n        P = zeros(self.P.shape)\n        for kf, wj in zip(self.filters, w):\n            y = kf.x - x\n            P += wj * (outer(y, y) + kf.P)\n        Ps.append(P)\n\n    #  compute each filter's prior using the mixed initial conditions\n    for i, f in enumerate(self.filters):\n        # propagate using the mixed state estimate and covariance\n        f.x = xs[i].copy()\n        f.P = Ps[i].copy()\n        f.predict(u)\n\n    # compute mixed IMM state and covariance and save posterior estimate\n    self._compute_state_estimate()\n    self.x_prior = self.x.copy()\n    self.P_prior = self.P.copy()\n</code></pre>"},{"location":"filters/imm-estimator/#bayesian_filters.kalman.IMM.IMMEstimator.update","title":"<code>update(z)</code>","text":"<p>Add a new measurement (z) to the Kalman filter. If z is None, nothing is changed.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>array</code> <p>measurement for this update.</p> required Source code in <code>bayesian_filters/kalman/IMM.py</code> <pre><code>def update(self, z):\n    \"\"\"\n    Add a new measurement (z) to the Kalman filter. If z is None, nothing\n    is changed.\n\n    Parameters\n    ----------\n\n    z : np.array\n        measurement for this update.\n    \"\"\"\n\n    # run update on each filter, and save the likelihood\n    for i, f in enumerate(self.filters):\n        f.update(z)\n        self.likelihood[i] = f.likelihood\n\n    # update mode probabilities from total probability * likelihood\n    self.mu = self.cbar * self.likelihood\n    self.mu /= np.sum(self.mu)  # normalize\n\n    self._compute_mixing_probabilities()\n\n    # compute mixed IMM state and covariance and save posterior estimate\n    self._compute_state_estimate()\n    self.x_post = self.x.copy()\n    self.P_post = self.P.copy()\n</code></pre>"},{"location":"filters/information-filter/","title":"Information Filter","text":""},{"location":"filters/information-filter/#informationfilter","title":"InformationFilter","text":""},{"location":"filters/information-filter/#introduction-and-overview","title":"Introduction and Overview","text":"<p>This is a basic implementation of the information filter.</p>"},{"location":"filters/information-filter/#api-reference","title":"API Reference","text":""},{"location":"filters/information-filter/#bayesian_filters.kalman.information_filter.InformationFilter","title":"<code>InformationFilter</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a linear Information filter. Information filters compute the inverse of the Kalman filter, allowing you to easily denote having no information at initialization.</p> <p>You are responsible for setting the various state variables to reasonable values; the defaults below will not give you a functional filter.</p> <p>Parameters:</p> Name Type Description Default <code>dim_x</code> <code>int</code> <p>Number of state variables for the  filter. For example, if you are tracking the position and velocity of an object in two dimensions, dim_x would be 4.</p> <p>This is used to set the default size of P, Q, and u</p> required <code>dim_z</code> <code>int</code> <p>Number of of measurement inputs. For example, if the sensor provides you with position in (x,y), dim_z would be 2.</p> required <code>dim_u</code> <code>int(optional)</code> <p>size of the control input, if it is being used. Default value of 0 indicates it is not used.</p> <code>0</code> <code>self</code> required <code>self</code> required <p>Attributes:</p> Name Type Description <code>x</code> <code>array(dim_x, 1)</code> <p>State estimate vector</p> <code>P_inv</code> <code>array(dim_x, dim_x)</code> <p>inverse state covariance matrix</p> <code>x_prior</code> <code>array(dim_x, 1)</code> <p>Prior (predicted) state estimate. The _prior and _post attributes are for convienence; they store the  prior and posterior of the current epoch. Read Only.</p> <code>P_inv_prior</code> <code>array(dim_x, dim_x)</code> <p>Inverse prior (predicted) state covariance matrix. Read Only.</p> <code>x_post</code> <code>array(dim_x, 1)</code> <p>Posterior (updated) state estimate. Read Only.</p> <code>P_inv_post</code> <code>array(dim_x, dim_x)</code> <p>Inverse posterior (updated) state covariance matrix. Read Only.</p> <code>z</code> <code>ndarray</code> <p>Last measurement used in update(). Read only.</p> <code>R_inv</code> <code>array(dim_z, dim_z)</code> <p>inverse of measurement noise matrix</p> <code>Q</code> <code>array(dim_x, dim_x)</code> <p>Process noise matrix</p> <code>H</code> <code>array(dim_z, dim_x)</code> <p>Measurement function</p> <code>y</code> <code>array</code> <p>Residual of the update step. Read only.</p> <code>K</code> <code>array(dim_x, dim_z)</code> <p>Kalman gain of the update step. Read only.</p> <code>S</code> <code>array</code> <p>Systen uncertaintly projected to measurement space. Read only.</p> <code>log_likelihood</code> <code>float</code> <p>log-likelihood of the last measurement. Read only.</p> <code>likelihood</code> <code>float</code> <p>likelihood of last measurment. Read only.</p> <p>Computed from the log-likelihood. The log-likelihood can be very small,  meaning a large negative value such as -28000. Taking the exp() of that results in 0.0, which can break typical algorithms which multiply by this value, so by default we always return a number &gt;= sys.float_info.min.</p> <code>inv</code> <code>function, default numpy.linalg.inv</code> <p>If you prefer another inverse function, such as the Moore-Penrose pseudo inverse, set it to that instead: kf.inv = np.linalg.pinv</p> <p>Examples:</p> <p>See my book Kalman and Bayesian Filters in Python https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> Source code in <code>bayesian_filters/kalman/information_filter.py</code> <pre><code>class InformationFilter(object):\n    \"\"\"\n    Create a linear Information filter. Information filters\n    compute the\n    inverse of the Kalman filter, allowing you to easily denote having\n    no information at initialization.\n\n    You are responsible for setting the various state variables to reasonable\n    values; the defaults below will not give you a functional filter.\n\n    Parameters\n    ----------\n\n    dim_x : int\n        Number of state variables for the  filter. For example, if you\n        are tracking the position and velocity of an object in two\n        dimensions, dim_x would be 4.\n\n        This is used to set the default size of P, Q, and u\n\n    dim_z : int\n        Number of of measurement inputs. For example, if the sensor\n        provides you with position in (x,y), dim_z would be 2.\n\n    dim_u : int (optional)\n        size of the control input, if it is being used.\n        Default value of 0 indicates it is not used.\n\n    self.compute_log_likelihood = compute_log_likelihood\n    self.log_likelihood = math.log(sys.float_info.min)\n\n\n    Attributes\n    ----------\n    x : numpy.array(dim_x, 1)\n        State estimate vector\n\n    P_inv : numpy.array(dim_x, dim_x)\n        inverse state covariance matrix\n\n    x_prior : numpy.array(dim_x, 1)\n        Prior (predicted) state estimate. The *_prior and *_post attributes\n        are for convienence; they store the  prior and posterior of the\n        current epoch. Read Only.\n\n    P_inv_prior : numpy.array(dim_x, dim_x)\n        Inverse prior (predicted) state covariance matrix. Read Only.\n\n    x_post : numpy.array(dim_x, 1)\n        Posterior (updated) state estimate. Read Only.\n\n    P_inv_post : numpy.array(dim_x, dim_x)\n        Inverse posterior (updated) state covariance matrix. Read Only.\n\n    z : ndarray\n        Last measurement used in update(). Read only.\n\n    R_inv : numpy.array(dim_z, dim_z)\n        inverse of measurement noise matrix\n\n    Q : numpy.array(dim_x, dim_x)\n        Process noise matrix\n\n    H : numpy.array(dim_z, dim_x)\n        Measurement function\n\n    y : numpy.array\n        Residual of the update step. Read only.\n\n    K : numpy.array(dim_x, dim_z)\n        Kalman gain of the update step. Read only.\n\n    S :  numpy.array\n        Systen uncertaintly projected to measurement space. Read only.\n\n    log_likelihood : float\n        log-likelihood of the last measurement. Read only.\n\n    likelihood : float\n        likelihood of last measurment. Read only.\n\n        Computed from the log-likelihood. The log-likelihood can be very\n        small,  meaning a large negative value such as -28000. Taking the\n        exp() of that results in 0.0, which can break typical algorithms\n        which multiply by this value, so by default we always return a\n        number &gt;= sys.float_info.min.\n\n    inv : function, default numpy.linalg.inv\n        If you prefer another inverse function, such as the Moore-Penrose\n        pseudo inverse, set it to that instead: kf.inv = np.linalg.pinv\n\n\n    Examples\n    --------\n\n    See my book Kalman and Bayesian Filters in Python\n    https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n    \"\"\"\n\n    def __init__(self, dim_x, dim_z, dim_u=0, compute_log_likelihood=True):\n        if dim_x &lt; 1:\n            raise ValueError(\"dim_x must be 1 or greater\")\n        if dim_z &lt; 1:\n            raise ValueError(\"dim_z must be 1 or greater\")\n        if dim_u &lt; 0:\n            raise ValueError(\"dim_u must be 0 or greater\")\n\n        self.dim_x = dim_x\n        self.dim_z = dim_z\n        self.dim_u = dim_u\n\n        self.x = zeros((dim_x, 1))  # state\n        self.P_inv = eye(dim_x)  # uncertainty covariance\n        self.Q = eye(dim_x)  # process uncertainty\n        self.B = 0.0  # control transition matrix\n        self._F = 0.0  # state transition matrix\n        self._F_inv = 0.0  # state transition matrix\n        self.H = np.zeros((dim_z, dim_x))  # Measurement function\n        self.R_inv = eye(dim_z)  # state uncertainty\n        self.z = np.array([[None] * self.dim_z]).T\n\n        # gain and residual are computed during the innovation step. We\n        # save them so that in case you want to inspect them for various\n        # purposes\n        self.K = 0.0  # kalman gain\n        self.y = zeros((dim_z, 1))\n        self.z = zeros((dim_z, 1))\n        self.S = 0.0  # system uncertainty in measurement space\n\n        # identity matrix. Do not alter this.\n        self._I = np.eye(dim_x)\n        self._no_information = False\n\n        self.compute_log_likelihood = compute_log_likelihood\n        self.log_likelihood = math.log(sys.float_info.min)\n        self.likelihood = sys.float_info.min\n\n        self.inv = np.linalg.inv\n\n        # save priors and posteriors\n        self.x_prior = np.copy(self.x)\n        self.P_inv_prior = np.copy(self.P_inv)\n        self.x_post = np.copy(self.x)\n        self.P_inv_post = np.copy(self.P_inv)\n\n    def update(self, z, R_inv=None):\n        \"\"\"\n        Add a new measurement (z) to the kalman filter. If z is None, nothing\n        is changed.\n\n        Parameters\n        ----------\n\n        z : np.array\n            measurement for this update.\n\n        R : np.array, scalar, or None\n            Optionally provide R to override the measurement noise for this\n            one call, otherwise  self.R will be used.\n        \"\"\"\n\n        if z is None:\n            self.z = None\n            self.x_post = self.x.copy()\n            self.P_inv_post = self.P_inv.copy()\n            return\n\n        if R_inv is None:\n            R_inv = self.R_inv\n        elif np.isscalar(R_inv):\n            R_inv = eye(self.dim_z) * R_inv\n\n        # rename for readability and a tiny extra bit of speed\n        H = self.H\n        H_T = H.T\n        P_inv = self.P_inv\n        x = self.x\n\n        if self._no_information:\n            self.x = dot(P_inv, x) + dot(H_T, R_inv).dot(z)\n            self.P_inv = P_inv + dot(H_T, R_inv).dot(H)\n            self.log_likelihood = math.log(sys.float_info.min)\n            self.likelihood = sys.float_info.min\n\n        else:\n            # y = z - Hx\n            # error (residual) between measurement and prediction\n            self.y = z - dot(H, x)\n\n            # S = HPH' + R\n            # project system uncertainty into measurement space\n            self.S = P_inv + dot(H_T, R_inv).dot(H)\n            self.K = dot(self.inv(self.S), H_T).dot(R_inv)\n\n            # x = x + Ky\n            # predict new x with residual scaled by the kalman gain\n            self.x = x + dot(self.K, self.y)\n            self.P_inv = P_inv + dot(H_T, R_inv).dot(H)\n\n            self.z = np.copy(reshape_z(z, self.dim_z, np.ndim(self.x)))\n\n            if self.compute_log_likelihood:\n                self.log_likelihood = logpdf(x=self.y, cov=self.S)\n                self.likelihood = math.exp(self.log_likelihood)\n                if self.likelihood == 0:\n                    self.likelihood = sys.float_info.min\n\n        # save measurement and posterior state\n        self.z = deepcopy(z)\n        self.x_post = self.x.copy()\n        self.P_inv_post = self.P_inv.copy()\n\n    def predict(self, u=0):\n        \"\"\"Predict next position.\n\n        Parameters\n        ----------\n\n        u : ndarray\n            Optional control vector. If non-zero, it is multiplied by B\n            to create the control input into the system.\n        \"\"\"\n\n        # x = Fx + Bu\n\n        A = dot(self._F_inv.T, self.P_inv).dot(self._F_inv)\n        # pylint: disable=bare-except\n        try:\n            AI = self.inv(A)\n            invertable = True\n            if self._no_information:\n                try:\n                    self.x = dot(self.inv(self.P_inv), self.x)\n                except:\n                    self.x = dot(0, self.x)\n                self._no_information = False\n        except:\n            invertable = False\n            self._no_information = True\n\n        if invertable:\n            self.x = dot(self._F, self.x) + dot(self.B, u)\n            self.P_inv = self.inv(AI + self.Q)\n\n            # save priors\n            self.P_inv_prior = np.copy(self.P_inv)\n            self.x_prior = np.copy(self.x)\n        else:\n            I_PF = self._I - dot(self.P_inv, self._F_inv)\n            FTI = self.inv(self._F.T)\n            FTIX = dot(FTI, self.x)\n            AQI = self.inv(A + self.Q)\n            self.x = dot(FTI, dot(I_PF, AQI).dot(FTIX))\n\n            # save priors\n            self.x_prior = np.copy(self.x)\n            self.P_inv_prior = np.copy(AQI)\n\n    def batch_filter(self, zs, Rs=None, update_first=False, saver=None):\n        \"\"\"Batch processes a sequences of measurements.\n\n        Parameters\n        ----------\n\n        zs : list-like\n            list of measurements at each time step `self.dt` Missing\n            measurements must be represented by 'None'.\n\n        Rs : list-like, optional\n            optional list of values to use for the measurement error\n            covariance; a value of None in any position will cause the filter\n            to use `self.R` for that time step.\n\n        update_first : bool, optional,\n            controls whether the order of operations is update followed by\n            predict, or predict followed by update. Default is predict-&gt;update.\n\n        saver : filterpy.common.Saver, optional\n            filterpy.common.Saver object. If provided, saver.save() will be\n            called after every epoch\n\n        Returns\n        -------\n\n        means: np.array((n,dim_x,1))\n            array of the state for each time step. Each entry is an np.array.\n            In other words `means[k,:]` is the state at step `k`.\n\n        covariance: np.array((n,dim_x,dim_x))\n            array of the covariances for each time step. In other words\n            `covariance[k,:,:]` is the covariance at step `k`.\n        \"\"\"\n\n        raise NotImplementedError(\"this is not implemented yet\")\n\n        # pylint: disable=unreachable, no-member\n\n        # this is a copy of the code from kalman_filter, it has not been\n        # turned into the information filter yet. DO NOT USE.\n\n        n = np.size(zs, 0)\n        if Rs is None:\n            Rs = [None] * n\n\n        # mean estimates from Kalman Filter\n        means = zeros((n, self.dim_x, 1))\n\n        # state covariances from Kalman Filter\n        covariances = zeros((n, self.dim_x, self.dim_x))\n\n        if update_first:\n            for i, (z, r) in enumerate(zip(zs, Rs)):\n                self.update(z, r)\n                means[i, :] = self.x\n                covariances[i, :, :] = self._P\n                self.predict()\n\n                if saver is not None:\n                    saver.save()\n        else:\n            for i, (z, r) in enumerate(zip(zs, Rs)):\n                self.predict()\n                self.update(z, r)\n\n                means[i, :] = self.x\n                covariances[i, :, :] = self._P\n\n                if saver is not None:\n                    saver.save()\n\n        return (means, covariances)\n\n    @property\n    def F(self):\n        \"\"\"State Transition matrix\"\"\"\n        return self._F\n\n    @F.setter\n    def F(self, value):\n        \"\"\"State Transition matrix\"\"\"\n        self._F = value\n        self._F_inv = self.inv(self._F)\n\n    @property\n    def P(self):\n        \"\"\"State covariance matrix\"\"\"\n        return self.inv(self.P_inv)\n\n    def __repr__(self):\n        return \"\\n\".join(\n            [\n                \"InformationFilter object\",\n                pretty_str(\"dim_x\", self.dim_x),\n                pretty_str(\"dim_z\", self.dim_z),\n                pretty_str(\"dim_u\", self.dim_u),\n                pretty_str(\"x\", self.x),\n                pretty_str(\"P_inv\", self.P_inv),\n                pretty_str(\"x_prior\", self.x_prior),\n                pretty_str(\"P_inv_prior\", self.P_inv_prior),\n                pretty_str(\"F\", self.F),\n                pretty_str(\"_F_inv\", self._F_inv),\n                pretty_str(\"Q\", self.Q),\n                pretty_str(\"R_inv\", self.R_inv),\n                pretty_str(\"H\", self.H),\n                pretty_str(\"K\", self.K),\n                pretty_str(\"y\", self.y),\n                pretty_str(\"z\", self.z),\n                pretty_str(\"S\", self.S),\n                pretty_str(\"B\", self.B),\n                pretty_str(\"log-likelihood\", self.log_likelihood),\n                pretty_str(\"likelihood\", self.likelihood),\n                pretty_str(\"inv\", self.inv),\n            ]\n        )\n</code></pre>"},{"location":"filters/information-filter/#bayesian_filters.kalman.information_filter.InformationFilter.F","title":"<code>F</code>  <code>property</code> <code>writable</code>","text":"<p>State Transition matrix</p>"},{"location":"filters/information-filter/#bayesian_filters.kalman.information_filter.InformationFilter.P","title":"<code>P</code>  <code>property</code>","text":"<p>State covariance matrix</p>"},{"location":"filters/information-filter/#bayesian_filters.kalman.information_filter.InformationFilter.batch_filter","title":"<code>batch_filter(zs, Rs=None, update_first=False, saver=None)</code>","text":"<p>Batch processes a sequences of measurements.</p> <p>Parameters:</p> Name Type Description Default <code>zs</code> <code>list - like</code> <p>list of measurements at each time step <code>self.dt</code> Missing measurements must be represented by 'None'.</p> required <code>Rs</code> <code>list - like</code> <p>optional list of values to use for the measurement error covariance; a value of None in any position will cause the filter to use <code>self.R</code> for that time step.</p> <code>None</code> <code>update_first</code> <code>(bool, optional)</code> <p>controls whether the order of operations is update followed by predict, or predict followed by update. Default is predict-&gt;update.</p> <code>False</code> <code>saver</code> <code>Saver</code> <p>filterpy.common.Saver object. If provided, saver.save() will be called after every epoch</p> <code>None</code> <p>Returns:</p> Name Type Description <code>means</code> <code>array((n, dim_x, 1))</code> <p>array of the state for each time step. Each entry is an np.array. In other words <code>means[k,:]</code> is the state at step <code>k</code>.</p> <code>covariance</code> <code>array((n, dim_x, dim_x))</code> <p>array of the covariances for each time step. In other words <code>covariance[k,:,:]</code> is the covariance at step <code>k</code>.</p> Source code in <code>bayesian_filters/kalman/information_filter.py</code> <pre><code>def batch_filter(self, zs, Rs=None, update_first=False, saver=None):\n    \"\"\"Batch processes a sequences of measurements.\n\n    Parameters\n    ----------\n\n    zs : list-like\n        list of measurements at each time step `self.dt` Missing\n        measurements must be represented by 'None'.\n\n    Rs : list-like, optional\n        optional list of values to use for the measurement error\n        covariance; a value of None in any position will cause the filter\n        to use `self.R` for that time step.\n\n    update_first : bool, optional,\n        controls whether the order of operations is update followed by\n        predict, or predict followed by update. Default is predict-&gt;update.\n\n    saver : filterpy.common.Saver, optional\n        filterpy.common.Saver object. If provided, saver.save() will be\n        called after every epoch\n\n    Returns\n    -------\n\n    means: np.array((n,dim_x,1))\n        array of the state for each time step. Each entry is an np.array.\n        In other words `means[k,:]` is the state at step `k`.\n\n    covariance: np.array((n,dim_x,dim_x))\n        array of the covariances for each time step. In other words\n        `covariance[k,:,:]` is the covariance at step `k`.\n    \"\"\"\n\n    raise NotImplementedError(\"this is not implemented yet\")\n\n    # pylint: disable=unreachable, no-member\n\n    # this is a copy of the code from kalman_filter, it has not been\n    # turned into the information filter yet. DO NOT USE.\n\n    n = np.size(zs, 0)\n    if Rs is None:\n        Rs = [None] * n\n\n    # mean estimates from Kalman Filter\n    means = zeros((n, self.dim_x, 1))\n\n    # state covariances from Kalman Filter\n    covariances = zeros((n, self.dim_x, self.dim_x))\n\n    if update_first:\n        for i, (z, r) in enumerate(zip(zs, Rs)):\n            self.update(z, r)\n            means[i, :] = self.x\n            covariances[i, :, :] = self._P\n            self.predict()\n\n            if saver is not None:\n                saver.save()\n    else:\n        for i, (z, r) in enumerate(zip(zs, Rs)):\n            self.predict()\n            self.update(z, r)\n\n            means[i, :] = self.x\n            covariances[i, :, :] = self._P\n\n            if saver is not None:\n                saver.save()\n\n    return (means, covariances)\n</code></pre>"},{"location":"filters/information-filter/#bayesian_filters.kalman.information_filter.InformationFilter.predict","title":"<code>predict(u=0)</code>","text":"<p>Predict next position.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>ndarray</code> <p>Optional control vector. If non-zero, it is multiplied by B to create the control input into the system.</p> <code>0</code> Source code in <code>bayesian_filters/kalman/information_filter.py</code> <pre><code>def predict(self, u=0):\n    \"\"\"Predict next position.\n\n    Parameters\n    ----------\n\n    u : ndarray\n        Optional control vector. If non-zero, it is multiplied by B\n        to create the control input into the system.\n    \"\"\"\n\n    # x = Fx + Bu\n\n    A = dot(self._F_inv.T, self.P_inv).dot(self._F_inv)\n    # pylint: disable=bare-except\n    try:\n        AI = self.inv(A)\n        invertable = True\n        if self._no_information:\n            try:\n                self.x = dot(self.inv(self.P_inv), self.x)\n            except:\n                self.x = dot(0, self.x)\n            self._no_information = False\n    except:\n        invertable = False\n        self._no_information = True\n\n    if invertable:\n        self.x = dot(self._F, self.x) + dot(self.B, u)\n        self.P_inv = self.inv(AI + self.Q)\n\n        # save priors\n        self.P_inv_prior = np.copy(self.P_inv)\n        self.x_prior = np.copy(self.x)\n    else:\n        I_PF = self._I - dot(self.P_inv, self._F_inv)\n        FTI = self.inv(self._F.T)\n        FTIX = dot(FTI, self.x)\n        AQI = self.inv(A + self.Q)\n        self.x = dot(FTI, dot(I_PF, AQI).dot(FTIX))\n\n        # save priors\n        self.x_prior = np.copy(self.x)\n        self.P_inv_prior = np.copy(AQI)\n</code></pre>"},{"location":"filters/information-filter/#bayesian_filters.kalman.information_filter.InformationFilter.update","title":"<code>update(z, R_inv=None)</code>","text":"<p>Add a new measurement (z) to the kalman filter. If z is None, nothing is changed.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>array</code> <p>measurement for this update.</p> required <code>R</code> <code>np.array, scalar, or None</code> <p>Optionally provide R to override the measurement noise for this one call, otherwise  self.R will be used.</p> required Source code in <code>bayesian_filters/kalman/information_filter.py</code> <pre><code>def update(self, z, R_inv=None):\n    \"\"\"\n    Add a new measurement (z) to the kalman filter. If z is None, nothing\n    is changed.\n\n    Parameters\n    ----------\n\n    z : np.array\n        measurement for this update.\n\n    R : np.array, scalar, or None\n        Optionally provide R to override the measurement noise for this\n        one call, otherwise  self.R will be used.\n    \"\"\"\n\n    if z is None:\n        self.z = None\n        self.x_post = self.x.copy()\n        self.P_inv_post = self.P_inv.copy()\n        return\n\n    if R_inv is None:\n        R_inv = self.R_inv\n    elif np.isscalar(R_inv):\n        R_inv = eye(self.dim_z) * R_inv\n\n    # rename for readability and a tiny extra bit of speed\n    H = self.H\n    H_T = H.T\n    P_inv = self.P_inv\n    x = self.x\n\n    if self._no_information:\n        self.x = dot(P_inv, x) + dot(H_T, R_inv).dot(z)\n        self.P_inv = P_inv + dot(H_T, R_inv).dot(H)\n        self.log_likelihood = math.log(sys.float_info.min)\n        self.likelihood = sys.float_info.min\n\n    else:\n        # y = z - Hx\n        # error (residual) between measurement and prediction\n        self.y = z - dot(H, x)\n\n        # S = HPH' + R\n        # project system uncertainty into measurement space\n        self.S = P_inv + dot(H_T, R_inv).dot(H)\n        self.K = dot(self.inv(self.S), H_T).dot(R_inv)\n\n        # x = x + Ky\n        # predict new x with residual scaled by the kalman gain\n        self.x = x + dot(self.K, self.y)\n        self.P_inv = P_inv + dot(H_T, R_inv).dot(H)\n\n        self.z = np.copy(reshape_z(z, self.dim_z, np.ndim(self.x)))\n\n        if self.compute_log_likelihood:\n            self.log_likelihood = logpdf(x=self.y, cov=self.S)\n            self.likelihood = math.exp(self.log_likelihood)\n            if self.likelihood == 0:\n                self.likelihood = sys.float_info.min\n\n    # save measurement and posterior state\n    self.z = deepcopy(z)\n    self.x_post = self.x.copy()\n    self.P_inv_post = self.P_inv.copy()\n</code></pre>"},{"location":"filters/kalman-filter/","title":"Kalman Filter","text":""},{"location":"filters/kalman-filter/#kalmanfilter","title":"KalmanFilter","text":"<p>Implements a linear Kalman filter. For now the best documentation is my free book Kalman and Bayesian Filters in Python [2]</p> <p>The test files in this directory also give you a basic idea of use, albeit without much description.</p> <p>In brief, you will first construct this object, specifying the size of the state vector with <code>dim_x</code> and the size of the measurement vector that you will be using with <code>dim_z</code>. These are mostly used to perform size checks when you assign values to the various matrices. For example, if you specified <code>dim_z=2</code> and then try to assign a 3x3 matrix to R (the measurement noise matrix you will get an assert exception because R should be 2x2. (If for whatever reason you need to alter the size of things midstream just use the underscore version of the matrices to assign directly: your_filter._R = a_3x3_matrix.)</p> <p>After construction the filter will have default matrices created for you, but you must specify the values for each. It's usually easiest to just overwrite them rather than assign to each element yourself. This will be clearer in the example below. All are of type numpy.array.</p> <p>These are the matrices (instance variables) which you must specify. All are of type numpy.array (do NOT use numpy.matrix) If dimensional analysis allows you to get away with a 1x1 matrix you may also use a scalar. All elements must have a type of float.</p> <p>Instance Variables</p> <p>You will have to assign reasonable values to all of these before running the filter. All must have dtype of float.</p> <p>x : ndarray (dim_x, 1), default = [0,0,0...0]     filter state estimate</p> <p>P : ndarray (dim_x, dim_x), default eye(dim_x)     covariance matrix</p> <p>Q : ndarray (dim_x, dim_x), default eye(dim_x)     Process uncertainty/noise</p> <p>R : ndarray (dim_z, dim_z), default eye(dim_z)     measurement uncertainty/noise</p> <p>H : ndarray (dim_z, dim_x)     measurement function</p> <p>F : ndarray (dim_x, dim_x)     state transition matrix</p> <p>B : ndarray (dim_x, dim_u), default 0     control transition matrix</p> <p>Optional Instance Variables</p> <p>alpha : float</p> <p>Assign a value &gt; 1.0 to turn this into a fading memory filter.</p> <p>Read-only Instance Variables</p> <p>K : ndarray     Kalman gain that was used in the most recent update() call.</p> <p>y : ndarray     Residual calculated in the most recent update() call. I.e., the     different between the measurement and the current estimated state     projected into measurement space (z - Hx)</p> <p>S : ndarray     System uncertainty projected into measurement space. I.e., HPH' + R.     Probably not very useful, but it is here if you want it.</p> <p>likelihood : float     Likelihood of last measurment update.</p> <p>log_likelihood : float     Log likelihood of last measurment update.</p> <p>Example</p> <p>Here is a filter that tracks position and velocity using a sensor that only reads position.</p> <p>First construct the object with the required dimensionality.</p> <p>.. code`` from filterpy.kalman import KalmanFilter f = KalmanFilter (dim_x=2, dim_z=1)</p> <p>``</p> <p>Assign the initial value for the state (position and velocity). You can do this with a two dimensional array like so:</p> <p>.. code`` f.x = np.array([[2.],# position [0.]])   # velocity</p> <p>``</p> <p>or just use a one dimensional array, which I prefer doing.</p> <p>.. code`` f.x = np.array([2., 0.])</p> <p>``</p> <p>Define the state transition matrix:</p> <p>.. code`` f.F = np.array([[1.,1.], [0.,1.]])</p> <p>``</p> <p>Define the measurement function:</p> <p>.. code`` f.H = np.array([[1.,0.]])</p> <p>``</p> <p>Define the covariance matrix. Here I take advantage of the fact that P already contains np.eye(dim_x), and just multiply by the uncertainty:</p> <p>.. code`` f.P *= 1000.</p> <p>``</p> <p>I could have written:</p> <p>.. code`` f.P = np.array([[1000.,0.], [   0., 1000.] ])</p> <p>``</p> <p>You decide which is more readable and understandable.</p> <p>Now assign the measurement noise. Here the dimension is 1x1, so I can use a scalar</p> <p>.. code`` f.R = 5</p> <p>``</p> <p>I could have done this instead:</p> <p>.. code`` f.R = np.array([[5.]])</p> <p>``</p> <p>Note that this must be a 2 dimensional array, as must all the matrices.</p> <p>Finally, I will assign the process noise. Here I will take advantage of another FilterPy library function:</p> <p>.. code`` from filterpy.common import Q_discrete_white_noise f.Q = Q_discrete_white_noise(dim=2, dt=0.1, var=0.13)</p> <p>``</p> <p>Now just perform the standard predict/update loop:</p> <p>while some_condition_is_true:</p> <p>.. code`` z = get_sensor_reading() f.predict() f.update(z)</p> <p>``</p> <pre><code>do_something_with_estimate (f.x)\n</code></pre> <p>Procedural Form</p> <p>This module also contains stand alone functions to perform Kalman filtering. Use these if you are not a fan of objects.</p> <p>Example</p> <p>.. code`` while True: z, R = read_sensor() x, P = predict(x, P, F, Q) x, P = update(x, P, z, R, H)</p> <p>``</p> <p>References</p> <p>.. [2] Labbe, Roger. \"Kalman and Bayesian Filters in Python\".</p> <p>github repo:     https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> <p>read online:     http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb</p>"},{"location":"filters/kalman-filter/#api-reference","title":"API Reference","text":""},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter","title":"<code>KalmanFilter</code>","text":"<p>               Bases: <code>object</code></p> <p>Implements a Kalman filter. You are responsible for setting the various state variables to reasonable values; the defaults  will not give you a functional filter.</p> <p>For now the best documentation is my free book Kalman and Bayesian Filters in Python [2]_. The test files in this directory also give you a basic idea of use, albeit without much description.</p> <p>In brief, you will first construct this object, specifying the size of the state vector with dim_x and the size of the measurement vector that you will be using with dim_z. These are mostly used to perform size checks when you assign values to the various matrices. For example, if you specified dim_z=2 and then try to assign a 3x3 matrix to R (the measurement noise matrix you will get an assert exception because R should be 2x2. (If for whatever reason you need to alter the size of things midstream just use the underscore version of the matrices to assign directly: your_filter._R = a_3x3_matrix.)</p> <p>After construction the filter will have default matrices created for you, but you must specify the values for each. It\u2019s usually easiest to just overwrite them rather than assign to each element yourself. This will be clearer in the example below. All are of type numpy.array.</p> <p>Examples:</p> <p>Here is a filter that tracks position and velocity using a sensor that only reads position.</p> <p>First construct the object with the required dimensionality. Here the state (<code>dim_x</code>) has 2 coefficients (position and velocity), and the measurement (<code>dim_z</code>) has one. In FilterPy <code>x</code> is the state, <code>z</code> is the measurement.</p> <p>.. code::</p> <pre><code>from bayesian_filters.kalman import KalmanFilter\nf = KalmanFilter (dim_x=2, dim_z=1)\n</code></pre> <p>Assign the initial value for the state (position and velocity). You can do this with a two dimensional array like so:</p> <pre><code>.. code::\n\n    f.x = np.array([[2.],    # position\n                    [0.]])   # velocity\n</code></pre> <p>or just use a one dimensional array, which I prefer doing.</p> <p>.. code::</p> <pre><code>f.x = np.array([2., 0.])\n</code></pre> <p>Define the state transition matrix:</p> <pre><code>.. code::\n\n    f.F = np.array([[1.,1.],\n                    [0.,1.]])\n</code></pre> <p>Define the measurement function. Here we need to convert a position-velocity vector into just a position vector, so we use:</p> <pre><code>.. code::\n\nf.H = np.array([[1., 0.]])\n</code></pre> <p>Define the state's covariance matrix P.</p> <p>.. code::</p> <pre><code>f.P = np.array([[1000.,    0.],\n                [   0., 1000.] ])\n</code></pre> <p>Now assign the measurement noise. Here the dimension is 1x1, so I can use a scalar</p> <p>.. code::</p> <pre><code>f.R = 5\n</code></pre> <p>I could have done this instead:</p> <p>.. code::</p> <pre><code>f.R = np.array([[5.]])\n</code></pre> <p>Note that this must be a 2 dimensional array.</p> <p>Finally, I will assign the process noise. Here I will take advantage of another FilterPy library function:</p> <p>.. code::</p> <pre><code>from bayesian_filters.common import Q_discrete_white_noise\nf.Q = Q_discrete_white_noise(dim=2, dt=0.1, var=0.13)\n</code></pre> <p>Now just perform the standard predict/update loop:</p> <p>.. code::</p> <pre><code>while some_condition_is_true:\n    z = get_sensor_reading()\n    f.predict()\n    f.update(z)\n\n    do_something_with_estimate (f.x)\n</code></pre> <p>Procedural Form</p> <p>This module also contains stand alone functions to perform Kalman filtering. Use these if you are not a fan of objects.</p> <p>Example</p> <p>.. code::</p> <pre><code>while True:\n    z, R = read_sensor()\n    x, P = predict(x, P, F, Q)\n    x, P = update(x, P, z, R, H)\n</code></pre> <p>See my book Kalman and Bayesian Filters in Python [2]_.</p> <p>You will have to set the following attributes after constructing this object for the filter to perform properly. Please note that there are various checks in place to ensure that you have made everything the 'correct' size. However, it is possible to provide incorrectly sized arrays such that the linear algebra can not perform an operation. It can also fail silently - you can end up with matrices of a size that allows the linear algebra to work, but are the wrong shape for the problem you are trying to solve.</p> <p>Parameters:</p> Name Type Description Default <code>dim_x</code> <code>int</code> <p>Number of state variables for the Kalman filter. For example, if you are tracking the position and velocity of an object in two dimensions, dim_x would be 4. This is used to set the default size of P, Q, and u</p> required <code>dim_z</code> <code>int</code> <p>Number of of measurement inputs. For example, if the sensor provides you with position in (x,y), dim_z would be 2.</p> required <code>dim_u</code> <code>int(optional)</code> <p>size of the control input, if it is being used. Default value of 0 indicates it is not used.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>x</code> <code>array(dim_x, 1)</code> <p>Current state estimate. Any call to update() or predict() updates this variable.</p> <code>P</code> <code>array(dim_x, dim_x)</code> <p>Current state covariance matrix. Any call to update() or predict() updates this variable.</p> <code>x_prior</code> <code>array(dim_x, 1)</code> <p>Prior (predicted) state estimate. The _prior and _post attributes are for convenience; they store the  prior and posterior of the current epoch. Read Only.</p> <code>P_prior</code> <code>array(dim_x, dim_x)</code> <p>Prior (predicted) state covariance matrix. Read Only.</p> <code>x_post</code> <code>array(dim_x, 1)</code> <p>Posterior (updated) state estimate. Read Only.</p> <code>P_post</code> <code>array(dim_x, dim_x)</code> <p>Posterior (updated) state covariance matrix. Read Only.</p> <code>z</code> <code>array</code> <p>Last measurement used in update(). Read only.</p> <code>R</code> <code>array(dim_z, dim_z)</code> <p>Measurement noise covariance matrix. Also known as the observation covariance.</p> <code>Q</code> <code>array(dim_x, dim_x)</code> <p>Process noise covariance matrix. Also known as the transition covariance.</p> <code>F</code> <code>array()</code> <p>State Transition matrix. Also known as <code>A</code> in some formulation.</p> <code>H</code> <code>array(dim_z, dim_x)</code> <p>Measurement function. Also known as the observation matrix, or as <code>C</code>.</p> <code>y</code> <code>array</code> <p>Residual of the update step. Read only.</p> <code>K</code> <code>array(dim_x, dim_z)</code> <p>Kalman gain of the update step. Read only.</p> <code>S</code> <code>array</code> <p>System uncertainty (P projected to measurement space). Read only.</p> <code>SI</code> <code>array</code> <p>Inverse system uncertainty. Read only.</p> <code>log_likelihood</code> <code>float</code> <p>log-likelihood of the last measurement. Read only.</p> <code>likelihood</code> <code>float</code> <p>likelihood of last measurement. Read only.</p> <p>Computed from the log-likelihood. The log-likelihood can be very small,  meaning a large negative value such as -28000. Taking the exp() of that results in 0.0, which can break typical algorithms which multiply by this value, so by default we always return a number &gt;= sys.float_info.min.</p> <code>mahalanobis</code> <code>float</code> <p>mahalanobis distance of the innovation. Read only.</p> <code>inv</code> <code>function, default numpy.linalg.inv</code> <p>If you prefer another inverse function, such as the Moore-Penrose pseudo inverse, set it to that instead: kf.inv = np.linalg.pinv</p> <p>This is only used to invert self.S. If you know it is diagonal, you might choose to set it to filterpy.common.inv_diagonal, which is several times faster than numpy.linalg.inv for diagonal matrices.</p> <code>alpha</code> <code>float</code> <p>Fading memory setting. 1.0 gives the normal Kalman filter, and values slightly larger than 1.0 (such as 1.02) give a fading memory effect - previous measurements have less influence on the filter's estimates. This formulation of the Fading memory filter (there are many) is due to Dan Simon [1]_.</p> References <p>.. [1] Dan Simon. \"Optimal State Estimation.\" John Wiley &amp; Sons.    p. 208-212. (2006)</p> <p>.. [2] Roger Labbe. \"Kalman and Bayesian Filters in Python\"    https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> Source code in <code>bayesian_filters/kalman/kalman_filter.py</code> <pre><code>class KalmanFilter(object):\n    r\"\"\"Implements a Kalman filter. You are responsible for setting the\n    various state variables to reasonable values; the defaults  will\n    not give you a functional filter.\n\n    For now the best documentation is my free book Kalman and Bayesian\n    Filters in Python [2]_. The test files in this directory also give you a\n    basic idea of use, albeit without much description.\n\n    In brief, you will first construct this object, specifying the size of\n    the state vector with dim_x and the size of the measurement vector that\n    you will be using with dim_z. These are mostly used to perform size checks\n    when you assign values to the various matrices. For example, if you\n    specified dim_z=2 and then try to assign a 3x3 matrix to R (the\n    measurement noise matrix you will get an assert exception because R\n    should be 2x2. (If for whatever reason you need to alter the size of\n    things midstream just use the underscore version of the matrices to\n    assign directly: your_filter._R = a_3x3_matrix.)\n\n    After construction the filter will have default matrices created for you,\n    but you must specify the values for each. It\u2019s usually easiest to just\n    overwrite them rather than assign to each element yourself. This will be\n    clearer in the example below. All are of type numpy.array.\n\n\n    Examples\n    --------\n\n    Here is a filter that tracks position and velocity using a sensor that only\n    reads position.\n\n    First construct the object with the required dimensionality. Here the state\n    (`dim_x`) has 2 coefficients (position and velocity), and the measurement\n    (`dim_z`) has one. In FilterPy `x` is the state, `z` is the measurement.\n\n    .. code::\n\n        from bayesian_filters.kalman import KalmanFilter\n        f = KalmanFilter (dim_x=2, dim_z=1)\n\n\n    Assign the initial value for the state (position and velocity). You can do this\n    with a two dimensional array like so:\n\n        .. code::\n\n            f.x = np.array([[2.],    # position\n                            [0.]])   # velocity\n\n    or just use a one dimensional array, which I prefer doing.\n\n    .. code::\n\n        f.x = np.array([2., 0.])\n\n\n    Define the state transition matrix:\n\n        .. code::\n\n            f.F = np.array([[1.,1.],\n                            [0.,1.]])\n\n    Define the measurement function. Here we need to convert a position-velocity\n    vector into just a position vector, so we use:\n\n        .. code::\n\n        f.H = np.array([[1., 0.]])\n\n    Define the state's covariance matrix P.\n\n    .. code::\n\n        f.P = np.array([[1000.,    0.],\n                        [   0., 1000.] ])\n\n    Now assign the measurement noise. Here the dimension is 1x1, so I can\n    use a scalar\n\n    .. code::\n\n        f.R = 5\n\n    I could have done this instead:\n\n    .. code::\n\n        f.R = np.array([[5.]])\n\n    Note that this must be a 2 dimensional array.\n\n    Finally, I will assign the process noise. Here I will take advantage of\n    another FilterPy library function:\n\n    .. code::\n\n        from bayesian_filters.common import Q_discrete_white_noise\n        f.Q = Q_discrete_white_noise(dim=2, dt=0.1, var=0.13)\n\n\n    Now just perform the standard predict/update loop:\n\n    .. code::\n\n        while some_condition_is_true:\n            z = get_sensor_reading()\n            f.predict()\n            f.update(z)\n\n            do_something_with_estimate (f.x)\n\n\n    **Procedural Form**\n\n    This module also contains stand alone functions to perform Kalman filtering.\n    Use these if you are not a fan of objects.\n\n    **Example**\n\n    .. code::\n\n        while True:\n            z, R = read_sensor()\n            x, P = predict(x, P, F, Q)\n            x, P = update(x, P, z, R, H)\n\n    See my book Kalman and Bayesian Filters in Python [2]_.\n\n\n    You will have to set the following attributes after constructing this\n    object for the filter to perform properly. Please note that there are\n    various checks in place to ensure that you have made everything the\n    'correct' size. However, it is possible to provide incorrectly sized\n    arrays such that the linear algebra can not perform an operation.\n    It can also fail silently - you can end up with matrices of a size that\n    allows the linear algebra to work, but are the wrong shape for the problem\n    you are trying to solve.\n\n    Parameters\n    ----------\n    dim_x : int\n        Number of state variables for the Kalman filter. For example, if\n        you are tracking the position and velocity of an object in two\n        dimensions, dim_x would be 4.\n        This is used to set the default size of P, Q, and u\n\n    dim_z : int\n        Number of of measurement inputs. For example, if the sensor\n        provides you with position in (x,y), dim_z would be 2.\n\n    dim_u : int (optional)\n        size of the control input, if it is being used.\n        Default value of 0 indicates it is not used.\n\n\n    Attributes\n    ----------\n    x : numpy.array(dim_x, 1)\n        Current state estimate. Any call to update() or predict() updates\n        this variable.\n\n    P : numpy.array(dim_x, dim_x)\n        Current state covariance matrix. Any call to update() or predict()\n        updates this variable.\n\n    x_prior : numpy.array(dim_x, 1)\n        Prior (predicted) state estimate. The *_prior and *_post attributes\n        are for convenience; they store the  prior and posterior of the\n        current epoch. Read Only.\n\n    P_prior : numpy.array(dim_x, dim_x)\n        Prior (predicted) state covariance matrix. Read Only.\n\n    x_post : numpy.array(dim_x, 1)\n        Posterior (updated) state estimate. Read Only.\n\n    P_post : numpy.array(dim_x, dim_x)\n        Posterior (updated) state covariance matrix. Read Only.\n\n    z : numpy.array\n        Last measurement used in update(). Read only.\n\n    R : numpy.array(dim_z, dim_z)\n        Measurement noise covariance matrix. Also known as the\n        observation covariance.\n\n    Q : numpy.array(dim_x, dim_x)\n        Process noise covariance matrix. Also known as the transition\n        covariance.\n\n    F : numpy.array()\n        State Transition matrix. Also known as `A` in some formulation.\n\n    H : numpy.array(dim_z, dim_x)\n        Measurement function. Also known as the observation matrix, or as `C`.\n\n    y : numpy.array\n        Residual of the update step. Read only.\n\n    K : numpy.array(dim_x, dim_z)\n        Kalman gain of the update step. Read only.\n\n    S :  numpy.array\n        System uncertainty (P projected to measurement space). Read only.\n\n    SI :  numpy.array\n        Inverse system uncertainty. Read only.\n\n    log_likelihood : float\n        log-likelihood of the last measurement. Read only.\n\n    likelihood : float\n        likelihood of last measurement. Read only.\n\n        Computed from the log-likelihood. The log-likelihood can be very\n        small,  meaning a large negative value such as -28000. Taking the\n        exp() of that results in 0.0, which can break typical algorithms\n        which multiply by this value, so by default we always return a\n        number &gt;= sys.float_info.min.\n\n    mahalanobis : float\n        mahalanobis distance of the innovation. Read only.\n\n    inv : function, default numpy.linalg.inv\n        If you prefer another inverse function, such as the Moore-Penrose\n        pseudo inverse, set it to that instead: kf.inv = np.linalg.pinv\n\n        This is only used to invert self.S. If you know it is diagonal, you\n        might choose to set it to filterpy.common.inv_diagonal, which is\n        several times faster than numpy.linalg.inv for diagonal matrices.\n\n    alpha : float\n        Fading memory setting. 1.0 gives the normal Kalman filter, and\n        values slightly larger than 1.0 (such as 1.02) give a fading\n        memory effect - previous measurements have less influence on the\n        filter's estimates. This formulation of the Fading memory filter\n        (there are many) is due to Dan Simon [1]_.\n\n    References\n    ----------\n\n    .. [1] Dan Simon. \"Optimal State Estimation.\" John Wiley &amp; Sons.\n       p. 208-212. (2006)\n\n    .. [2] Roger Labbe. \"Kalman and Bayesian Filters in Python\"\n       https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n\n    \"\"\"\n\n    def __init__(self, dim_x, dim_z, dim_u=0):\n        if dim_x &lt; 1:\n            raise ValueError(\"dim_x must be 1 or greater\")\n        if dim_z &lt; 1:\n            raise ValueError(\"dim_z must be 1 or greater\")\n        if dim_u &lt; 0:\n            raise ValueError(\"dim_u must be 0 or greater\")\n\n        self.dim_x = dim_x\n        self.dim_z = dim_z\n        self.dim_u = dim_u\n\n        self.x = zeros((dim_x, 1))  # state\n        self.P = eye(dim_x)  # uncertainty covariance\n        self.Q = eye(dim_x)  # process uncertainty\n        self.B = None  # control transition matrix\n        self.F = eye(dim_x)  # state transition matrix\n        self.H = zeros((dim_z, dim_x))  # measurement function\n        self.R = eye(dim_z)  # measurement uncertainty\n        self._alpha_sq = 1.0  # fading memory control\n        self.M = np.zeros((dim_x, dim_z))  # process-measurement cross correlation\n        self.z = np.array([[None] * self.dim_z]).T\n\n        # gain and residual are computed during the innovation step. We\n        # save them so that in case you want to inspect them for various\n        # purposes\n        self.K = np.zeros((dim_x, dim_z))  # kalman gain\n        self.y = zeros((dim_z, 1))\n        self.S = np.zeros((dim_z, dim_z))  # system uncertainty\n        self.SI = np.zeros((dim_z, dim_z))  # inverse system uncertainty\n\n        # identity matrix. Do not alter this.\n        self._I = np.eye(dim_x)\n\n        # these will always be a copy of x,P after predict() is called\n        self.x_prior = self.x.copy()\n        self.P_prior = self.P.copy()\n\n        # these will always be a copy of x,P after update() is called\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n\n        # Only computed only if requested via property\n        self._log_likelihood = log(sys.float_info.min)\n        self._likelihood = sys.float_info.min\n        self._mahalanobis = None\n\n        self.inv = np.linalg.inv\n\n    def predict(self, u=None, B=None, F=None, Q=None):\n        \"\"\"\n        Predict next state (prior) using the Kalman filter state propagation\n        equations.\n\n        Parameters\n        ----------\n\n        u : np.array, default 0\n            Optional control vector.\n\n        B : np.array(dim_x, dim_u), or None\n            Optional control transition matrix; a value of None\n            will cause the filter to use `self.B`.\n\n        F : np.array(dim_x, dim_x), or None\n            Optional state transition matrix; a value of None\n            will cause the filter to use `self.F`.\n\n        Q : np.array(dim_x, dim_x), scalar, or None\n            Optional process noise matrix; a value of None will cause the\n            filter to use `self.Q`.\n        \"\"\"\n\n        if B is None:\n            B = self.B\n        if F is None:\n            F = self.F\n        if Q is None:\n            Q = self.Q\n        elif isscalar(Q):\n            Q = eye(self.dim_x) * Q\n\n        # x = Fx + Bu\n        if B is not None and u is not None:\n            self.x = dot(F, self.x) + dot(B, u)\n        else:\n            self.x = dot(F, self.x)\n\n        # P = FPF' + Q\n        self.P = self._alpha_sq * dot(dot(F, self.P), F.T) + Q\n\n        # save prior\n        self.x_prior = self.x.copy()\n        self.P_prior = self.P.copy()\n\n    def update(self, z, R=None, H=None):\n        \"\"\"\n        Add a new measurement (z) to the Kalman filter.\n\n        If z is None, nothing is computed. However, x_post and P_post are\n        updated with the prior (x_prior, P_prior), and self.z is set to None.\n\n        Parameters\n        ----------\n        z : (dim_z, 1): array_like\n            measurement for this update. z can be a scalar if dim_z is 1,\n            otherwise it must be convertible to a column vector.\n\n            If you pass in a value of H, z must be a column vector the\n            of the correct size.\n\n        R : np.array, scalar, or None\n            Optionally provide R to override the measurement noise for this\n            one call, otherwise  self.R will be used.\n\n        H : np.array, or None\n            Optionally provide H to override the measurement function for this\n            one call, otherwise self.H will be used.\n        \"\"\"\n\n        # set to None to force recompute\n        self._log_likelihood = None\n        self._likelihood = None\n        self._mahalanobis = None\n\n        if z is None:\n            self.z = np.array([[None] * self.dim_z]).T\n            self.x_post = self.x.copy()\n            self.P_post = self.P.copy()\n            self.y = zeros((self.dim_z, 1))\n            return\n\n        if R is None:\n            R = self.R\n        elif isscalar(R):\n            R = eye(self.dim_z) * R\n\n        if H is None:\n            z = reshape_z(z, self.dim_z, self.x.ndim)\n            H = self.H\n\n        # y = z - Hx\n        # error (residual) between measurement and prediction\n        self.y = z - dot(H, self.x)\n\n        # common subexpression for speed\n        PHT = dot(self.P, H.T)\n\n        # S = HPH' + R\n        # project system uncertainty into measurement space\n        self.S = dot(H, PHT) + R\n        self.SI = self.inv(self.S)\n        # K = PH'inv(S)\n        # map system uncertainty into kalman gain\n        self.K = dot(PHT, self.SI)\n\n        # x = x + Ky\n        # predict new x with residual scaled by the kalman gain\n        self.x = self.x + dot(self.K, self.y)\n\n        # P = (I-KH)P(I-KH)' + KRK'\n        # This is more numerically stable\n        # and works for non-optimal K vs the equation\n        # P = (I-KH)P usually seen in the literature.\n\n        I_KH = self._I - dot(self.K, H)\n        self.P = dot(dot(I_KH, self.P), I_KH.T) + dot(dot(self.K, R), self.K.T)\n\n        # save measurement and posterior state\n        self.z = deepcopy(z)\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n\n    def predict_steadystate(self, u=0, B=None):\n        \"\"\"\n        Predict state (prior) using the Kalman filter state propagation\n        equations. Only x is updated, P is left unchanged. See\n        update_steadstate() for a longer explanation of when to use this\n        method.\n\n        Parameters\n        ----------\n\n        u : np.array\n            Optional control vector. If non-zero, it is multiplied by B\n            to create the control input into the system.\n\n        B : np.array(dim_x, dim_u), or None\n            Optional control transition matrix; a value of None\n            will cause the filter to use `self.B`.\n        \"\"\"\n\n        if B is None:\n            B = self.B\n\n        # x = Fx + Bu\n        if B is not None:\n            self.x = dot(self.F, self.x) + dot(B, u)\n        else:\n            self.x = dot(self.F, self.x)\n\n        # save prior\n        self.x_prior = self.x.copy()\n        self.P_prior = self.P.copy()\n\n    def update_steadystate(self, z):\n        \"\"\"\n        Add a new measurement (z) to the Kalman filter without recomputing\n        the Kalman gain K, the state covariance P, or the system\n        uncertainty S.\n\n        You can use this for LTI systems since the Kalman gain and covariance\n        converge to a fixed value. Precompute these and assign them explicitly,\n        or run the Kalman filter using the normal predict()/update(0 cycle\n        until they converge.\n\n        The main advantage of this call is speed. We do significantly less\n        computation, notably avoiding a costly matrix inversion.\n\n        Use in conjunction with predict_steadystate(), otherwise P will grow\n        without bound.\n\n        Parameters\n        ----------\n        z : (dim_z, 1): array_like\n            measurement for this update. z can be a scalar if dim_z is 1,\n            otherwise it must be convertible to a column vector.\n\n\n        Examples\n        --------\n        &gt;&gt;&gt; cv = kinematic_kf(dim=3, order=2) # 3D const velocity filter\n        &gt;&gt;&gt; # let filter converge on representative data, then save k and P\n        &gt;&gt;&gt; for i in range(100):\n        &gt;&gt;&gt;     cv.predict()\n        &gt;&gt;&gt;     cv.update([i, i, i])\n        &gt;&gt;&gt; saved_k = np.copy(cv.K)\n        &gt;&gt;&gt; saved_P = np.copy(cv.P)\n\n        later on:\n\n        &gt;&gt;&gt; cv = kinematic_kf(dim=3, order=2) # 3D const velocity filter\n        &gt;&gt;&gt; cv.K = np.copy(saved_K)\n        &gt;&gt;&gt; cv.P = np.copy(saved_P)\n        &gt;&gt;&gt; for i in range(100):\n        &gt;&gt;&gt;     cv.predict_steadystate()\n        &gt;&gt;&gt;     cv.update_steadystate([i, i, i])\n        \"\"\"\n\n        # set to None to force recompute\n        self._log_likelihood = None\n        self._likelihood = None\n        self._mahalanobis = None\n\n        if z is None:\n            self.z = np.array([[None] * self.dim_z]).T\n            self.x_post = self.x.copy()\n            self.P_post = self.P.copy()\n            self.y = zeros((self.dim_z, 1))\n            return\n\n        z = reshape_z(z, self.dim_z, self.x.ndim)\n\n        # y = z - Hx\n        # error (residual) between measurement and prediction\n        self.y = z - dot(self.H, self.x)\n\n        # x = x + Ky\n        # predict new x with residual scaled by the kalman gain\n        self.x = self.x + dot(self.K, self.y)\n\n        self.z = deepcopy(z)\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n\n        # set to None to force recompute\n        self._log_likelihood = None\n        self._likelihood = None\n        self._mahalanobis = None\n\n    def update_correlated(self, z, R=None, H=None):\n        \"\"\"Add a new measurement (z) to the Kalman filter assuming that\n        process noise and measurement noise are correlated as defined in\n        the `self.M` matrix.\n\n        A partial derivation can be found in [1]\n\n        If z is None, nothing is changed.\n\n        Parameters\n        ----------\n        z : (dim_z, 1): array_like\n            measurement for this update. z can be a scalar if dim_z is 1,\n            otherwise it must be convertible to a column vector.\n\n        R : np.array, scalar, or None\n            Optionally provide R to override the measurement noise for this\n            one call, otherwise  self.R will be used.\n\n        H : np.array,  or None\n            Optionally provide H to override the measurement function for this\n            one call, otherwise  self.H will be used.\n\n        References\n        ----------\n\n        .. [1] Bulut, Y. (2011). Applied Kalman filter theory (Doctoral dissertation, Northeastern University).\n               http://people.duke.edu/~hpgavin/SystemID/References/Balut-KalmanFilter-PhD-NEU-2011.pdf\n        \"\"\"\n\n        # set to None to force recompute\n        self._log_likelihood = None\n        self._likelihood = None\n        self._mahalanobis = None\n\n        if z is None:\n            self.z = np.array([[None] * self.dim_z]).T\n            self.x_post = self.x.copy()\n            self.P_post = self.P.copy()\n            self.y = zeros((self.dim_z, 1))\n            return\n\n        if R is None:\n            R = self.R\n        elif isscalar(R):\n            R = eye(self.dim_z) * R\n\n        # rename for readability and a tiny extra bit of speed\n        if H is None:\n            z = reshape_z(z, self.dim_z, self.x.ndim)\n            H = self.H\n\n        # handle special case: if z is in form [[z]] but x is not a column\n        # vector dimensions will not match\n        if self.x.ndim == 1 and shape(z) == (1, 1):\n            z = z[0]\n\n        if shape(z) == ():  # is it scalar, e.g. z=3 or z=np.array(3)\n            z = np.asarray([z])\n\n        # y = z - Hx\n        # error (residual) between measurement and prediction\n        self.y = z - dot(H, self.x)\n\n        # common subexpression for speed\n        PHT = dot(self.P, H.T)\n\n        # project system uncertainty into measurement space\n        self.S = dot(H, PHT) + dot(H, self.M) + dot(self.M.T, H.T) + R\n        self.SI = self.inv(self.S)\n\n        # K = PH'inv(S)\n        # map system uncertainty into kalman gain\n        self.K = dot(PHT + self.M, self.SI)\n\n        # x = x + Ky\n        # predict new x with residual scaled by the kalman gain\n        self.x = self.x + dot(self.K, self.y)\n        self.P = self.P - dot(self.K, dot(H, self.P) + self.M.T)\n\n        self.z = deepcopy(z)\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n\n    def update_sequential(self, start, z_i, R_i=None, H_i=None):\n        \"\"\"\n        Add a single input measurement (z_i) to the Kalman filter.\n        In sequential processing, inputs are processed one at a time.\n\n        Parameters\n        ----------\n        start : integer\n            Index of the first measurement input updated by this call.\n\n        z_i : np.array or scalar\n            Measurement of inputs for this partial update.\n\n        R_i : np.array, scalar, or None\n            Optionally provide R_i to override the measurement noise of\n            inputs for this one call, otherwise a slice of self.R will\n            be used.\n\n        H_i : np.array, or None\n            Optionally provide H[i] to override the partial measurement\n            function for this one call, otherwise a slice of self.H will\n            be used.\n        \"\"\"\n\n        if isscalar(z_i):\n            length = 1\n        else:\n            length = len(z_i)\n        z_i = np.reshape(z_i, [length, 1])\n        stop = start + length\n\n        if R_i is None:\n            R_i = self.R[start:stop, start:stop]\n        elif isscalar(R_i):\n            R_i = eye(length) * R_i\n\n        if H_i is None:\n            H_i = self.H[start:stop]\n\n        H_i = np.reshape(H_i, [length, self.dim_x])\n\n        # y_i = z_i - H_i @ x\n        # error (residual) between measurement and prediction\n        y_i = z_i - dot(H_i, self.x)\n        self.y[start:stop] = y_i\n\n        # common subexpression for speed\n        PHT = dot(self.P, H_i.T)\n\n        # project system uncertainty into the measurement subspace\n        S_i = dot(H_i, PHT) + R_i\n\n        if length == 1:\n            K_i = PHT * (1.0 / S_i)\n        else:\n            K_i = dot(PHT, linalg.inv(S_i))\n\n        self.K[:, start:stop] = K_i\n        I_KH = self._I - np.dot(K_i, H_i)\n\n        # x = x + K_i @ y_i\n        # update state estimation with residual scaled by the kalman gain\n        self.x += dot(K_i, y_i)\n\n        # compute the posterior covariance\n        self.P = dot(dot(I_KH, self.P), I_KH.T) + dot(dot(K_i, R_i), K_i.T)\n\n        # save measurement component #i and the posterior state\n        self.z[start:stop] = z_i\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n\n    def batch_filter(\n        self,\n        zs,\n        Fs=None,\n        Qs=None,\n        Hs=None,\n        Rs=None,\n        Bs=None,\n        us=None,\n        update_first=False,\n        saver=None,\n    ):\n        \"\"\"Batch processes a sequences of measurements.\n\n         Parameters\n         ----------\n\n         zs : list-like\n             list of measurements at each time step `self.dt`. Missing\n             measurements must be represented by `None`.\n\n         Fs : None, list-like, default=None\n             optional value or list of values to use for the state transition\n             matrix F.\n\n             If Fs is None then self.F is used for all epochs.\n\n             Otherwise it must contain a list-like list of F's, one for\n             each epoch.  This allows you to have varying F per epoch.\n\n         Qs : None, np.array or list-like, default=None\n             optional value or list of values to use for the process error\n             covariance Q.\n\n             If Qs is None then self.Q is used for all epochs.\n\n             Otherwise it must contain a list-like list of Q's, one for\n             each epoch.  This allows you to have varying Q per epoch.\n\n         Hs : None, np.array or list-like, default=None\n             optional list of values to use for the measurement matrix H.\n\n             If Hs is None then self.H is used for all epochs.\n\n             If Hs contains a single matrix, then it is used as H for all\n             epochs.\n\n             Otherwise it must contain a list-like list of H's, one for\n             each epoch.  This allows you to have varying H per epoch.\n\n         Rs : None, np.array or list-like, default=None\n             optional list of values to use for the measurement error\n             covariance R.\n\n             If Rs is None then self.R is used for all epochs.\n\n             Otherwise it must contain a list-like list of R's, one for\n             each epoch.  This allows you to have varying R per epoch.\n\n         Bs : None, np.array or list-like, default=None\n             optional list of values to use for the control transition matrix B.\n\n             If Bs is None then self.B is used for all epochs.\n\n             Otherwise it must contain a list-like list of B's, one for\n             each epoch.  This allows you to have varying B per epoch.\n\n         us : None, np.array or list-like, default=None\n             optional list of values to use for the control input vector;\n\n             If us is None then None is used for all epochs (equivalent to 0,\n             or no control input).\n\n             Otherwise it must contain a list-like list of u's, one for\n             each epoch.\n\n        update_first : bool, optional, default=False\n             controls whether the order of operations is update followed by\n             predict, or predict followed by update. Default is predict-&gt;update.\n\n         saver : filterpy.common.Saver, optional\n             filterpy.common.Saver object. If provided, saver.save() will be\n             called after every epoch\n\n         Returns\n         -------\n\n         means : np.array((n,dim_x,1))\n             array of the state for each time step after the update. Each entry\n             is an np.array. In other words `means[k,:]` is the state at step\n             `k`.\n\n         covariance : np.array((n,dim_x,dim_x))\n             array of the covariances for each time step after the update.\n             In other words `covariance[k,:,:]` is the covariance at step `k`.\n\n         means_predictions : np.array((n,dim_x,1))\n             array of the state for each time step after the predictions. Each\n             entry is an np.array. In other words `means[k,:]` is the state at\n             step `k`.\n\n         covariance_predictions : np.array((n,dim_x,dim_x))\n             array of the covariances for each time step after the prediction.\n             In other words `covariance[k,:,:]` is the covariance at step `k`.\n\n         Examples\n         --------\n\n         .. code-block:: Python\n\n             # this example demonstrates tracking a measurement where the time\n             # between measurement varies, as stored in dts. This requires\n             # that F be recomputed for each epoch. The output is then smoothed\n             # with an RTS smoother.\n\n             zs = [t + random.randn()*4 for t in range (40)]\n             Fs = [np.array([[1., dt], [0, 1]] for dt in dts]\n\n             (mu, cov, _, _) = kf.batch_filter(zs, Fs=Fs)\n             (xs, Ps, Ks, Pps) = kf.rts_smoother(mu, cov, Fs=Fs)\n        \"\"\"\n\n        # pylint: disable=too-many-statements\n        n = np.size(zs, 0)\n        if Fs is None:\n            Fs = [self.F] * n\n        if Qs is None:\n            Qs = [self.Q] * n\n        if Hs is None:\n            Hs = [self.H] * n\n        if Rs is None:\n            Rs = [self.R] * n\n        if Bs is None:\n            Bs = [self.B] * n\n        if us is None:\n            us = [0] * n\n\n        # mean estimates from Kalman Filter\n        if self.x.ndim == 1:\n            means = zeros((n, self.dim_x))\n            means_p = zeros((n, self.dim_x))\n        else:\n            means = zeros((n, self.dim_x, 1))\n            means_p = zeros((n, self.dim_x, 1))\n\n        # state covariances from Kalman Filter\n        covariances = zeros((n, self.dim_x, self.dim_x))\n        covariances_p = zeros((n, self.dim_x, self.dim_x))\n\n        if update_first:\n            for i, (z, F, Q, H, R, B, u) in enumerate(zip(zs, Fs, Qs, Hs, Rs, Bs, us)):\n                self.update(z, R=R, H=H)\n                means[i, :] = self.x\n                covariances[i, :, :] = self.P\n\n                self.predict(u=u, B=B, F=F, Q=Q)\n                means_p[i, :] = self.x\n                covariances_p[i, :, :] = self.P\n\n                if saver is not None:\n                    saver.save()\n        else:\n            for i, (z, F, Q, H, R, B, u) in enumerate(zip(zs, Fs, Qs, Hs, Rs, Bs, us)):\n                self.predict(u=u, B=B, F=F, Q=Q)\n                means_p[i, :] = self.x\n                covariances_p[i, :, :] = self.P\n\n                self.update(z, R=R, H=H)\n                means[i, :] = self.x\n                covariances[i, :, :] = self.P\n\n                if saver is not None:\n                    saver.save()\n\n        return (means, covariances, means_p, covariances_p)\n\n    def rts_smoother(self, Xs, Ps, Fs=None, Qs=None, inv=np.linalg.inv):\n        \"\"\"\n        Runs the Rauch-Tung-Striebel Kalman smoother on a set of\n        means and covariances computed by a Kalman filter. The usual input\n        would come from the output of `KalmanFilter.batch_filter()`.\n\n        Parameters\n        ----------\n\n        Xs : numpy.array\n            array of the means (state variable x) of the output of a Kalman\n            filter.\n\n        Ps : numpy.array\n            array of the covariances of the output of a kalman filter.\n\n        Fs : list-like collection of numpy.array, optional\n            State transition matrix of the Kalman filter at each time step.\n            Optional, if not provided the filter's self.F will be used\n\n        Qs : list-like collection of numpy.array, optional\n            Process noise of the Kalman filter at each time step. Optional,\n            if not provided the filter's self.Q will be used\n\n        inv : function, default numpy.linalg.inv\n            If you prefer another inverse function, such as the Moore-Penrose\n            pseudo inverse, set it to that instead: kf.inv = np.linalg.pinv\n\n\n        Returns\n        -------\n\n        x : numpy.ndarray\n            smoothed means\n\n        P : numpy.ndarray\n            smoothed state covariances\n\n        K : numpy.ndarray\n            smoother gain at each step\n\n        Pp : numpy.ndarray\n            Predicted state covariances\n\n        Examples\n        --------\n\n        .. code-block:: Python\n\n            zs = [t + random.randn()*4 for t in range (40)]\n\n            (mu, cov, _, _) = kalman.batch_filter(zs)\n            (x, P, K, Pp) = rts_smoother(mu, cov, kf.F, kf.Q)\n\n        \"\"\"\n\n        if len(Xs) != len(Ps):\n            raise ValueError(\"length of Xs and Ps must be the same\")\n\n        n = Xs.shape[0]\n        dim_x = Xs.shape[1]\n\n        if Fs is None:\n            Fs = [self.F] * n\n        if Qs is None:\n            Qs = [self.Q] * n\n\n        # smoother gain\n        K = zeros((n, dim_x, dim_x))\n\n        x, P, Pp = Xs.copy(), Ps.copy(), Ps.copy()\n        for k in range(n - 2, -1, -1):\n            Pp[k] = dot(dot(Fs[k + 1], P[k]), Fs[k + 1].T) + Qs[k + 1]\n\n            # pylint: disable=bad-whitespace\n            K[k] = dot(dot(P[k], Fs[k + 1].T), inv(Pp[k]))\n            x[k] += dot(K[k], x[k + 1] - dot(Fs[k + 1], x[k]))\n            P[k] += dot(dot(K[k], P[k + 1] - Pp[k]), K[k].T)\n\n        return (x, P, K, Pp)\n\n    def get_prediction(self, u=None, B=None, F=None, Q=None):\n        \"\"\"\n        Predict next state (prior) using the Kalman filter state propagation\n        equations and returns it without modifying the object.\n\n        Parameters\n        ----------\n\n        u : np.array, default 0\n            Optional control vector.\n\n        B : np.array(dim_x, dim_u), or None\n            Optional control transition matrix; a value of None\n            will cause the filter to use `self.B`.\n\n        F : np.array(dim_x, dim_x), or None\n            Optional state transition matrix; a value of None\n            will cause the filter to use `self.F`.\n\n        Q : np.array(dim_x, dim_x), scalar, or None\n            Optional process noise matrix; a value of None will cause the\n            filter to use `self.Q`.\n\n        Returns\n        -------\n\n        (x, P) : tuple\n            State vector and covariance array of the prediction.\n        \"\"\"\n\n        if B is None:\n            B = self.B\n        if F is None:\n            F = self.F\n        if Q is None:\n            Q = self.Q\n        elif isscalar(Q):\n            Q = eye(self.dim_x) * Q\n\n        # x = Fx + Bu\n        if B is not None and u is not None:\n            x = dot(F, self.x) + dot(B, u)\n        else:\n            x = dot(F, self.x)\n\n        # P = FPF' + Q\n        P = self._alpha_sq * dot(dot(F, self.P), F.T) + Q\n\n        return x, P\n\n    def get_update(self, z=None):\n        \"\"\"\n        Computes the new estimate based on measurement `z` and returns it\n        without altering the state of the filter.\n\n        Parameters\n        ----------\n\n        z : (dim_z, 1): array_like\n            measurement for this update. z can be a scalar if dim_z is 1,\n            otherwise it must be convertible to a column vector.\n\n        Returns\n        -------\n\n        (x, P) : tuple\n            State vector and covariance array of the update.\n        \"\"\"\n\n        if z is None:\n            return self.x, self.P\n        z = reshape_z(z, self.dim_z, self.x.ndim)\n\n        R = self.R\n        H = self.H\n        P = self.P\n        x = self.x\n\n        # error (residual) between measurement and prediction\n        y = z - dot(H, x)\n\n        # common subexpression for speed\n        PHT = dot(P, H.T)\n\n        # project system uncertainty into measurement space\n        S = dot(H, PHT) + R\n\n        # map system uncertainty into kalman gain\n        K = dot(PHT, self.inv(S))\n\n        # predict new x with residual scaled by the kalman gain\n        x = x + dot(K, y)\n\n        # P = (I-KH)P(I-KH)' + KRK'\n        I_KH = self._I - dot(K, H)\n        P = dot(dot(I_KH, P), I_KH.T) + dot(dot(K, R), K.T)\n\n        return x, P\n\n    def residual_of(self, z):\n        \"\"\"\n        Returns the residual for the given measurement (z). Does not alter\n        the state of the filter.\n        \"\"\"\n        z = reshape_z(z, self.dim_z, self.x.ndim)\n        return z - dot(self.H, self.x_prior)\n\n    def measurement_of_state(self, x):\n        \"\"\"\n        Helper function that converts a state into a measurement.\n\n        Parameters\n        ----------\n\n        x : np.array\n            kalman state vector\n\n        Returns\n        -------\n\n        z : (dim_z, 1): array_like\n            measurement for this update. z can be a scalar if dim_z is 1,\n            otherwise it must be convertible to a column vector.\n        \"\"\"\n\n        return dot(self.H, x)\n\n    @property\n    def log_likelihood(self):\n        \"\"\"\n        log-likelihood of the last measurement.\n        \"\"\"\n        if self._log_likelihood is None:\n            self._log_likelihood = logpdf(x=self.y, cov=self.S)\n        return self._log_likelihood\n\n    @property\n    def likelihood(self):\n        \"\"\"\n        Computed from the log-likelihood. The log-likelihood can be very\n        small,  meaning a large negative value such as -28000. Taking the\n        exp() of that results in 0.0, which can break typical algorithms\n        which multiply by this value, so by default we always return a\n        number &gt;= sys.float_info.min.\n        \"\"\"\n        if self._likelihood is None:\n            self._likelihood = exp(self.log_likelihood)\n            if self._likelihood == 0:\n                self._likelihood = sys.float_info.min\n        return self._likelihood\n\n    @property\n    def mahalanobis(self):\n        \"\"\" \"\n        Mahalanobis distance of measurement. E.g. 3 means measurement\n        was 3 standard deviations away from the predicted value.\n\n        Returns\n        -------\n        mahalanobis : float\n        \"\"\"\n        if self._mahalanobis is None:\n            self._mahalanobis = sqrt(float(dot(dot(self.y.T, self.SI), self.y)))\n        return self._mahalanobis\n\n    @property\n    def alpha(self):\n        \"\"\"\n        Fading memory setting. 1.0 gives the normal Kalman filter, and\n        values slightly larger than 1.0 (such as 1.02) give a fading\n        memory effect - previous measurements have less influence on the\n        filter's estimates. This formulation of the Fading memory filter\n        (there are many) is due to Dan Simon [1]_.\n        \"\"\"\n        return self._alpha_sq**0.5\n\n    def log_likelihood_of(self, z):\n        \"\"\"\n        log likelihood of the measurement `z`. This should only be called\n        after a call to update(). Calling after predict() will yield an\n        incorrect result.\"\"\"\n\n        if z is None:\n            return log(sys.float_info.min)\n        return logpdf(z, dot(self.H, self.x), self.S)\n\n    @alpha.setter\n    def alpha(self, value):\n        if not np.isscalar(value) or value &lt; 1:\n            raise ValueError(\"alpha must be a float greater than 1\")\n\n        self._alpha_sq = value**2\n\n    def __repr__(self):\n        return \"\\n\".join(\n            [\n                \"KalmanFilter object\",\n                pretty_str(\"dim_x\", self.dim_x),\n                pretty_str(\"dim_z\", self.dim_z),\n                pretty_str(\"dim_u\", self.dim_u),\n                pretty_str(\"x\", self.x),\n                pretty_str(\"P\", self.P),\n                pretty_str(\"x_prior\", self.x_prior),\n                pretty_str(\"P_prior\", self.P_prior),\n                pretty_str(\"x_post\", self.x_post),\n                pretty_str(\"P_post\", self.P_post),\n                pretty_str(\"F\", self.F),\n                pretty_str(\"Q\", self.Q),\n                pretty_str(\"R\", self.R),\n                pretty_str(\"H\", self.H),\n                pretty_str(\"K\", self.K),\n                pretty_str(\"y\", self.y),\n                pretty_str(\"S\", self.S),\n                pretty_str(\"SI\", self.SI),\n                pretty_str(\"M\", self.M),\n                pretty_str(\"B\", self.B),\n                pretty_str(\"z\", self.z),\n                pretty_str(\"log-likelihood\", self.log_likelihood),\n                pretty_str(\"likelihood\", self.likelihood),\n                pretty_str(\"mahalanobis\", self.mahalanobis),\n                pretty_str(\"alpha\", self.alpha),\n                pretty_str(\"inv\", self.inv),\n            ]\n        )\n\n    def test_matrix_dimensions(self, z=None, H=None, R=None, F=None, Q=None):\n        \"\"\"\n        Performs a series of asserts to check that the size of everything\n        is what it should be. This can help you debug problems in your design.\n\n        If you pass in H, R, F, Q those will be used instead of this object's\n        value for those matrices.\n\n        Testing `z` (the measurement) is problamatic. x is a vector, and can be\n        implemented as either a 1D array or as a nx1 column vector. Thus Hx\n        can be of different shapes. Then, if Hx is a single value, it can\n        be either a 1D array or 2D vector. If either is true, z can reasonably\n        be a scalar (either '3' or np.array('3') are scalars under this\n        definition), a 1D, 1 element array, or a 2D, 1 element array. You are\n        allowed to pass in any combination that works.\n        \"\"\"\n\n        if H is None:\n            H = self.H\n        if R is None:\n            R = self.R\n        if F is None:\n            F = self.F\n        if Q is None:\n            Q = self.Q\n        x = self.x\n        P = self.P\n\n        assert x.ndim == 1 or x.ndim == 2, \"x must have one or two dimensions, but has {}\".format(x.ndim)\n\n        if x.ndim == 1:\n            assert x.shape[0] == self.dim_x, \"Shape of x must be ({},{}), but is {}\".format(self.dim_x, 1, x.shape)\n        else:\n            assert x.shape == (self.dim_x, 1), \"Shape of x must be ({},{}), but is {}\".format(self.dim_x, 1, x.shape)\n\n        assert P.shape == (self.dim_x, self.dim_x), \"Shape of P must be ({},{}), but is {}\".format(\n            self.dim_x, self.dim_x, P.shape\n        )\n\n        assert Q.shape == (self.dim_x, self.dim_x), \"Shape of Q must be ({},{}), but is {}\".format(\n            self.dim_x, self.dim_x, P.shape\n        )\n\n        assert F.shape == (self.dim_x, self.dim_x), \"Shape of F must be ({},{}), but is {}\".format(\n            self.dim_x, self.dim_x, F.shape\n        )\n\n        assert np.ndim(H) == 2, \"Shape of H must be (dim_z, {}), but is {}\".format(P.shape[0], shape(H))\n\n        assert H.shape[1] == P.shape[0], \"Shape of H must be (dim_z, {}), but is {}\".format(P.shape[0], H.shape)\n\n        # shape of R must be the same as HPH'\n        hph_shape = (H.shape[0], H.shape[0])\n        r_shape = shape(R)\n\n        if H.shape[0] == 1:\n            # r can be scalar, 1D, or 2D in this case\n            assert r_shape in [(), (1,), (1, 1)], \"R must be scalar or one element array, but is shaped {}\".format(\n                r_shape\n            )\n        else:\n            assert r_shape == hph_shape, \"shape of R should be {} but it is {}\".format(hph_shape, r_shape)\n\n        if z is not None:\n            z_shape = shape(z)\n        else:\n            z_shape = (self.dim_z, 1)\n\n        # H@x must have shape of z\n        Hx = dot(H, x)\n\n        if z_shape == ():  # scalar or np.array(scalar)\n            assert Hx.ndim == 1 or shape(Hx) == (1, 1), \"shape of z should be {}, not {} for the given H\".format(\n                shape(Hx), z_shape\n            )\n\n        elif shape(Hx) == (1,):\n            assert z_shape[0] == 1, \"Shape of z must be {} for the given H\".format(shape(Hx))\n\n        else:\n            assert z_shape == shape(Hx) or (len(z_shape) == 1 and shape(Hx) == (z_shape[0], 1)), (\n                \"shape of z should be {}, not {} for the given H\".format(shape(Hx), z_shape)\n            )\n\n        if np.ndim(Hx) &gt; 1 and shape(Hx) != (1, 1):\n            assert shape(Hx) == z_shape, \"shape of z should be {} for the given H, but it is {}\".format(\n                shape(Hx), z_shape\n            )\n</code></pre>"},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter.alpha","title":"<code>alpha</code>  <code>property</code> <code>writable</code>","text":"<p>Fading memory setting. 1.0 gives the normal Kalman filter, and values slightly larger than 1.0 (such as 1.02) give a fading memory effect - previous measurements have less influence on the filter's estimates. This formulation of the Fading memory filter (there are many) is due to Dan Simon [1]_.</p>"},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter.likelihood","title":"<code>likelihood</code>  <code>property</code>","text":"<p>Computed from the log-likelihood. The log-likelihood can be very small,  meaning a large negative value such as -28000. Taking the exp() of that results in 0.0, which can break typical algorithms which multiply by this value, so by default we always return a number &gt;= sys.float_info.min.</p>"},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter.log_likelihood","title":"<code>log_likelihood</code>  <code>property</code>","text":"<p>log-likelihood of the last measurement.</p>"},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter.mahalanobis","title":"<code>mahalanobis</code>  <code>property</code>","text":"<p>\" Mahalanobis distance of measurement. E.g. 3 means measurement was 3 standard deviations away from the predicted value.</p> <p>Returns:</p> Name Type Description <code>mahalanobis</code> <code>float</code>"},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter.batch_filter","title":"<code>batch_filter(zs, Fs=None, Qs=None, Hs=None, Rs=None, Bs=None, us=None, update_first=False, saver=None)</code>","text":"<p>Batch processes a sequences of measurements.</p> Parameters <p>zs : list-like      list of measurements at each time step <code>self.dt</code>. Missing      measurements must be represented by <code>None</code>.</p> <p>Fs : None, list-like, default=None      optional value or list of values to use for the state transition      matrix F.</p> <pre><code> If Fs is None then self.F is used for all epochs.\n\n Otherwise it must contain a list-like list of F's, one for\n each epoch.  This allows you to have varying F per epoch.\n</code></pre> <p>Qs : None, np.array or list-like, default=None      optional value or list of values to use for the process error      covariance Q.</p> <pre><code> If Qs is None then self.Q is used for all epochs.\n\n Otherwise it must contain a list-like list of Q's, one for\n each epoch.  This allows you to have varying Q per epoch.\n</code></pre> <p>Hs : None, np.array or list-like, default=None      optional list of values to use for the measurement matrix H.</p> <pre><code> If Hs is None then self.H is used for all epochs.\n\n If Hs contains a single matrix, then it is used as H for all\n epochs.\n\n Otherwise it must contain a list-like list of H's, one for\n each epoch.  This allows you to have varying H per epoch.\n</code></pre> <p>Rs : None, np.array or list-like, default=None      optional list of values to use for the measurement error      covariance R.</p> <pre><code> If Rs is None then self.R is used for all epochs.\n\n Otherwise it must contain a list-like list of R's, one for\n each epoch.  This allows you to have varying R per epoch.\n</code></pre> <p>Bs : None, np.array or list-like, default=None      optional list of values to use for the control transition matrix B.</p> <pre><code> If Bs is None then self.B is used for all epochs.\n\n Otherwise it must contain a list-like list of B's, one for\n each epoch.  This allows you to have varying B per epoch.\n</code></pre> <p>us : None, np.array or list-like, default=None      optional list of values to use for the control input vector;</p> <pre><code> If us is None then None is used for all epochs (equivalent to 0,\n or no control input).\n\n Otherwise it must contain a list-like list of u's, one for\n each epoch.\n</code></pre> <p>update_first : bool, optional, default=False      controls whether the order of operations is update followed by      predict, or predict followed by update. Default is predict-&gt;update.</p> <p>saver : filterpy.common.Saver, optional      filterpy.common.Saver object. If provided, saver.save() will be      called after every epoch</p> Returns <p>means : np.array((n,dim_x,1))      array of the state for each time step after the update. Each entry      is an np.array. In other words <code>means[k,:]</code> is the state at step      <code>k</code>.</p> <p>covariance : np.array((n,dim_x,dim_x))      array of the covariances for each time step after the update.      In other words <code>covariance[k,:,:]</code> is the covariance at step <code>k</code>.</p> <p>means_predictions : np.array((n,dim_x,1))      array of the state for each time step after the predictions. Each      entry is an np.array. In other words <code>means[k,:]</code> is the state at      step <code>k</code>.</p> <p>covariance_predictions : np.array((n,dim_x,dim_x))      array of the covariances for each time step after the prediction.      In other words <code>covariance[k,:,:]</code> is the covariance at step <code>k</code>.</p> Examples <p>.. code-block:: Python</p> <pre><code> # this example demonstrates tracking a measurement where the time\n # between measurement varies, as stored in dts. This requires\n # that F be recomputed for each epoch. The output is then smoothed\n # with an RTS smoother.\n\n zs = [t + random.randn()*4 for t in range (40)]\n Fs = [np.array([[1., dt], [0, 1]] for dt in dts]\n\n (mu, cov, _, _) = kf.batch_filter(zs, Fs=Fs)\n (xs, Ps, Ks, Pps) = kf.rts_smoother(mu, cov, Fs=Fs)\n</code></pre> Source code in <code>bayesian_filters/kalman/kalman_filter.py</code> <pre><code>def batch_filter(\n    self,\n    zs,\n    Fs=None,\n    Qs=None,\n    Hs=None,\n    Rs=None,\n    Bs=None,\n    us=None,\n    update_first=False,\n    saver=None,\n):\n    \"\"\"Batch processes a sequences of measurements.\n\n     Parameters\n     ----------\n\n     zs : list-like\n         list of measurements at each time step `self.dt`. Missing\n         measurements must be represented by `None`.\n\n     Fs : None, list-like, default=None\n         optional value or list of values to use for the state transition\n         matrix F.\n\n         If Fs is None then self.F is used for all epochs.\n\n         Otherwise it must contain a list-like list of F's, one for\n         each epoch.  This allows you to have varying F per epoch.\n\n     Qs : None, np.array or list-like, default=None\n         optional value or list of values to use for the process error\n         covariance Q.\n\n         If Qs is None then self.Q is used for all epochs.\n\n         Otherwise it must contain a list-like list of Q's, one for\n         each epoch.  This allows you to have varying Q per epoch.\n\n     Hs : None, np.array or list-like, default=None\n         optional list of values to use for the measurement matrix H.\n\n         If Hs is None then self.H is used for all epochs.\n\n         If Hs contains a single matrix, then it is used as H for all\n         epochs.\n\n         Otherwise it must contain a list-like list of H's, one for\n         each epoch.  This allows you to have varying H per epoch.\n\n     Rs : None, np.array or list-like, default=None\n         optional list of values to use for the measurement error\n         covariance R.\n\n         If Rs is None then self.R is used for all epochs.\n\n         Otherwise it must contain a list-like list of R's, one for\n         each epoch.  This allows you to have varying R per epoch.\n\n     Bs : None, np.array or list-like, default=None\n         optional list of values to use for the control transition matrix B.\n\n         If Bs is None then self.B is used for all epochs.\n\n         Otherwise it must contain a list-like list of B's, one for\n         each epoch.  This allows you to have varying B per epoch.\n\n     us : None, np.array or list-like, default=None\n         optional list of values to use for the control input vector;\n\n         If us is None then None is used for all epochs (equivalent to 0,\n         or no control input).\n\n         Otherwise it must contain a list-like list of u's, one for\n         each epoch.\n\n    update_first : bool, optional, default=False\n         controls whether the order of operations is update followed by\n         predict, or predict followed by update. Default is predict-&gt;update.\n\n     saver : filterpy.common.Saver, optional\n         filterpy.common.Saver object. If provided, saver.save() will be\n         called after every epoch\n\n     Returns\n     -------\n\n     means : np.array((n,dim_x,1))\n         array of the state for each time step after the update. Each entry\n         is an np.array. In other words `means[k,:]` is the state at step\n         `k`.\n\n     covariance : np.array((n,dim_x,dim_x))\n         array of the covariances for each time step after the update.\n         In other words `covariance[k,:,:]` is the covariance at step `k`.\n\n     means_predictions : np.array((n,dim_x,1))\n         array of the state for each time step after the predictions. Each\n         entry is an np.array. In other words `means[k,:]` is the state at\n         step `k`.\n\n     covariance_predictions : np.array((n,dim_x,dim_x))\n         array of the covariances for each time step after the prediction.\n         In other words `covariance[k,:,:]` is the covariance at step `k`.\n\n     Examples\n     --------\n\n     .. code-block:: Python\n\n         # this example demonstrates tracking a measurement where the time\n         # between measurement varies, as stored in dts. This requires\n         # that F be recomputed for each epoch. The output is then smoothed\n         # with an RTS smoother.\n\n         zs = [t + random.randn()*4 for t in range (40)]\n         Fs = [np.array([[1., dt], [0, 1]] for dt in dts]\n\n         (mu, cov, _, _) = kf.batch_filter(zs, Fs=Fs)\n         (xs, Ps, Ks, Pps) = kf.rts_smoother(mu, cov, Fs=Fs)\n    \"\"\"\n\n    # pylint: disable=too-many-statements\n    n = np.size(zs, 0)\n    if Fs is None:\n        Fs = [self.F] * n\n    if Qs is None:\n        Qs = [self.Q] * n\n    if Hs is None:\n        Hs = [self.H] * n\n    if Rs is None:\n        Rs = [self.R] * n\n    if Bs is None:\n        Bs = [self.B] * n\n    if us is None:\n        us = [0] * n\n\n    # mean estimates from Kalman Filter\n    if self.x.ndim == 1:\n        means = zeros((n, self.dim_x))\n        means_p = zeros((n, self.dim_x))\n    else:\n        means = zeros((n, self.dim_x, 1))\n        means_p = zeros((n, self.dim_x, 1))\n\n    # state covariances from Kalman Filter\n    covariances = zeros((n, self.dim_x, self.dim_x))\n    covariances_p = zeros((n, self.dim_x, self.dim_x))\n\n    if update_first:\n        for i, (z, F, Q, H, R, B, u) in enumerate(zip(zs, Fs, Qs, Hs, Rs, Bs, us)):\n            self.update(z, R=R, H=H)\n            means[i, :] = self.x\n            covariances[i, :, :] = self.P\n\n            self.predict(u=u, B=B, F=F, Q=Q)\n            means_p[i, :] = self.x\n            covariances_p[i, :, :] = self.P\n\n            if saver is not None:\n                saver.save()\n    else:\n        for i, (z, F, Q, H, R, B, u) in enumerate(zip(zs, Fs, Qs, Hs, Rs, Bs, us)):\n            self.predict(u=u, B=B, F=F, Q=Q)\n            means_p[i, :] = self.x\n            covariances_p[i, :, :] = self.P\n\n            self.update(z, R=R, H=H)\n            means[i, :] = self.x\n            covariances[i, :, :] = self.P\n\n            if saver is not None:\n                saver.save()\n\n    return (means, covariances, means_p, covariances_p)\n</code></pre>"},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter.get_prediction","title":"<code>get_prediction(u=None, B=None, F=None, Q=None)</code>","text":"<p>Predict next state (prior) using the Kalman filter state propagation equations and returns it without modifying the object.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>array</code> <p>Optional control vector.</p> <code>0</code> <code>B</code> <code>np.array(dim_x, dim_u), or None</code> <p>Optional control transition matrix; a value of None will cause the filter to use <code>self.B</code>.</p> <code>None</code> <code>F</code> <code>np.array(dim_x, dim_x), or None</code> <p>Optional state transition matrix; a value of None will cause the filter to use <code>self.F</code>.</p> <code>None</code> <code>Q</code> <code>np.array(dim_x, dim_x), scalar, or None</code> <p>Optional process noise matrix; a value of None will cause the filter to use <code>self.Q</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>(x, P) : tuple</code> <p>State vector and covariance array of the prediction.</p> Source code in <code>bayesian_filters/kalman/kalman_filter.py</code> <pre><code>def get_prediction(self, u=None, B=None, F=None, Q=None):\n    \"\"\"\n    Predict next state (prior) using the Kalman filter state propagation\n    equations and returns it without modifying the object.\n\n    Parameters\n    ----------\n\n    u : np.array, default 0\n        Optional control vector.\n\n    B : np.array(dim_x, dim_u), or None\n        Optional control transition matrix; a value of None\n        will cause the filter to use `self.B`.\n\n    F : np.array(dim_x, dim_x), or None\n        Optional state transition matrix; a value of None\n        will cause the filter to use `self.F`.\n\n    Q : np.array(dim_x, dim_x), scalar, or None\n        Optional process noise matrix; a value of None will cause the\n        filter to use `self.Q`.\n\n    Returns\n    -------\n\n    (x, P) : tuple\n        State vector and covariance array of the prediction.\n    \"\"\"\n\n    if B is None:\n        B = self.B\n    if F is None:\n        F = self.F\n    if Q is None:\n        Q = self.Q\n    elif isscalar(Q):\n        Q = eye(self.dim_x) * Q\n\n    # x = Fx + Bu\n    if B is not None and u is not None:\n        x = dot(F, self.x) + dot(B, u)\n    else:\n        x = dot(F, self.x)\n\n    # P = FPF' + Q\n    P = self._alpha_sq * dot(dot(F, self.P), F.T) + Q\n\n    return x, P\n</code></pre>"},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter.get_update","title":"<code>get_update(z=None)</code>","text":"<p>Computes the new estimate based on measurement <code>z</code> and returns it without altering the state of the filter.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>(dim_z, 1): array_like</code> <p>measurement for this update. z can be a scalar if dim_z is 1, otherwise it must be convertible to a column vector.</p> <code>None</code> <p>Returns:</p> Type Description <code>(x, P) : tuple</code> <p>State vector and covariance array of the update.</p> Source code in <code>bayesian_filters/kalman/kalman_filter.py</code> <pre><code>def get_update(self, z=None):\n    \"\"\"\n    Computes the new estimate based on measurement `z` and returns it\n    without altering the state of the filter.\n\n    Parameters\n    ----------\n\n    z : (dim_z, 1): array_like\n        measurement for this update. z can be a scalar if dim_z is 1,\n        otherwise it must be convertible to a column vector.\n\n    Returns\n    -------\n\n    (x, P) : tuple\n        State vector and covariance array of the update.\n    \"\"\"\n\n    if z is None:\n        return self.x, self.P\n    z = reshape_z(z, self.dim_z, self.x.ndim)\n\n    R = self.R\n    H = self.H\n    P = self.P\n    x = self.x\n\n    # error (residual) between measurement and prediction\n    y = z - dot(H, x)\n\n    # common subexpression for speed\n    PHT = dot(P, H.T)\n\n    # project system uncertainty into measurement space\n    S = dot(H, PHT) + R\n\n    # map system uncertainty into kalman gain\n    K = dot(PHT, self.inv(S))\n\n    # predict new x with residual scaled by the kalman gain\n    x = x + dot(K, y)\n\n    # P = (I-KH)P(I-KH)' + KRK'\n    I_KH = self._I - dot(K, H)\n    P = dot(dot(I_KH, P), I_KH.T) + dot(dot(K, R), K.T)\n\n    return x, P\n</code></pre>"},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter.log_likelihood_of","title":"<code>log_likelihood_of(z)</code>","text":"<p>log likelihood of the measurement <code>z</code>. This should only be called after a call to update(). Calling after predict() will yield an incorrect result.</p> Source code in <code>bayesian_filters/kalman/kalman_filter.py</code> <pre><code>def log_likelihood_of(self, z):\n    \"\"\"\n    log likelihood of the measurement `z`. This should only be called\n    after a call to update(). Calling after predict() will yield an\n    incorrect result.\"\"\"\n\n    if z is None:\n        return log(sys.float_info.min)\n    return logpdf(z, dot(self.H, self.x), self.S)\n</code></pre>"},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter.measurement_of_state","title":"<code>measurement_of_state(x)</code>","text":"<p>Helper function that converts a state into a measurement.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>kalman state vector</p> required <p>Returns:</p> Name Type Description <code>z</code> <code>(dim_z, 1): array_like</code> <p>measurement for this update. z can be a scalar if dim_z is 1, otherwise it must be convertible to a column vector.</p> Source code in <code>bayesian_filters/kalman/kalman_filter.py</code> <pre><code>def measurement_of_state(self, x):\n    \"\"\"\n    Helper function that converts a state into a measurement.\n\n    Parameters\n    ----------\n\n    x : np.array\n        kalman state vector\n\n    Returns\n    -------\n\n    z : (dim_z, 1): array_like\n        measurement for this update. z can be a scalar if dim_z is 1,\n        otherwise it must be convertible to a column vector.\n    \"\"\"\n\n    return dot(self.H, x)\n</code></pre>"},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter.predict","title":"<code>predict(u=None, B=None, F=None, Q=None)</code>","text":"<p>Predict next state (prior) using the Kalman filter state propagation equations.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>array</code> <p>Optional control vector.</p> <code>0</code> <code>B</code> <code>np.array(dim_x, dim_u), or None</code> <p>Optional control transition matrix; a value of None will cause the filter to use <code>self.B</code>.</p> <code>None</code> <code>F</code> <code>np.array(dim_x, dim_x), or None</code> <p>Optional state transition matrix; a value of None will cause the filter to use <code>self.F</code>.</p> <code>None</code> <code>Q</code> <code>np.array(dim_x, dim_x), scalar, or None</code> <p>Optional process noise matrix; a value of None will cause the filter to use <code>self.Q</code>.</p> <code>None</code> Source code in <code>bayesian_filters/kalman/kalman_filter.py</code> <pre><code>def predict(self, u=None, B=None, F=None, Q=None):\n    \"\"\"\n    Predict next state (prior) using the Kalman filter state propagation\n    equations.\n\n    Parameters\n    ----------\n\n    u : np.array, default 0\n        Optional control vector.\n\n    B : np.array(dim_x, dim_u), or None\n        Optional control transition matrix; a value of None\n        will cause the filter to use `self.B`.\n\n    F : np.array(dim_x, dim_x), or None\n        Optional state transition matrix; a value of None\n        will cause the filter to use `self.F`.\n\n    Q : np.array(dim_x, dim_x), scalar, or None\n        Optional process noise matrix; a value of None will cause the\n        filter to use `self.Q`.\n    \"\"\"\n\n    if B is None:\n        B = self.B\n    if F is None:\n        F = self.F\n    if Q is None:\n        Q = self.Q\n    elif isscalar(Q):\n        Q = eye(self.dim_x) * Q\n\n    # x = Fx + Bu\n    if B is not None and u is not None:\n        self.x = dot(F, self.x) + dot(B, u)\n    else:\n        self.x = dot(F, self.x)\n\n    # P = FPF' + Q\n    self.P = self._alpha_sq * dot(dot(F, self.P), F.T) + Q\n\n    # save prior\n    self.x_prior = self.x.copy()\n    self.P_prior = self.P.copy()\n</code></pre>"},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter.predict_steadystate","title":"<code>predict_steadystate(u=0, B=None)</code>","text":"<p>Predict state (prior) using the Kalman filter state propagation equations. Only x is updated, P is left unchanged. See update_steadstate() for a longer explanation of when to use this method.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>array</code> <p>Optional control vector. If non-zero, it is multiplied by B to create the control input into the system.</p> <code>0</code> <code>B</code> <code>np.array(dim_x, dim_u), or None</code> <p>Optional control transition matrix; a value of None will cause the filter to use <code>self.B</code>.</p> <code>None</code> Source code in <code>bayesian_filters/kalman/kalman_filter.py</code> <pre><code>def predict_steadystate(self, u=0, B=None):\n    \"\"\"\n    Predict state (prior) using the Kalman filter state propagation\n    equations. Only x is updated, P is left unchanged. See\n    update_steadstate() for a longer explanation of when to use this\n    method.\n\n    Parameters\n    ----------\n\n    u : np.array\n        Optional control vector. If non-zero, it is multiplied by B\n        to create the control input into the system.\n\n    B : np.array(dim_x, dim_u), or None\n        Optional control transition matrix; a value of None\n        will cause the filter to use `self.B`.\n    \"\"\"\n\n    if B is None:\n        B = self.B\n\n    # x = Fx + Bu\n    if B is not None:\n        self.x = dot(self.F, self.x) + dot(B, u)\n    else:\n        self.x = dot(self.F, self.x)\n\n    # save prior\n    self.x_prior = self.x.copy()\n    self.P_prior = self.P.copy()\n</code></pre>"},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter.residual_of","title":"<code>residual_of(z)</code>","text":"<p>Returns the residual for the given measurement (z). Does not alter the state of the filter.</p> Source code in <code>bayesian_filters/kalman/kalman_filter.py</code> <pre><code>def residual_of(self, z):\n    \"\"\"\n    Returns the residual for the given measurement (z). Does not alter\n    the state of the filter.\n    \"\"\"\n    z = reshape_z(z, self.dim_z, self.x.ndim)\n    return z - dot(self.H, self.x_prior)\n</code></pre>"},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter.rts_smoother","title":"<code>rts_smoother(Xs, Ps, Fs=None, Qs=None, inv=np.linalg.inv)</code>","text":"<p>Runs the Rauch-Tung-Striebel Kalman smoother on a set of means and covariances computed by a Kalman filter. The usual input would come from the output of <code>KalmanFilter.batch_filter()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>Xs</code> <code>array</code> <p>array of the means (state variable x) of the output of a Kalman filter.</p> required <code>Ps</code> <code>array</code> <p>array of the covariances of the output of a kalman filter.</p> required <code>Fs</code> <code>list-like collection of numpy.array</code> <p>State transition matrix of the Kalman filter at each time step. Optional, if not provided the filter's self.F will be used</p> <code>None</code> <code>Qs</code> <code>list-like collection of numpy.array</code> <p>Process noise of the Kalman filter at each time step. Optional, if not provided the filter's self.Q will be used</p> <code>None</code> <code>inv</code> <code>function</code> <p>If you prefer another inverse function, such as the Moore-Penrose pseudo inverse, set it to that instead: kf.inv = np.linalg.pinv</p> <code>numpy.linalg.inv</code> <p>Returns:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>smoothed means</p> <code>P</code> <code>ndarray</code> <p>smoothed state covariances</p> <code>K</code> <code>ndarray</code> <p>smoother gain at each step</p> <code>Pp</code> <code>ndarray</code> <p>Predicted state covariances</p> <p>Examples:</p> <p>.. code-block:: Python</p> <pre><code>zs = [t + random.randn()*4 for t in range (40)]\n\n(mu, cov, _, _) = kalman.batch_filter(zs)\n(x, P, K, Pp) = rts_smoother(mu, cov, kf.F, kf.Q)\n</code></pre> Source code in <code>bayesian_filters/kalman/kalman_filter.py</code> <pre><code>def rts_smoother(self, Xs, Ps, Fs=None, Qs=None, inv=np.linalg.inv):\n    \"\"\"\n    Runs the Rauch-Tung-Striebel Kalman smoother on a set of\n    means and covariances computed by a Kalman filter. The usual input\n    would come from the output of `KalmanFilter.batch_filter()`.\n\n    Parameters\n    ----------\n\n    Xs : numpy.array\n        array of the means (state variable x) of the output of a Kalman\n        filter.\n\n    Ps : numpy.array\n        array of the covariances of the output of a kalman filter.\n\n    Fs : list-like collection of numpy.array, optional\n        State transition matrix of the Kalman filter at each time step.\n        Optional, if not provided the filter's self.F will be used\n\n    Qs : list-like collection of numpy.array, optional\n        Process noise of the Kalman filter at each time step. Optional,\n        if not provided the filter's self.Q will be used\n\n    inv : function, default numpy.linalg.inv\n        If you prefer another inverse function, such as the Moore-Penrose\n        pseudo inverse, set it to that instead: kf.inv = np.linalg.pinv\n\n\n    Returns\n    -------\n\n    x : numpy.ndarray\n        smoothed means\n\n    P : numpy.ndarray\n        smoothed state covariances\n\n    K : numpy.ndarray\n        smoother gain at each step\n\n    Pp : numpy.ndarray\n        Predicted state covariances\n\n    Examples\n    --------\n\n    .. code-block:: Python\n\n        zs = [t + random.randn()*4 for t in range (40)]\n\n        (mu, cov, _, _) = kalman.batch_filter(zs)\n        (x, P, K, Pp) = rts_smoother(mu, cov, kf.F, kf.Q)\n\n    \"\"\"\n\n    if len(Xs) != len(Ps):\n        raise ValueError(\"length of Xs and Ps must be the same\")\n\n    n = Xs.shape[0]\n    dim_x = Xs.shape[1]\n\n    if Fs is None:\n        Fs = [self.F] * n\n    if Qs is None:\n        Qs = [self.Q] * n\n\n    # smoother gain\n    K = zeros((n, dim_x, dim_x))\n\n    x, P, Pp = Xs.copy(), Ps.copy(), Ps.copy()\n    for k in range(n - 2, -1, -1):\n        Pp[k] = dot(dot(Fs[k + 1], P[k]), Fs[k + 1].T) + Qs[k + 1]\n\n        # pylint: disable=bad-whitespace\n        K[k] = dot(dot(P[k], Fs[k + 1].T), inv(Pp[k]))\n        x[k] += dot(K[k], x[k + 1] - dot(Fs[k + 1], x[k]))\n        P[k] += dot(dot(K[k], P[k + 1] - Pp[k]), K[k].T)\n\n    return (x, P, K, Pp)\n</code></pre>"},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter.test_matrix_dimensions","title":"<code>test_matrix_dimensions(z=None, H=None, R=None, F=None, Q=None)</code>","text":"<p>Performs a series of asserts to check that the size of everything is what it should be. This can help you debug problems in your design.</p> <p>If you pass in H, R, F, Q those will be used instead of this object's value for those matrices.</p> <p>Testing <code>z</code> (the measurement) is problamatic. x is a vector, and can be implemented as either a 1D array or as a nx1 column vector. Thus Hx can be of different shapes. Then, if Hx is a single value, it can be either a 1D array or 2D vector. If either is true, z can reasonably be a scalar (either '3' or np.array('3') are scalars under this definition), a 1D, 1 element array, or a 2D, 1 element array. You are allowed to pass in any combination that works.</p> Source code in <code>bayesian_filters/kalman/kalman_filter.py</code> <pre><code>def test_matrix_dimensions(self, z=None, H=None, R=None, F=None, Q=None):\n    \"\"\"\n    Performs a series of asserts to check that the size of everything\n    is what it should be. This can help you debug problems in your design.\n\n    If you pass in H, R, F, Q those will be used instead of this object's\n    value for those matrices.\n\n    Testing `z` (the measurement) is problamatic. x is a vector, and can be\n    implemented as either a 1D array or as a nx1 column vector. Thus Hx\n    can be of different shapes. Then, if Hx is a single value, it can\n    be either a 1D array or 2D vector. If either is true, z can reasonably\n    be a scalar (either '3' or np.array('3') are scalars under this\n    definition), a 1D, 1 element array, or a 2D, 1 element array. You are\n    allowed to pass in any combination that works.\n    \"\"\"\n\n    if H is None:\n        H = self.H\n    if R is None:\n        R = self.R\n    if F is None:\n        F = self.F\n    if Q is None:\n        Q = self.Q\n    x = self.x\n    P = self.P\n\n    assert x.ndim == 1 or x.ndim == 2, \"x must have one or two dimensions, but has {}\".format(x.ndim)\n\n    if x.ndim == 1:\n        assert x.shape[0] == self.dim_x, \"Shape of x must be ({},{}), but is {}\".format(self.dim_x, 1, x.shape)\n    else:\n        assert x.shape == (self.dim_x, 1), \"Shape of x must be ({},{}), but is {}\".format(self.dim_x, 1, x.shape)\n\n    assert P.shape == (self.dim_x, self.dim_x), \"Shape of P must be ({},{}), but is {}\".format(\n        self.dim_x, self.dim_x, P.shape\n    )\n\n    assert Q.shape == (self.dim_x, self.dim_x), \"Shape of Q must be ({},{}), but is {}\".format(\n        self.dim_x, self.dim_x, P.shape\n    )\n\n    assert F.shape == (self.dim_x, self.dim_x), \"Shape of F must be ({},{}), but is {}\".format(\n        self.dim_x, self.dim_x, F.shape\n    )\n\n    assert np.ndim(H) == 2, \"Shape of H must be (dim_z, {}), but is {}\".format(P.shape[0], shape(H))\n\n    assert H.shape[1] == P.shape[0], \"Shape of H must be (dim_z, {}), but is {}\".format(P.shape[0], H.shape)\n\n    # shape of R must be the same as HPH'\n    hph_shape = (H.shape[0], H.shape[0])\n    r_shape = shape(R)\n\n    if H.shape[0] == 1:\n        # r can be scalar, 1D, or 2D in this case\n        assert r_shape in [(), (1,), (1, 1)], \"R must be scalar or one element array, but is shaped {}\".format(\n            r_shape\n        )\n    else:\n        assert r_shape == hph_shape, \"shape of R should be {} but it is {}\".format(hph_shape, r_shape)\n\n    if z is not None:\n        z_shape = shape(z)\n    else:\n        z_shape = (self.dim_z, 1)\n\n    # H@x must have shape of z\n    Hx = dot(H, x)\n\n    if z_shape == ():  # scalar or np.array(scalar)\n        assert Hx.ndim == 1 or shape(Hx) == (1, 1), \"shape of z should be {}, not {} for the given H\".format(\n            shape(Hx), z_shape\n        )\n\n    elif shape(Hx) == (1,):\n        assert z_shape[0] == 1, \"Shape of z must be {} for the given H\".format(shape(Hx))\n\n    else:\n        assert z_shape == shape(Hx) or (len(z_shape) == 1 and shape(Hx) == (z_shape[0], 1)), (\n            \"shape of z should be {}, not {} for the given H\".format(shape(Hx), z_shape)\n        )\n\n    if np.ndim(Hx) &gt; 1 and shape(Hx) != (1, 1):\n        assert shape(Hx) == z_shape, \"shape of z should be {} for the given H, but it is {}\".format(\n            shape(Hx), z_shape\n        )\n</code></pre>"},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter.update","title":"<code>update(z, R=None, H=None)</code>","text":"<p>Add a new measurement (z) to the Kalman filter.</p> <p>If z is None, nothing is computed. However, x_post and P_post are updated with the prior (x_prior, P_prior), and self.z is set to None.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>(dim_z, 1): array_like</code> <p>measurement for this update. z can be a scalar if dim_z is 1, otherwise it must be convertible to a column vector.</p> <p>If you pass in a value of H, z must be a column vector the of the correct size.</p> required <code>R</code> <code>np.array, scalar, or None</code> <p>Optionally provide R to override the measurement noise for this one call, otherwise  self.R will be used.</p> <code>None</code> <code>H</code> <code>np.array, or None</code> <p>Optionally provide H to override the measurement function for this one call, otherwise self.H will be used.</p> <code>None</code> Source code in <code>bayesian_filters/kalman/kalman_filter.py</code> <pre><code>def update(self, z, R=None, H=None):\n    \"\"\"\n    Add a new measurement (z) to the Kalman filter.\n\n    If z is None, nothing is computed. However, x_post and P_post are\n    updated with the prior (x_prior, P_prior), and self.z is set to None.\n\n    Parameters\n    ----------\n    z : (dim_z, 1): array_like\n        measurement for this update. z can be a scalar if dim_z is 1,\n        otherwise it must be convertible to a column vector.\n\n        If you pass in a value of H, z must be a column vector the\n        of the correct size.\n\n    R : np.array, scalar, or None\n        Optionally provide R to override the measurement noise for this\n        one call, otherwise  self.R will be used.\n\n    H : np.array, or None\n        Optionally provide H to override the measurement function for this\n        one call, otherwise self.H will be used.\n    \"\"\"\n\n    # set to None to force recompute\n    self._log_likelihood = None\n    self._likelihood = None\n    self._mahalanobis = None\n\n    if z is None:\n        self.z = np.array([[None] * self.dim_z]).T\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n        self.y = zeros((self.dim_z, 1))\n        return\n\n    if R is None:\n        R = self.R\n    elif isscalar(R):\n        R = eye(self.dim_z) * R\n\n    if H is None:\n        z = reshape_z(z, self.dim_z, self.x.ndim)\n        H = self.H\n\n    # y = z - Hx\n    # error (residual) between measurement and prediction\n    self.y = z - dot(H, self.x)\n\n    # common subexpression for speed\n    PHT = dot(self.P, H.T)\n\n    # S = HPH' + R\n    # project system uncertainty into measurement space\n    self.S = dot(H, PHT) + R\n    self.SI = self.inv(self.S)\n    # K = PH'inv(S)\n    # map system uncertainty into kalman gain\n    self.K = dot(PHT, self.SI)\n\n    # x = x + Ky\n    # predict new x with residual scaled by the kalman gain\n    self.x = self.x + dot(self.K, self.y)\n\n    # P = (I-KH)P(I-KH)' + KRK'\n    # This is more numerically stable\n    # and works for non-optimal K vs the equation\n    # P = (I-KH)P usually seen in the literature.\n\n    I_KH = self._I - dot(self.K, H)\n    self.P = dot(dot(I_KH, self.P), I_KH.T) + dot(dot(self.K, R), self.K.T)\n\n    # save measurement and posterior state\n    self.z = deepcopy(z)\n    self.x_post = self.x.copy()\n    self.P_post = self.P.copy()\n</code></pre>"},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter.update_correlated","title":"<code>update_correlated(z, R=None, H=None)</code>","text":"<p>Add a new measurement (z) to the Kalman filter assuming that process noise and measurement noise are correlated as defined in the <code>self.M</code> matrix.</p> <p>A partial derivation can be found in [1]</p> <p>If z is None, nothing is changed.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>(dim_z, 1): array_like</code> <p>measurement for this update. z can be a scalar if dim_z is 1, otherwise it must be convertible to a column vector.</p> required <code>R</code> <code>np.array, scalar, or None</code> <p>Optionally provide R to override the measurement noise for this one call, otherwise  self.R will be used.</p> <code>None</code> <code>H</code> <code>np.array,  or None</code> <p>Optionally provide H to override the measurement function for this one call, otherwise  self.H will be used.</p> <code>None</code> References <p>.. [1] Bulut, Y. (2011). Applied Kalman filter theory (Doctoral dissertation, Northeastern University).        http://people.duke.edu/~hpgavin/SystemID/References/Balut-KalmanFilter-PhD-NEU-2011.pdf</p> Source code in <code>bayesian_filters/kalman/kalman_filter.py</code> <pre><code>def update_correlated(self, z, R=None, H=None):\n    \"\"\"Add a new measurement (z) to the Kalman filter assuming that\n    process noise and measurement noise are correlated as defined in\n    the `self.M` matrix.\n\n    A partial derivation can be found in [1]\n\n    If z is None, nothing is changed.\n\n    Parameters\n    ----------\n    z : (dim_z, 1): array_like\n        measurement for this update. z can be a scalar if dim_z is 1,\n        otherwise it must be convertible to a column vector.\n\n    R : np.array, scalar, or None\n        Optionally provide R to override the measurement noise for this\n        one call, otherwise  self.R will be used.\n\n    H : np.array,  or None\n        Optionally provide H to override the measurement function for this\n        one call, otherwise  self.H will be used.\n\n    References\n    ----------\n\n    .. [1] Bulut, Y. (2011). Applied Kalman filter theory (Doctoral dissertation, Northeastern University).\n           http://people.duke.edu/~hpgavin/SystemID/References/Balut-KalmanFilter-PhD-NEU-2011.pdf\n    \"\"\"\n\n    # set to None to force recompute\n    self._log_likelihood = None\n    self._likelihood = None\n    self._mahalanobis = None\n\n    if z is None:\n        self.z = np.array([[None] * self.dim_z]).T\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n        self.y = zeros((self.dim_z, 1))\n        return\n\n    if R is None:\n        R = self.R\n    elif isscalar(R):\n        R = eye(self.dim_z) * R\n\n    # rename for readability and a tiny extra bit of speed\n    if H is None:\n        z = reshape_z(z, self.dim_z, self.x.ndim)\n        H = self.H\n\n    # handle special case: if z is in form [[z]] but x is not a column\n    # vector dimensions will not match\n    if self.x.ndim == 1 and shape(z) == (1, 1):\n        z = z[0]\n\n    if shape(z) == ():  # is it scalar, e.g. z=3 or z=np.array(3)\n        z = np.asarray([z])\n\n    # y = z - Hx\n    # error (residual) between measurement and prediction\n    self.y = z - dot(H, self.x)\n\n    # common subexpression for speed\n    PHT = dot(self.P, H.T)\n\n    # project system uncertainty into measurement space\n    self.S = dot(H, PHT) + dot(H, self.M) + dot(self.M.T, H.T) + R\n    self.SI = self.inv(self.S)\n\n    # K = PH'inv(S)\n    # map system uncertainty into kalman gain\n    self.K = dot(PHT + self.M, self.SI)\n\n    # x = x + Ky\n    # predict new x with residual scaled by the kalman gain\n    self.x = self.x + dot(self.K, self.y)\n    self.P = self.P - dot(self.K, dot(H, self.P) + self.M.T)\n\n    self.z = deepcopy(z)\n    self.x_post = self.x.copy()\n    self.P_post = self.P.copy()\n</code></pre>"},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter.update_sequential","title":"<code>update_sequential(start, z_i, R_i=None, H_i=None)</code>","text":"<p>Add a single input measurement (z_i) to the Kalman filter. In sequential processing, inputs are processed one at a time.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>integer</code> <p>Index of the first measurement input updated by this call.</p> required <code>z_i</code> <code>array or scalar</code> <p>Measurement of inputs for this partial update.</p> required <code>R_i</code> <code>np.array, scalar, or None</code> <p>Optionally provide R_i to override the measurement noise of inputs for this one call, otherwise a slice of self.R will be used.</p> <code>None</code> <code>H_i</code> <code>np.array, or None</code> <p>Optionally provide H[i] to override the partial measurement function for this one call, otherwise a slice of self.H will be used.</p> <code>None</code> Source code in <code>bayesian_filters/kalman/kalman_filter.py</code> <pre><code>def update_sequential(self, start, z_i, R_i=None, H_i=None):\n    \"\"\"\n    Add a single input measurement (z_i) to the Kalman filter.\n    In sequential processing, inputs are processed one at a time.\n\n    Parameters\n    ----------\n    start : integer\n        Index of the first measurement input updated by this call.\n\n    z_i : np.array or scalar\n        Measurement of inputs for this partial update.\n\n    R_i : np.array, scalar, or None\n        Optionally provide R_i to override the measurement noise of\n        inputs for this one call, otherwise a slice of self.R will\n        be used.\n\n    H_i : np.array, or None\n        Optionally provide H[i] to override the partial measurement\n        function for this one call, otherwise a slice of self.H will\n        be used.\n    \"\"\"\n\n    if isscalar(z_i):\n        length = 1\n    else:\n        length = len(z_i)\n    z_i = np.reshape(z_i, [length, 1])\n    stop = start + length\n\n    if R_i is None:\n        R_i = self.R[start:stop, start:stop]\n    elif isscalar(R_i):\n        R_i = eye(length) * R_i\n\n    if H_i is None:\n        H_i = self.H[start:stop]\n\n    H_i = np.reshape(H_i, [length, self.dim_x])\n\n    # y_i = z_i - H_i @ x\n    # error (residual) between measurement and prediction\n    y_i = z_i - dot(H_i, self.x)\n    self.y[start:stop] = y_i\n\n    # common subexpression for speed\n    PHT = dot(self.P, H_i.T)\n\n    # project system uncertainty into the measurement subspace\n    S_i = dot(H_i, PHT) + R_i\n\n    if length == 1:\n        K_i = PHT * (1.0 / S_i)\n    else:\n        K_i = dot(PHT, linalg.inv(S_i))\n\n    self.K[:, start:stop] = K_i\n    I_KH = self._I - np.dot(K_i, H_i)\n\n    # x = x + K_i @ y_i\n    # update state estimation with residual scaled by the kalman gain\n    self.x += dot(K_i, y_i)\n\n    # compute the posterior covariance\n    self.P = dot(dot(I_KH, self.P), I_KH.T) + dot(dot(K_i, R_i), K_i.T)\n\n    # save measurement component #i and the posterior state\n    self.z[start:stop] = z_i\n    self.x_post = self.x.copy()\n    self.P_post = self.P.copy()\n</code></pre>"},{"location":"filters/kalman-filter/#bayesian_filters.kalman.kalman_filter.KalmanFilter.update_steadystate","title":"<code>update_steadystate(z)</code>","text":"<p>Add a new measurement (z) to the Kalman filter without recomputing the Kalman gain K, the state covariance P, or the system uncertainty S.</p> <p>You can use this for LTI systems since the Kalman gain and covariance converge to a fixed value. Precompute these and assign them explicitly, or run the Kalman filter using the normal predict()/update(0 cycle until they converge.</p> <p>The main advantage of this call is speed. We do significantly less computation, notably avoiding a costly matrix inversion.</p> <p>Use in conjunction with predict_steadystate(), otherwise P will grow without bound.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>(dim_z, 1): array_like</code> <p>measurement for this update. z can be a scalar if dim_z is 1, otherwise it must be convertible to a column vector.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cv = kinematic_kf(dim=3, order=2) # 3D const velocity filter\n&gt;&gt;&gt; # let filter converge on representative data, then save k and P\n&gt;&gt;&gt; for i in range(100):\n&gt;&gt;&gt;     cv.predict()\n&gt;&gt;&gt;     cv.update([i, i, i])\n&gt;&gt;&gt; saved_k = np.copy(cv.K)\n&gt;&gt;&gt; saved_P = np.copy(cv.P)\n</code></pre> <p>later on:</p> <pre><code>&gt;&gt;&gt; cv = kinematic_kf(dim=3, order=2) # 3D const velocity filter\n&gt;&gt;&gt; cv.K = np.copy(saved_K)\n&gt;&gt;&gt; cv.P = np.copy(saved_P)\n&gt;&gt;&gt; for i in range(100):\n&gt;&gt;&gt;     cv.predict_steadystate()\n&gt;&gt;&gt;     cv.update_steadystate([i, i, i])\n</code></pre> Source code in <code>bayesian_filters/kalman/kalman_filter.py</code> <pre><code>def update_steadystate(self, z):\n    \"\"\"\n    Add a new measurement (z) to the Kalman filter without recomputing\n    the Kalman gain K, the state covariance P, or the system\n    uncertainty S.\n\n    You can use this for LTI systems since the Kalman gain and covariance\n    converge to a fixed value. Precompute these and assign them explicitly,\n    or run the Kalman filter using the normal predict()/update(0 cycle\n    until they converge.\n\n    The main advantage of this call is speed. We do significantly less\n    computation, notably avoiding a costly matrix inversion.\n\n    Use in conjunction with predict_steadystate(), otherwise P will grow\n    without bound.\n\n    Parameters\n    ----------\n    z : (dim_z, 1): array_like\n        measurement for this update. z can be a scalar if dim_z is 1,\n        otherwise it must be convertible to a column vector.\n\n\n    Examples\n    --------\n    &gt;&gt;&gt; cv = kinematic_kf(dim=3, order=2) # 3D const velocity filter\n    &gt;&gt;&gt; # let filter converge on representative data, then save k and P\n    &gt;&gt;&gt; for i in range(100):\n    &gt;&gt;&gt;     cv.predict()\n    &gt;&gt;&gt;     cv.update([i, i, i])\n    &gt;&gt;&gt; saved_k = np.copy(cv.K)\n    &gt;&gt;&gt; saved_P = np.copy(cv.P)\n\n    later on:\n\n    &gt;&gt;&gt; cv = kinematic_kf(dim=3, order=2) # 3D const velocity filter\n    &gt;&gt;&gt; cv.K = np.copy(saved_K)\n    &gt;&gt;&gt; cv.P = np.copy(saved_P)\n    &gt;&gt;&gt; for i in range(100):\n    &gt;&gt;&gt;     cv.predict_steadystate()\n    &gt;&gt;&gt;     cv.update_steadystate([i, i, i])\n    \"\"\"\n\n    # set to None to force recompute\n    self._log_likelihood = None\n    self._likelihood = None\n    self._mahalanobis = None\n\n    if z is None:\n        self.z = np.array([[None] * self.dim_z]).T\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n        self.y = zeros((self.dim_z, 1))\n        return\n\n    z = reshape_z(z, self.dim_z, self.x.ndim)\n\n    # y = z - Hx\n    # error (residual) between measurement and prediction\n    self.y = z - dot(self.H, self.x)\n\n    # x = x + Ky\n    # predict new x with residual scaled by the kalman gain\n    self.x = self.x + dot(self.K, self.y)\n\n    self.z = deepcopy(z)\n    self.x_post = self.x.copy()\n    self.P_post = self.P.copy()\n\n    # set to None to force recompute\n    self._log_likelihood = None\n    self._likelihood = None\n    self._mahalanobis = None\n</code></pre>"},{"location":"filters/mmae-filter-bank/","title":"MMAE Filter Bank","text":""},{"location":"filters/mmae-filter-bank/#mmae-filter-bank","title":"MMAE Filter Bank","text":"<p>needs documentation....</p> <p>Example</p> <p>.. code`` from filterpy.kalman import MMAEFilterBank</p> <p>``</p> <pre><code>pos, zs = generate_data(120, noise_factor=0.2)\nz_xs = zs[:, 0]\nt = np.arange(0, len(z_xs) * dt, dt)\n\ndt = 0.1\nfilters = [make_cv_filter(dt), make_ca_filter(dt)]\nH_cv = np.array([[1., 0, 0],\n                 [0., 1, 0]])\n\nH_ca = np.array([[1., 0., 0.],\n                 [0., 1., 0.],\n                 [0., 0., 1.]])\n\nbank = MMAEFilterBank(filters, (0.5, 0.5), dim_x=3, H=(H_cv, H_ca))\n\nxs, probs = [], []\nfor z in z_xs:\n    bank.predict()\n    bank.update(z)\n    xs.append(bank.x[0])\n    probs.append(bank.p[0])\n\nplt.subplot(121)\nplt.plot(xs)\nplt.subplot(122)\nplt.plot(probs)\n</code></pre>"},{"location":"filters/mmae-filter-bank/#api-reference","title":"API Reference","text":""},{"location":"filters/mmae-filter-bank/#bayesian_filters.kalman.mmae.MMAEFilterBank","title":"<code>MMAEFilterBank</code>","text":"<p>               Bases: <code>object</code></p> <p>Implements the fixed Multiple Model Adaptive Estimator (MMAE). This is a bank of independent Kalman filters. This estimator computes the likelihood that each filter is the correct one, and blends their state estimates weighted by their likelihood to produce the state estimate.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>list of Kalman filters</code> <p>List of Kalman filters.</p> required <code>p</code> <code>list-like of floats</code> <p>Initial probability that each filter is the correct one. In general you'd probably set each element to 1./len(p).</p> required <code>dim_x</code> <code>float</code> <p>number of random variables in the state X</p> required <code>H</code> <code>Measurement matrix</code> <code>None</code> <p>Attributes:</p> Name Type Description <code>x</code> <code>array(dim_x, 1)</code> <p>Current state estimate. Any call to update() or predict() updates this variable.</p> <code>P</code> <code>array(dim_x, dim_x)</code> <p>Current state covariance matrix. Any call to update() or predict() updates this variable.</p> <code>x_prior</code> <code>array(dim_x, 1)</code> <p>Prior (predicted) state estimate. The _prior and _post attributes are for convienence; they store the  prior and posterior of the current epoch. Read Only.</p> <code>P_prior</code> <code>array(dim_x, dim_x)</code> <p>Prior (predicted) state covariance matrix. Read Only.</p> <code>x_post</code> <code>array(dim_x, 1)</code> <p>Posterior (updated) state estimate. Read Only.</p> <code>P_post</code> <code>array(dim_x, dim_x)</code> <p>Posterior (updated) state covariance matrix. Read Only.</p> <code>z</code> <code>ndarray</code> <p>Last measurement used in update(). Read only.</p> <code>filters</code> <code>list of Kalman filters</code> <p>List of Kalman filters.</p> <p>Examples:</p> <p>..code:     ca = make_ca_filter(dt, noise_factor=0.6)     cv = make_ca_filter(dt, noise_factor=0.6)     cv.F[:,2] = 0 # remove acceleration term     cv.P[2,2] = 0     cv.Q[2,2] = 0</p> <pre><code>filters = [cv, ca]\nbank = MMAEFilterBank(filters, p=(0.5, 0.5), dim_x=3)\n\nfor z in zs:\n    bank.predict()\n    bank.update(z)\n</code></pre> <p>Also, see my book Kalman and Bayesian Filters in Python https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> References <p>Zarchan and Musoff. \"Fundamentals of Kalman filtering: A Practical Approach.\" AIAA, third edition.</p> Source code in <code>bayesian_filters/kalman/mmae.py</code> <pre><code>class MMAEFilterBank(object):\n    \"\"\"\n    Implements the fixed Multiple Model Adaptive Estimator (MMAE). This\n    is a bank of independent Kalman filters. This estimator computes the\n    likelihood that each filter is the correct one, and blends their state\n    estimates weighted by their likelihood to produce the state estimate.\n\n    Parameters\n    ----------\n\n    filters : list of Kalman filters\n        List of Kalman filters.\n\n    p : list-like of floats\n        Initial probability that each filter is the correct one. In general\n        you'd probably set each element to 1./len(p).\n\n    dim_x : float\n        number of random variables in the state X\n\n    H : Measurement matrix\n\n    Attributes\n    ----------\n    x : numpy.array(dim_x, 1)\n        Current state estimate. Any call to update() or predict() updates\n        this variable.\n\n    P : numpy.array(dim_x, dim_x)\n        Current state covariance matrix. Any call to update() or predict()\n        updates this variable.\n\n    x_prior : numpy.array(dim_x, 1)\n        Prior (predicted) state estimate. The *_prior and *_post attributes\n        are for convienence; they store the  prior and posterior of the\n        current epoch. Read Only.\n\n    P_prior : numpy.array(dim_x, dim_x)\n        Prior (predicted) state covariance matrix. Read Only.\n\n    x_post : numpy.array(dim_x, 1)\n        Posterior (updated) state estimate. Read Only.\n\n    P_post : numpy.array(dim_x, dim_x)\n        Posterior (updated) state covariance matrix. Read Only.\n\n    z : ndarray\n        Last measurement used in update(). Read only.\n\n    filters : list of Kalman filters\n        List of Kalman filters.\n\n    Examples\n    --------\n\n    ..code:\n        ca = make_ca_filter(dt, noise_factor=0.6)\n        cv = make_ca_filter(dt, noise_factor=0.6)\n        cv.F[:,2] = 0 # remove acceleration term\n        cv.P[2,2] = 0\n        cv.Q[2,2] = 0\n\n        filters = [cv, ca]\n        bank = MMAEFilterBank(filters, p=(0.5, 0.5), dim_x=3)\n\n        for z in zs:\n            bank.predict()\n            bank.update(z)\n\n    Also, see my book Kalman and Bayesian Filters in Python\n    https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n\n    References\n    ----------\n\n    Zarchan and Musoff. \"Fundamentals of Kalman filtering: A Practical\n    Approach.\" AIAA, third edition.\n\n    \"\"\"\n\n    def __init__(self, filters, p, dim_x, H=None):\n        if len(filters) != len(p):\n            raise ValueError(\"length of filters and p must be the same\")\n\n        if dim_x &lt; 1:\n            raise ValueError(\"dim_x must be &gt;= 1\")\n\n        self.filters = filters\n        self.p = np.asarray(p)\n        self.dim_x = dim_x\n        if H is None:\n            self.H = None\n        else:\n            self.H = np.copy(H)\n\n        # try to form a reasonable initial values, but good luck!\n        try:\n            self.z = np.copy(filters[0].z)\n            self.x = np.copy(filters[0].x)\n            self.P = np.copy(filters[0].P)\n\n        except AttributeError:\n            self.z = 0\n            self.x = None\n            self.P = None\n\n        # these will always be a copy of x,P after predict() is called\n        self.x_prior = self.x.copy()\n        self.P_prior = self.P.copy()\n\n        # these will always be a copy of x,P after update() is called\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n\n    def predict(self, u=0):\n        \"\"\"\n        Predict next position using the Kalman filter state propagation\n        equations for each filter in the bank.\n\n        Parameters\n        ----------\n\n        u : np.array\n            Optional control vector. If non-zero, it is multiplied by B\n            to create the control input into the system.\n        \"\"\"\n\n        for f in self.filters:\n            f.predict(u)\n\n        # save prior\n        self.x_prior = self.x.copy()\n        self.P_prior = self.P.copy()\n\n    def update(self, z, R=None, H=None):\n        \"\"\"\n        Add a new measurement (z) to the Kalman filter. If z is None, nothing\n        is changed.\n\n        Parameters\n        ----------\n\n        z : np.array\n            measurement for this update.\n\n        R : np.array, scalar, or None\n            Optionally provide R to override the measurement noise for this\n            one call, otherwise  self.R will be used.\n\n        H : np.array,  or None\n            Optionally provide H to override the measurement function for this\n            one call, otherwise  self.H will be used.\n        \"\"\"\n\n        if H is None:\n            H = self.H\n\n        # new probability is recursively defined as prior * likelihood\n        for i, f in enumerate(self.filters):\n            f.update(z, R, H)\n            self.p[i] *= f.likelihood\n\n        self.p /= sum(self.p)  # normalize\n\n        # compute estimated state and covariance of the bank of filters.\n        self.P = np.zeros(self.filters[0].P.shape)\n\n        # state can be in form [x,y,z,...] or [[x, y, z,...]].T\n        is_row_vector = self.filters[0].x.ndim == 1\n        if is_row_vector:\n            self.x = np.zeros(self.dim_x)\n            for f, p in zip(self.filters, self.p):\n                self.x += np.dot(f.x, p)\n        else:\n            self.x = np.zeros((self.dim_x, 1))\n            for f, p in zip(self.filters, self.p):\n                self.x += np.dot(f.x, p)\n\n        for x, f, p in zip(self.x, self.filters, self.p):\n            y = f.x - x\n            self.P += p * (np.outer(y, y) + f.P)\n\n        # save measurement and posterior state\n        self.z = deepcopy(z)\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n\n    def __repr__(self):\n        return \"\\n\".join(\n            [\n                \"MMAEFilterBank object\",\n                pretty_str(\"dim_x\", self.dim_x),\n                pretty_str(\"x\", self.x),\n                pretty_str(\"P\", self.P),\n                pretty_str(\"log-p\", self.p),\n            ]\n        )\n</code></pre>"},{"location":"filters/mmae-filter-bank/#bayesian_filters.kalman.mmae.MMAEFilterBank.predict","title":"<code>predict(u=0)</code>","text":"<p>Predict next position using the Kalman filter state propagation equations for each filter in the bank.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>array</code> <p>Optional control vector. If non-zero, it is multiplied by B to create the control input into the system.</p> <code>0</code> Source code in <code>bayesian_filters/kalman/mmae.py</code> <pre><code>def predict(self, u=0):\n    \"\"\"\n    Predict next position using the Kalman filter state propagation\n    equations for each filter in the bank.\n\n    Parameters\n    ----------\n\n    u : np.array\n        Optional control vector. If non-zero, it is multiplied by B\n        to create the control input into the system.\n    \"\"\"\n\n    for f in self.filters:\n        f.predict(u)\n\n    # save prior\n    self.x_prior = self.x.copy()\n    self.P_prior = self.P.copy()\n</code></pre>"},{"location":"filters/mmae-filter-bank/#bayesian_filters.kalman.mmae.MMAEFilterBank.update","title":"<code>update(z, R=None, H=None)</code>","text":"<p>Add a new measurement (z) to the Kalman filter. If z is None, nothing is changed.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>array</code> <p>measurement for this update.</p> required <code>R</code> <code>np.array, scalar, or None</code> <p>Optionally provide R to override the measurement noise for this one call, otherwise  self.R will be used.</p> <code>None</code> <code>H</code> <code>np.array,  or None</code> <p>Optionally provide H to override the measurement function for this one call, otherwise  self.H will be used.</p> <code>None</code> Source code in <code>bayesian_filters/kalman/mmae.py</code> <pre><code>def update(self, z, R=None, H=None):\n    \"\"\"\n    Add a new measurement (z) to the Kalman filter. If z is None, nothing\n    is changed.\n\n    Parameters\n    ----------\n\n    z : np.array\n        measurement for this update.\n\n    R : np.array, scalar, or None\n        Optionally provide R to override the measurement noise for this\n        one call, otherwise  self.R will be used.\n\n    H : np.array,  or None\n        Optionally provide H to override the measurement function for this\n        one call, otherwise  self.H will be used.\n    \"\"\"\n\n    if H is None:\n        H = self.H\n\n    # new probability is recursively defined as prior * likelihood\n    for i, f in enumerate(self.filters):\n        f.update(z, R, H)\n        self.p[i] *= f.likelihood\n\n    self.p /= sum(self.p)  # normalize\n\n    # compute estimated state and covariance of the bank of filters.\n    self.P = np.zeros(self.filters[0].P.shape)\n\n    # state can be in form [x,y,z,...] or [[x, y, z,...]].T\n    is_row_vector = self.filters[0].x.ndim == 1\n    if is_row_vector:\n        self.x = np.zeros(self.dim_x)\n        for f, p in zip(self.filters, self.p):\n            self.x += np.dot(f.x, p)\n    else:\n        self.x = np.zeros((self.dim_x, 1))\n        for f, p in zip(self.filters, self.p):\n            self.x += np.dot(f.x, p)\n\n    for x, f, p in zip(self.x, self.filters, self.p):\n        y = f.x - x\n        self.P += p * (np.outer(y, y) + f.P)\n\n    # save measurement and posterior state\n    self.z = deepcopy(z)\n    self.x_post = self.x.copy()\n    self.P_post = self.P.copy()\n</code></pre>"},{"location":"filters/square-root-filter/","title":"Square Root Filter","text":""},{"location":"filters/square-root-filter/#squarerootkalmanfilter","title":"SquareRootKalmanFilter","text":""},{"location":"filters/square-root-filter/#introduction-and-overview","title":"Introduction and Overview","text":"<p>This implements a square root Kalman filter. No real attempt has been made to make this fast; it is a pedalogical exercise. The idea is that by computing and storing the square root of the covariance matrix we get about double the significant number of bits. Some authors consider this somewhat unnecessary with modern hardware. Of course, with microcontrollers being all the rage these days, that calculus has changed. But, will you really run a Kalman filter in Python on a tiny chip? No. So, this is for learning.</p>"},{"location":"filters/square-root-filter/#api-reference","title":"API Reference","text":""},{"location":"filters/square-root-filter/#bayesian_filters.kalman.square_root.SquareRootKalmanFilter","title":"<code>SquareRootKalmanFilter</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Kalman filter which uses a square root implementation. This uses the square root of the state covariance matrix, which doubles the numerical precision of the filter, Therebuy reducing the effect of round off errors.</p> <p>It is likely that you do not need to use this algorithm; we understand divergence issues very well now. However, if you expect the covariance matrix P to vary by 20 or more orders of magnitude then perhaps this will be useful to you, as the square root will vary by 10 orders of magnitude. From my point of view this is merely a 'reference' algorithm; I have not used this code in real world software. Brown[1] has a useful discussion of when you might need to use the square root form of this algorithm.</p> <p>You are responsible for setting the various state variables to reasonable values; the defaults below will not give you a functional filter.</p> <p>Parameters:</p> Name Type Description Default <code>dim_x</code> <code>int</code> <p>Number of state variables for the Kalman filter. For example, if you are tracking the position and velocity of an object in two dimensions, dim_x would be 4.</p> <p>This is used to set the default size of P, Q, and u</p> required <code>dim_z</code> <code>int</code> <p>Number of of measurement inputs. For example, if the sensor provides you with position in (x,y), dim_z would be 2.</p> required <code>dim_u</code> <code>int(optional)</code> <p>size of the control input, if it is being used. Default value of 0 indicates it is not used.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>x</code> <code>array(dim_x, 1)</code> <p>State estimate</p> <code>P</code> <code>array(dim_x, dim_x)</code> <p>State covariance matrix</p> <code>x_prior</code> <code>array(dim_x, 1)</code> <p>Prior (predicted) state estimate. The _prior and _post attributes are for convienence; they store the  prior and posterior of the current epoch. Read Only.</p> <code>P_prior</code> <code>array(dim_x, dim_x)</code> <p>Prior (predicted) state covariance matrix. Read Only.</p> <code>x_post</code> <code>array(dim_x, 1)</code> <p>Posterior (updated) state estimate. Read Only.</p> <code>P_post</code> <code>array(dim_x, dim_x)</code> <p>Posterior (updated) state covariance matrix. Read Only.</p> <code>z</code> <code>array</code> <p>Last measurement used in update(). Read only.</p> <code>R</code> <code>array(dim_z, dim_z)</code> <p>Measurement noise matrix</p> <code>Q</code> <code>array(dim_x, dim_x)</code> <p>Process noise matrix</p> <code>F</code> <code>array()</code> <p>State Transition matrix</p> <code>H</code> <code>array(dim_z, dim_x)</code> <p>Measurement function</p> <code>y</code> <code>array</code> <p>Residual of the update step. Read only.</p> <code>K</code> <code>array(dim_x, dim_z)</code> <p>Kalman gain of the update step. Read only.</p> <p>Examples:</p> <p>See my book Kalman and Bayesian Filters in Python https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> References <p>[1] Robert Grover Brown. Introduction to Random Signals and Applied     Kalman Filtering. Wiley and sons, 2012.</p> Source code in <code>bayesian_filters/kalman/square_root.py</code> <pre><code>class SquareRootKalmanFilter(object):\n    \"\"\"\n\n    Create a Kalman filter which uses a square root implementation.\n    This uses the square root of the state covariance matrix, which doubles\n    the numerical precision of the filter, Therebuy reducing the effect\n    of round off errors.\n\n    It is likely that you do not need to use this algorithm; we understand\n    divergence issues very well now. However, if you expect the covariance\n    matrix P to vary by 20 or more orders of magnitude then perhaps this\n    will be useful to you, as the square root will vary by 10 orders\n    of magnitude. From my point of view this is merely a 'reference'\n    algorithm; I have not used this code in real world software. Brown[1]\n    has a useful discussion of when you might need to use the square\n    root form of this algorithm.\n\n    You are responsible for setting the various state variables to\n    reasonable values; the defaults below will not give you a functional\n    filter.\n\n    Parameters\n    ----------\n\n    dim_x : int\n        Number of state variables for the Kalman filter. For example, if\n        you are tracking the position and velocity of an object in two\n        dimensions, dim_x would be 4.\n\n        This is used to set the default size of P, Q, and u\n\n    dim_z : int\n        Number of of measurement inputs. For example, if the sensor\n        provides you with position in (x,y), dim_z would be 2.\n\n    dim_u : int (optional)\n        size of the control input, if it is being used.\n        Default value of 0 indicates it is not used.\n\n\n    Attributes\n    ----------\n\n    x : numpy.array(dim_x, 1)\n        State estimate\n\n    P : numpy.array(dim_x, dim_x)\n        State covariance matrix\n\n    x_prior : numpy.array(dim_x, 1)\n        Prior (predicted) state estimate. The *_prior and *_post attributes\n        are for convienence; they store the  prior and posterior of the\n        current epoch. Read Only.\n\n    P_prior : numpy.array(dim_x, dim_x)\n        Prior (predicted) state covariance matrix. Read Only.\n\n    x_post : numpy.array(dim_x, 1)\n        Posterior (updated) state estimate. Read Only.\n\n    P_post : numpy.array(dim_x, dim_x)\n        Posterior (updated) state covariance matrix. Read Only.\n\n    z : numpy.array\n        Last measurement used in update(). Read only.\n\n    R : numpy.array(dim_z, dim_z)\n        Measurement noise matrix\n\n    Q : numpy.array(dim_x, dim_x)\n        Process noise matrix\n\n    F : numpy.array()\n        State Transition matrix\n\n    H : numpy.array(dim_z, dim_x)\n        Measurement function\n\n    y : numpy.array\n        Residual of the update step. Read only.\n\n    K : numpy.array(dim_x, dim_z)\n        Kalman gain of the update step. Read only.\n\n    Examples\n    --------\n\n    See my book Kalman and Bayesian Filters in Python\n    https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n\n    References\n    ----------\n\n    [1] Robert Grover Brown. Introduction to Random Signals and Applied\n        Kalman Filtering. Wiley and sons, 2012.\n\n    \"\"\"\n\n    def __init__(self, dim_x, dim_z, dim_u=0):\n        if dim_z &lt; 1:\n            raise ValueError(\"dim_x must be 1 or greater\")\n        if dim_z &lt; 1:\n            raise ValueError(\"dim_x must be 1 or greater\")\n        if dim_u &lt; 0:\n            raise ValueError(\"dim_x must be 0 or greater\")\n\n        self.dim_x = dim_x\n        self.dim_z = dim_z\n        self.dim_u = dim_u\n\n        self.x = zeros((dim_x, 1))  # state\n        self._P = eye(dim_x)  # uncertainty covariance\n        self._P1_2 = eye(dim_x)  # sqrt uncertainty covariance\n        self._Q = eye(dim_x)  # sqrt process uncertainty\n        self._Q1_2 = eye(dim_x)  # sqrt process uncertainty\n        self.B = 0.0  # control transition matrix\n        self.F = np.eye(dim_x)  # state transition matrix\n        self.H = np.zeros((dim_z, dim_x))  # Measurement function\n        self._R1_2 = eye(dim_z)  # sqrt state uncertainty\n        self._R = eye(dim_z)  # state uncertainty\n        self.z = np.array([[None] * self.dim_z]).T\n\n        self.K = np.zeros((dim_x, dim_z))  # kalman gain\n        self.S1_2 = np.zeros((dim_z, dim_z))  # sqrt system uncertainty\n        self.SI1_2 = np.zeros((dim_z, dim_z))  # Inverse sqrt system uncertainty\n\n        # Residual is computed during the innovation (update) step. We\n        # save it so that in case you want to inspect it for various\n        # purposes\n        self.y = zeros((dim_z, 1))\n\n        # identity matrix.\n        self._I = np.eye(dim_x)\n\n        self.M = np.zeros((dim_z + dim_x, dim_z + dim_x))\n\n        # copy prior and posterior\n        self.x_prior = np.copy(self.x)\n        self._P1_2_prior = np.copy(self._P1_2)\n        self.x_post = np.copy(self.x)\n        self._P1_2_post = np.copy(self._P1_2)\n\n    def update(self, z, R2=None):\n        \"\"\"\n        Add a new measurement (z) to the kalman filter. If z is None, nothing\n        is changed.\n\n        Parameters\n        ----------\n\n        z : np.array\n            measurement for this update.\n\n        R2 : np.array, scalar, or None\n            Sqrt of meaaurement noize. Optionally provide to override the\n            measurement noise for this one call, otherwise  self.R2 will\n            be used.\n        \"\"\"\n\n        if z is None:\n            self.z = np.array([[None] * self.dim_z]).T\n            self.x_post = self.x.copy()\n            self._P1_2_post = np.copy(self._P1_2)\n            return\n\n        if R2 is None:\n            R2 = self._R1_2\n        elif np.isscalar(R2):\n            R2 = eye(self.dim_z) * R2\n\n        # rename for convienance\n        dim_z = self.dim_z\n        M = self.M\n\n        M[0:dim_z, 0:dim_z] = R2.T\n        M[dim_z:, 0:dim_z] = dot(self.H, self._P1_2).T\n        M[dim_z:, dim_z:] = self._P1_2.T\n\n        _, r_decomp = qr(M)\n        self.S1_2 = r_decomp[0:dim_z, 0:dim_z].T\n        self.SI1_2 = pinv(self.S1_2)\n        self.K = dot(r_decomp[0:dim_z, dim_z:].T, self.SI1_2)\n\n        # y = z - Hx\n        # error (residual) between measurement and prediction\n        self.y = z - dot(self.H, self.x)\n\n        # x = x + Ky\n        # predict new x with residual scaled by the kalman gain\n        self.x += dot(self.K, self.y)\n        self._P1_2 = r_decomp[dim_z:, dim_z:].T\n\n        self.z = deepcopy(z)\n        self.x_post = self.x.copy()\n        self._P1_2_post = np.copy(self._P1_2)\n\n    def predict(self, u=0):\n        \"\"\"\n        Predict next state (prior) using the Kalman filter state propagation\n        equations.\n\n        Parameters\n        ----------\n\n        u : np.array, optional\n            Optional control vector. If non-zero, it is multiplied by B\n            to create the control input into the system.\n        \"\"\"\n\n        # x = Fx + Bu\n        self.x = dot(self.F, self.x) + dot(self.B, u)\n\n        # P = FPF' + Q\n        _, P2 = qr(np.hstack([dot(self.F, self._P1_2), self._Q1_2]).T)\n        self._P1_2 = P2[: self.dim_x, : self.dim_x].T\n\n        # copy prior\n        self.x_prior = np.copy(self.x)\n        self._P1_2_prior = np.copy(self._P1_2)\n\n    def residual_of(self, z):\n        \"\"\"returns the residual for the given measurement (z). Does not alter\n        the state of the filter.\n        \"\"\"\n\n        return z - dot(self.H, self.x)\n\n    def measurement_of_state(self, x):\n        \"\"\"Helper function that converts a state into a measurement.\n\n        Parameters\n        ----------\n\n        x : np.array\n            kalman state vector\n\n        Returns\n        -------\n\n        z : np.array\n            measurement corresponding to the given state\n        \"\"\"\n        return dot(self.H, x)\n\n    @property\n    def Q(self):\n        \"\"\"Process uncertainty\"\"\"\n        return dot(self._Q1_2, self._Q1_2.T)\n\n    @property\n    def Q1_2(self):\n        \"\"\"Sqrt Process uncertainty\"\"\"\n        return self._Q1_2\n\n    @Q.setter\n    def Q(self, value):\n        \"\"\"Process uncertainty\"\"\"\n        self._Q = value\n        self._Q1_2 = cholesky(self._Q, lower=True)\n\n    @property\n    def P(self):\n        \"\"\"covariance matrix\"\"\"\n        return dot(self._P1_2, self._P1_2.T)\n\n    @property\n    def P_prior(self):\n        \"\"\"covariance matrix of the prior\"\"\"\n        return dot(self._P1_2_prior, self._P1_2_prior.T)\n\n    @property\n    def P_post(self):\n        \"\"\"covariance matrix of the posterior\"\"\"\n        return dot(self._P1_2_prior, self._P1_2_prior.T)\n\n    @property\n    def P1_2(self):\n        \"\"\"sqrt of covariance matrix\"\"\"\n        return self._P1_2\n\n    @P.setter\n    def P(self, value):\n        \"\"\"covariance matrix\"\"\"\n        self._P = value\n        self._P1_2 = cholesky(self._P, lower=True)\n\n    @property\n    def R(self):\n        \"\"\"measurement uncertainty\"\"\"\n        return dot(self._R1_2, self._R1_2.T)\n\n    @property\n    def R1_2(self):\n        \"\"\"sqrt of measurement uncertainty\"\"\"\n        return self._R1_2\n\n    @R.setter\n    def R(self, value):\n        \"\"\"measurement uncertainty\"\"\"\n        self._R = value\n        self._R1_2 = cholesky(self._R, lower=True)\n\n    @property\n    def S(self):\n        \"\"\"system uncertainty (P projected to measurement space)\"\"\"\n        return dot(self.S1_2, self.S1_2.T)\n\n    @property\n    def SI(self):\n        \"\"\"inverse system uncertainty (P projected to measurement space)\"\"\"\n        return dot(self.SI1_2.T, self.SI1_2)\n\n    def __repr__(self):\n        return \"\\n\".join(\n            [\n                \"SquareRootKalmanFilter object\",\n                pretty_str(\"dim_x\", self.dim_x),\n                pretty_str(\"dim_z\", self.dim_z),\n                pretty_str(\"dim_u\", self.dim_u),\n                pretty_str(\"x\", self.x),\n                pretty_str(\"P\", self.P),\n                pretty_str(\"F\", self.F),\n                pretty_str(\"Q\", self.Q),\n                pretty_str(\"R\", self.R),\n                pretty_str(\"H\", self.H),\n                pretty_str(\"K\", self.K),\n                pretty_str(\"y\", self.y),\n                pretty_str(\"S\", self.S),\n                pretty_str(\"SI\", self.SI),\n                pretty_str(\"M\", self.M),\n                pretty_str(\"B\", self.B),\n            ]\n        )\n</code></pre>"},{"location":"filters/square-root-filter/#bayesian_filters.kalman.square_root.SquareRootKalmanFilter.P","title":"<code>P</code>  <code>property</code> <code>writable</code>","text":"<p>covariance matrix</p>"},{"location":"filters/square-root-filter/#bayesian_filters.kalman.square_root.SquareRootKalmanFilter.P1_2","title":"<code>P1_2</code>  <code>property</code>","text":"<p>sqrt of covariance matrix</p>"},{"location":"filters/square-root-filter/#bayesian_filters.kalman.square_root.SquareRootKalmanFilter.P_post","title":"<code>P_post</code>  <code>property</code>","text":"<p>covariance matrix of the posterior</p>"},{"location":"filters/square-root-filter/#bayesian_filters.kalman.square_root.SquareRootKalmanFilter.P_prior","title":"<code>P_prior</code>  <code>property</code>","text":"<p>covariance matrix of the prior</p>"},{"location":"filters/square-root-filter/#bayesian_filters.kalman.square_root.SquareRootKalmanFilter.Q","title":"<code>Q</code>  <code>property</code> <code>writable</code>","text":"<p>Process uncertainty</p>"},{"location":"filters/square-root-filter/#bayesian_filters.kalman.square_root.SquareRootKalmanFilter.Q1_2","title":"<code>Q1_2</code>  <code>property</code>","text":"<p>Sqrt Process uncertainty</p>"},{"location":"filters/square-root-filter/#bayesian_filters.kalman.square_root.SquareRootKalmanFilter.R","title":"<code>R</code>  <code>property</code> <code>writable</code>","text":"<p>measurement uncertainty</p>"},{"location":"filters/square-root-filter/#bayesian_filters.kalman.square_root.SquareRootKalmanFilter.R1_2","title":"<code>R1_2</code>  <code>property</code>","text":"<p>sqrt of measurement uncertainty</p>"},{"location":"filters/square-root-filter/#bayesian_filters.kalman.square_root.SquareRootKalmanFilter.S","title":"<code>S</code>  <code>property</code>","text":"<p>system uncertainty (P projected to measurement space)</p>"},{"location":"filters/square-root-filter/#bayesian_filters.kalman.square_root.SquareRootKalmanFilter.SI","title":"<code>SI</code>  <code>property</code>","text":"<p>inverse system uncertainty (P projected to measurement space)</p>"},{"location":"filters/square-root-filter/#bayesian_filters.kalman.square_root.SquareRootKalmanFilter.measurement_of_state","title":"<code>measurement_of_state(x)</code>","text":"<p>Helper function that converts a state into a measurement.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>kalman state vector</p> required <p>Returns:</p> Name Type Description <code>z</code> <code>array</code> <p>measurement corresponding to the given state</p> Source code in <code>bayesian_filters/kalman/square_root.py</code> <pre><code>def measurement_of_state(self, x):\n    \"\"\"Helper function that converts a state into a measurement.\n\n    Parameters\n    ----------\n\n    x : np.array\n        kalman state vector\n\n    Returns\n    -------\n\n    z : np.array\n        measurement corresponding to the given state\n    \"\"\"\n    return dot(self.H, x)\n</code></pre>"},{"location":"filters/square-root-filter/#bayesian_filters.kalman.square_root.SquareRootKalmanFilter.predict","title":"<code>predict(u=0)</code>","text":"<p>Predict next state (prior) using the Kalman filter state propagation equations.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>array</code> <p>Optional control vector. If non-zero, it is multiplied by B to create the control input into the system.</p> <code>0</code> Source code in <code>bayesian_filters/kalman/square_root.py</code> <pre><code>def predict(self, u=0):\n    \"\"\"\n    Predict next state (prior) using the Kalman filter state propagation\n    equations.\n\n    Parameters\n    ----------\n\n    u : np.array, optional\n        Optional control vector. If non-zero, it is multiplied by B\n        to create the control input into the system.\n    \"\"\"\n\n    # x = Fx + Bu\n    self.x = dot(self.F, self.x) + dot(self.B, u)\n\n    # P = FPF' + Q\n    _, P2 = qr(np.hstack([dot(self.F, self._P1_2), self._Q1_2]).T)\n    self._P1_2 = P2[: self.dim_x, : self.dim_x].T\n\n    # copy prior\n    self.x_prior = np.copy(self.x)\n    self._P1_2_prior = np.copy(self._P1_2)\n</code></pre>"},{"location":"filters/square-root-filter/#bayesian_filters.kalman.square_root.SquareRootKalmanFilter.residual_of","title":"<code>residual_of(z)</code>","text":"<p>returns the residual for the given measurement (z). Does not alter the state of the filter.</p> Source code in <code>bayesian_filters/kalman/square_root.py</code> <pre><code>def residual_of(self, z):\n    \"\"\"returns the residual for the given measurement (z). Does not alter\n    the state of the filter.\n    \"\"\"\n\n    return z - dot(self.H, self.x)\n</code></pre>"},{"location":"filters/square-root-filter/#bayesian_filters.kalman.square_root.SquareRootKalmanFilter.update","title":"<code>update(z, R2=None)</code>","text":"<p>Add a new measurement (z) to the kalman filter. If z is None, nothing is changed.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>array</code> <p>measurement for this update.</p> required <code>R2</code> <code>np.array, scalar, or None</code> <p>Sqrt of meaaurement noize. Optionally provide to override the measurement noise for this one call, otherwise  self.R2 will be used.</p> <code>None</code> Source code in <code>bayesian_filters/kalman/square_root.py</code> <pre><code>def update(self, z, R2=None):\n    \"\"\"\n    Add a new measurement (z) to the kalman filter. If z is None, nothing\n    is changed.\n\n    Parameters\n    ----------\n\n    z : np.array\n        measurement for this update.\n\n    R2 : np.array, scalar, or None\n        Sqrt of meaaurement noize. Optionally provide to override the\n        measurement noise for this one call, otherwise  self.R2 will\n        be used.\n    \"\"\"\n\n    if z is None:\n        self.z = np.array([[None] * self.dim_z]).T\n        self.x_post = self.x.copy()\n        self._P1_2_post = np.copy(self._P1_2)\n        return\n\n    if R2 is None:\n        R2 = self._R1_2\n    elif np.isscalar(R2):\n        R2 = eye(self.dim_z) * R2\n\n    # rename for convienance\n    dim_z = self.dim_z\n    M = self.M\n\n    M[0:dim_z, 0:dim_z] = R2.T\n    M[dim_z:, 0:dim_z] = dot(self.H, self._P1_2).T\n    M[dim_z:, dim_z:] = self._P1_2.T\n\n    _, r_decomp = qr(M)\n    self.S1_2 = r_decomp[0:dim_z, 0:dim_z].T\n    self.SI1_2 = pinv(self.S1_2)\n    self.K = dot(r_decomp[0:dim_z, dim_z:].T, self.SI1_2)\n\n    # y = z - Hx\n    # error (residual) between measurement and prediction\n    self.y = z - dot(self.H, self.x)\n\n    # x = x + Ky\n    # predict new x with residual scaled by the kalman gain\n    self.x += dot(self.K, self.y)\n    self._P1_2 = r_decomp[dim_z:, dim_z:].T\n\n    self.z = deepcopy(z)\n    self.x_post = self.x.copy()\n    self._P1_2_post = np.copy(self._P1_2)\n</code></pre>"},{"location":"filters/unscented-kalman-filter/","title":"Unscented Kalman Filter","text":""},{"location":"filters/unscented-kalman-filter/#unscentedkalmanfilter","title":"UnscentedKalmanFilter","text":""},{"location":"filters/unscented-kalman-filter/#introduction-and-overview","title":"Introduction and Overview","text":"<p>This implements the unscented Kalman filter.</p>"},{"location":"filters/unscented-kalman-filter/#api-reference","title":"API Reference","text":""},{"location":"filters/unscented-kalman-filter/#unscentedkalmanfilter_1","title":"UnscentedKalmanFilter","text":""},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.UKF.UnscentedKalmanFilter","title":"<code>UnscentedKalmanFilter</code>","text":"<p>               Bases: <code>object</code></p> <p>Implements the Scaled Unscented Kalman filter (UKF) as defined by Simon Julier in [1], using the formulation provided by Wan and Merle in [2]. This filter scales the sigma points to avoid strong nonlinearities.</p> <p>Parameters:</p> Name Type Description Default <code>dim_x</code> <code>int</code> <p>Number of state variables for the filter. For example, if you are tracking the position and velocity of an object in two dimensions, dim_x would be 4.</p> required <code>dim_z</code> <code>int</code> <p>Number of of measurement inputs. For example, if the sensor provides you with position in (x,y), dim_z would be 2.</p> <p>This is for convience, so everything is sized correctly on creation. If you are using multiple sensors the size of <code>z</code> can change based on the sensor. Just provide the appropriate hx function</p> required <code>hx</code> <code>function(x, **hx_args)</code> <p>Measurement function. Converts state vector x into a measurement vector of shape (dim_z).</p> required <code>fx</code> <code>function(x, dt, **fx_args)</code> <p>function that returns the state x transformed by the state transition function. dt is the time step in seconds.</p> required <code>points</code> <code>class</code> <p>Class which computes the sigma points and weights for a UKF algorithm. You can vary the UKF implementation by changing this class. For example, MerweScaledSigmaPoints implements the alpha, beta, kappa parameterization of Van der Merwe, and JulierSigmaPoints implements Julier's original kappa parameterization. See either of those for the required signature of this class if you want to implement your own.</p> required <code>sqrt_fn</code> <code>callable(ndarray)</code> <p>Defines how we compute the square root of a matrix, which has no unique answer. Cholesky is the default choice due to its speed. Typically your alternative choice will be scipy.linalg.sqrtm. Different choices affect how the sigma points are arranged relative to the eigenvectors of the covariance matrix. Usually this will not matter to you; if so the default cholesky() yields maximal performance. As of van der Merwe's dissertation of 2004 [6] this was not a well reseached area so I have no advice to give you.</p> <p>If your method returns a triangular matrix it must be upper triangular. Do not use numpy.linalg.cholesky - for historical reasons it returns a lower triangular matrix. The SciPy version does the right thing as far as this class is concerned.</p> <code>None (implies scipy.linalg.cholesky)</code> <code>x_mean_fn</code> <code>callable(sigma_points, weights)</code> <p>Function that computes the mean of the provided sigma points and weights. Use this if your state variable contains nonlinear values such as angles which cannot be summed.</p> <p>.. code-block:: Python</p> <pre><code>def state_mean(sigmas, Wm):\n    x = np.zeros(3)\n    sum_sin, sum_cos = 0., 0.\n\n    for i in range(len(sigmas)):\n        s = sigmas[i]\n        x[0] += s[0] * Wm[i]\n        x[1] += s[1] * Wm[i]\n        sum_sin += sin(s[2])*Wm[i]\n        sum_cos += cos(s[2])*Wm[i]\n    x[2] = atan2(sum_sin, sum_cos)\n    return x\n</code></pre> <code>None</code> <code>z_mean_fn</code> <code>callable(sigma_points, weights)</code> <p>Same as x_mean_fn, except it is called for sigma points which form the measurements after being passed through hx().</p> <code>None</code> <code>residual_x</code> <code>callable(x, y)</code> <code>None</code> <code>residual_z</code> <code>callable(x, y)</code> <p>Function that computes the residual (difference) between x and y. You will have to supply this if your state variable cannot support subtraction, such as angles (359-1 degreees is 2, not 358). x and y are state vectors, not scalars. One is for the state variable, the other is for the measurement state.</p> <p>.. code-block:: Python</p> <pre><code>def residual(a, b):\n    y = a[0] - b[0]\n    if y &gt; np.pi:\n        y -= 2*np.pi\n    if y &lt; -np.pi:\n        y += 2*np.pi\n    return y\n</code></pre> <code>None</code> <code>state_add</code> <p>Function that subtracts two state vectors, returning a new state vector. Used during update to compute <code>x + K@y</code> You will have to supply this if your state variable does not suport addition, such as it contains angles.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>x</code> <code>array(dim_x)</code> <p>state estimate vector</p> <code>P</code> <code>array(dim_x, dim_x)</code> <p>covariance estimate matrix</p> <code>x_prior</code> <code>array(dim_x)</code> <p>Prior (predicted) state estimate. The _prior and _post attributes are for convienence; they store the  prior and posterior of the current epoch. Read Only.</p> <code>P_prior</code> <code>array(dim_x, dim_x)</code> <p>Prior (predicted) state covariance matrix. Read Only.</p> <code>x_post</code> <code>array(dim_x)</code> <p>Posterior (updated) state estimate. Read Only.</p> <code>P_post</code> <code>array(dim_x, dim_x)</code> <p>Posterior (updated) state covariance matrix. Read Only.</p> <code>z</code> <code>ndarray</code> <p>Last measurement used in update(). Read only.</p> <code>R</code> <code>array(dim_z, dim_z)</code> <p>measurement noise matrix</p> <code>Q</code> <code>array(dim_x, dim_x)</code> <p>process noise matrix</p> <code>K</code> <code>array</code> <p>Kalman gain</p> <code>y</code> <code>array</code> <p>innovation residual</p> <code>log_likelihood</code> <code>scalar</code> <p>Log likelihood of last measurement update.</p> <code>likelihood</code> <code>float</code> <p>likelihood of last measurment. Read only.</p> <p>Computed from the log-likelihood. The log-likelihood can be very small,  meaning a large negative value such as -28000. Taking the exp() of that results in 0.0, which can break typical algorithms which multiply by this value, so by default we always return a number &gt;= sys.float_info.min.</p> <code>mahalanobis</code> <code>float</code> <p>mahalanobis distance of the measurement. Read only.</p> <code>inv</code> <code>function, default numpy.linalg.inv</code> <p>If you prefer another inverse function, such as the Moore-Penrose pseudo inverse, set it to that instead:</p> <p>.. code-block:: Python</p> <pre><code>kf.inv = np.linalg.pinv\n</code></pre> <p>Examples:</p> <p>Simple example of a linear order 1 kinematic filter in 2D. There is no need to use a UKF for this example, but it is easy to read.</p> <pre><code>&gt;&gt;&gt; def fx(x, dt):\n&gt;&gt;&gt;     # state transition function - predict next state based\n&gt;&gt;&gt;     # on constant velocity model x = vt + x_0\n&gt;&gt;&gt;     F = np.array([[1, dt, 0, 0],\n&gt;&gt;&gt;                   [0, 1, 0, 0],\n&gt;&gt;&gt;                   [0, 0, 1, dt],\n&gt;&gt;&gt;                   [0, 0, 0, 1]], dtype=float)\n&gt;&gt;&gt;     return np.dot(F, x)\n&gt;&gt;&gt;\n&gt;&gt;&gt; def hx(x):\n&gt;&gt;&gt;    # measurement function - convert state into a measurement\n&gt;&gt;&gt;    # where measurements are [x_pos, y_pos]\n&gt;&gt;&gt;    return np.array([x[0], x[2]])\n&gt;&gt;&gt;\n&gt;&gt;&gt; dt = 0.1\n&gt;&gt;&gt; # create sigma points to use in the filter. This is standard for Gaussian processes\n&gt;&gt;&gt; points = MerweScaledSigmaPoints(4, alpha=.1, beta=2., kappa=-1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; kf = UnscentedKalmanFilter(dim_x=4, dim_z=2, dt=dt, fx=fx, hx=hx, points=points)\n&gt;&gt;&gt; kf.x = np.array([-1., 1., -1., 1]) # initial state\n&gt;&gt;&gt; kf.P *= 0.2 # initial uncertainty\n&gt;&gt;&gt; z_std = 0.1\n&gt;&gt;&gt; kf.R = np.diag([z_std**2, z_std**2]) # 1 standard\n&gt;&gt;&gt; kf.Q = Q_discrete_white_noise(dim=2, dt=dt, var=0.01**2, block_size=2)\n&gt;&gt;&gt;\n&gt;&gt;&gt; zs = [[i+randn()*z_std, i+randn()*z_std] for i in range(50)] # measurements\n&gt;&gt;&gt; for z in zs:\n&gt;&gt;&gt;     kf.predict()\n&gt;&gt;&gt;     kf.update(z)\n&gt;&gt;&gt;     print(kf.x, 'log-likelihood', kf.log_likelihood)\n</code></pre> <p>For in depth explanations see my book Kalman and Bayesian Filters in Python https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> <p>Also see the filterpy/kalman/tests subdirectory for test code that may be illuminating.</p> References <p>.. [1] Julier, Simon J. \"The scaled unscented transformation,\"     American Control Converence, 2002, pp 4555-4559, vol 6.</p> <pre><code>Online copy:\nhttps://www.cs.unc.edu/~welch/kalman/media/pdf/ACC02-IEEE1357.PDF\n</code></pre> <p>.. [2] E. A. Wan and R. Van der Merwe, \u201cThe unscented Kalman filter for     nonlinear estimation,\u201d in Proc. Symp. Adaptive Syst. Signal     Process., Commun. Contr., Lake Louise, AB, Canada, Oct. 2000.</p> <pre><code>Online Copy:\nhttps://www.seas.harvard.edu/courses/cs281/papers/unscented.pdf\n</code></pre> <p>.. [3] S. Julier, J. Uhlmann, and H. Durrant-Whyte. \"A new method for        the nonlinear transformation of means and covariances in filters        and estimators,\" IEEE Transactions on Automatic Control, 45(3),        pp. 477-482 (March 2000).</p> <p>.. [4] E. A. Wan and R. Van der Merwe, \u201cThe Unscented Kalman filter for        Nonlinear Estimation,\u201d in Proc. Symp. Adaptive Syst. Signal        Process., Commun. Contr., Lake Louise, AB, Canada, Oct. 2000.</p> <pre><code>   https://www.seas.harvard.edu/courses/cs281/papers/unscented.pdf\n</code></pre> <p>.. [5] Wan, Merle \"The Unscented Kalman Filter,\" chapter in Kalman        Filtering and Neural Networks, John Wiley &amp; Sons, Inc., 2001.</p> <p>.. [6] R. Van der Merwe \"Sigma-Point Kalman Filters for Probabilitic        Inference in Dynamic State-Space Models\" (Doctoral dissertation)</p> Source code in <code>bayesian_filters/kalman/UKF.py</code> <pre><code>class UnscentedKalmanFilter(object):\n    # pylint: disable=too-many-instance-attributes\n    # pylint: disable=invalid-name\n    r\"\"\"\n    Implements the Scaled Unscented Kalman filter (UKF) as defined by\n    Simon Julier in [1], using the formulation provided by Wan and Merle\n    in [2]. This filter scales the sigma points to avoid strong nonlinearities.\n\n\n    Parameters\n    ----------\n\n    dim_x : int\n        Number of state variables for the filter. For example, if\n        you are tracking the position and velocity of an object in two\n        dimensions, dim_x would be 4.\n\n\n    dim_z : int\n        Number of of measurement inputs. For example, if the sensor\n        provides you with position in (x,y), dim_z would be 2.\n\n        This is for convience, so everything is sized correctly on\n        creation. If you are using multiple sensors the size of `z` can\n        change based on the sensor. Just provide the appropriate hx function\n\n\n\n\n\n    hx : function(x,**hx_args)\n        Measurement function. Converts state vector x into a measurement\n        vector of shape (dim_z).\n\n    fx : function(x,dt,**fx_args)\n        function that returns the state x transformed by the\n        state transition function. dt is the time step in seconds.\n\n    points : class\n        Class which computes the sigma points and weights for a UKF\n        algorithm. You can vary the UKF implementation by changing this\n        class. For example, MerweScaledSigmaPoints implements the alpha,\n        beta, kappa parameterization of Van der Merwe, and\n        JulierSigmaPoints implements Julier's original kappa\n        parameterization. See either of those for the required\n        signature of this class if you want to implement your own.\n\n    sqrt_fn : callable(ndarray), default=None (implies scipy.linalg.cholesky)\n        Defines how we compute the square root of a matrix, which has\n        no unique answer. Cholesky is the default choice due to its\n        speed. Typically your alternative choice will be\n        scipy.linalg.sqrtm. Different choices affect how the sigma points\n        are arranged relative to the eigenvectors of the covariance matrix.\n        Usually this will not matter to you; if so the default cholesky()\n        yields maximal performance. As of van der Merwe's dissertation of\n        2004 [6] this was not a well reseached area so I have no advice\n        to give you.\n\n        If your method returns a triangular matrix it must be upper\n        triangular. Do not use numpy.linalg.cholesky - for historical\n        reasons it returns a lower triangular matrix. The SciPy version\n        does the right thing as far as this class is concerned.\n\n    x_mean_fn : callable  (sigma_points, weights), optional\n        Function that computes the mean of the provided sigma points\n        and weights. Use this if your state variable contains nonlinear\n        values such as angles which cannot be summed.\n\n        .. code-block:: Python\n\n            def state_mean(sigmas, Wm):\n                x = np.zeros(3)\n                sum_sin, sum_cos = 0., 0.\n\n                for i in range(len(sigmas)):\n                    s = sigmas[i]\n                    x[0] += s[0] * Wm[i]\n                    x[1] += s[1] * Wm[i]\n                    sum_sin += sin(s[2])*Wm[i]\n                    sum_cos += cos(s[2])*Wm[i]\n                x[2] = atan2(sum_sin, sum_cos)\n                return x\n\n    z_mean_fn : callable  (sigma_points, weights), optional\n        Same as x_mean_fn, except it is called for sigma points which\n        form the measurements after being passed through hx().\n\n    residual_x : callable (x, y), optional\n    residual_z : callable (x, y), optional\n        Function that computes the residual (difference) between x and y.\n        You will have to supply this if your state variable cannot support\n        subtraction, such as angles (359-1 degreees is 2, not 358). x and y\n        are state vectors, not scalars. One is for the state variable,\n        the other is for the measurement state.\n\n        .. code-block:: Python\n\n            def residual(a, b):\n                y = a[0] - b[0]\n                if y &gt; np.pi:\n                    y -= 2*np.pi\n                if y &lt; -np.pi:\n                    y += 2*np.pi\n                return y\n\n    state_add: callable (x, y), optional, default np.add\n        Function that subtracts two state vectors, returning a new\n        state vector. Used during update to compute `x + K@y`\n        You will have to supply this if your state variable does not\n        suport addition, such as it contains angles.\n\n    Attributes\n    ----------\n\n    x : numpy.array(dim_x)\n        state estimate vector\n\n    P : numpy.array(dim_x, dim_x)\n        covariance estimate matrix\n\n    x_prior : numpy.array(dim_x)\n        Prior (predicted) state estimate. The *_prior and *_post attributes\n        are for convienence; they store the  prior and posterior of the\n        current epoch. Read Only.\n\n    P_prior : numpy.array(dim_x, dim_x)\n        Prior (predicted) state covariance matrix. Read Only.\n\n    x_post : numpy.array(dim_x)\n        Posterior (updated) state estimate. Read Only.\n\n    P_post : numpy.array(dim_x, dim_x)\n        Posterior (updated) state covariance matrix. Read Only.\n\n    z : ndarray\n        Last measurement used in update(). Read only.\n\n    R : numpy.array(dim_z, dim_z)\n        measurement noise matrix\n\n    Q : numpy.array(dim_x, dim_x)\n        process noise matrix\n\n    K : numpy.array\n        Kalman gain\n\n    y : numpy.array\n        innovation residual\n\n    log_likelihood : scalar\n        Log likelihood of last measurement update.\n\n    likelihood : float\n        likelihood of last measurment. Read only.\n\n        Computed from the log-likelihood. The log-likelihood can be very\n        small,  meaning a large negative value such as -28000. Taking the\n        exp() of that results in 0.0, which can break typical algorithms\n        which multiply by this value, so by default we always return a\n        number &gt;= sys.float_info.min.\n\n    mahalanobis : float\n        mahalanobis distance of the measurement. Read only.\n\n    inv : function, default numpy.linalg.inv\n        If you prefer another inverse function, such as the Moore-Penrose\n        pseudo inverse, set it to that instead:\n\n        .. code-block:: Python\n\n            kf.inv = np.linalg.pinv\n\n\n    Examples\n    --------\n\n    Simple example of a linear order 1 kinematic filter in 2D. There is no\n    need to use a UKF for this example, but it is easy to read.\n\n    &gt;&gt;&gt; def fx(x, dt):\n    &gt;&gt;&gt;     # state transition function - predict next state based\n    &gt;&gt;&gt;     # on constant velocity model x = vt + x_0\n    &gt;&gt;&gt;     F = np.array([[1, dt, 0, 0],\n    &gt;&gt;&gt;                   [0, 1, 0, 0],\n    &gt;&gt;&gt;                   [0, 0, 1, dt],\n    &gt;&gt;&gt;                   [0, 0, 0, 1]], dtype=float)\n    &gt;&gt;&gt;     return np.dot(F, x)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; def hx(x):\n    &gt;&gt;&gt;    # measurement function - convert state into a measurement\n    &gt;&gt;&gt;    # where measurements are [x_pos, y_pos]\n    &gt;&gt;&gt;    return np.array([x[0], x[2]])\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; dt = 0.1\n    &gt;&gt;&gt; # create sigma points to use in the filter. This is standard for Gaussian processes\n    &gt;&gt;&gt; points = MerweScaledSigmaPoints(4, alpha=.1, beta=2., kappa=-1)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; kf = UnscentedKalmanFilter(dim_x=4, dim_z=2, dt=dt, fx=fx, hx=hx, points=points)\n    &gt;&gt;&gt; kf.x = np.array([-1., 1., -1., 1]) # initial state\n    &gt;&gt;&gt; kf.P *= 0.2 # initial uncertainty\n    &gt;&gt;&gt; z_std = 0.1\n    &gt;&gt;&gt; kf.R = np.diag([z_std**2, z_std**2]) # 1 standard\n    &gt;&gt;&gt; kf.Q = Q_discrete_white_noise(dim=2, dt=dt, var=0.01**2, block_size=2)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; zs = [[i+randn()*z_std, i+randn()*z_std] for i in range(50)] # measurements\n    &gt;&gt;&gt; for z in zs:\n    &gt;&gt;&gt;     kf.predict()\n    &gt;&gt;&gt;     kf.update(z)\n    &gt;&gt;&gt;     print(kf.x, 'log-likelihood', kf.log_likelihood)\n\n    For in depth explanations see my book Kalman and Bayesian Filters in Python\n    https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n\n    Also see the filterpy/kalman/tests subdirectory for test code that\n    may be illuminating.\n\n    References\n    ----------\n\n    .. [1] Julier, Simon J. \"The scaled unscented transformation,\"\n        American Control Converence, 2002, pp 4555-4559, vol 6.\n\n        Online copy:\n        https://www.cs.unc.edu/~welch/kalman/media/pdf/ACC02-IEEE1357.PDF\n\n    .. [2] E. A. Wan and R. Van der Merwe, \u201cThe unscented Kalman filter for\n        nonlinear estimation,\u201d in Proc. Symp. Adaptive Syst. Signal\n        Process., Commun. Contr., Lake Louise, AB, Canada, Oct. 2000.\n\n        Online Copy:\n        https://www.seas.harvard.edu/courses/cs281/papers/unscented.pdf\n\n    .. [3] S. Julier, J. Uhlmann, and H. Durrant-Whyte. \"A new method for\n           the nonlinear transformation of means and covariances in filters\n           and estimators,\" IEEE Transactions on Automatic Control, 45(3),\n           pp. 477-482 (March 2000).\n\n    .. [4] E. A. Wan and R. Van der Merwe, \u201cThe Unscented Kalman filter for\n           Nonlinear Estimation,\u201d in Proc. Symp. Adaptive Syst. Signal\n           Process., Commun. Contr., Lake Louise, AB, Canada, Oct. 2000.\n\n           https://www.seas.harvard.edu/courses/cs281/papers/unscented.pdf\n\n    .. [5] Wan, Merle \"The Unscented Kalman Filter,\" chapter in *Kalman\n           Filtering and Neural Networks*, John Wiley &amp; Sons, Inc., 2001.\n\n    .. [6] R. Van der Merwe \"Sigma-Point Kalman Filters for Probabilitic\n           Inference in Dynamic State-Space Models\" (Doctoral dissertation)\n    \"\"\"\n\n    def __init__(\n        self,\n        dim_x,\n        dim_z,\n        dt,\n        hx,\n        fx,\n        points,\n        sqrt_fn=None,\n        x_mean_fn=None,\n        z_mean_fn=None,\n        residual_x=None,\n        residual_z=None,\n        state_add=None,\n    ):\n        \"\"\"\n        Create a Kalman filter. You are responsible for setting the\n        various state variables to reasonable values; the defaults below will\n        not give you a functional filter.\n\n        \"\"\"\n\n        # pylint: disable=too-many-arguments\n\n        self.x = zeros(dim_x)\n        self.P = eye(dim_x)\n        self.x_prior = np.copy(self.x)\n        self.P_prior = np.copy(self.P)\n        self.Q = eye(dim_x)\n        self.R = eye(dim_z)\n        self._dim_x = dim_x\n        self._dim_z = dim_z\n        self.points_fn = points\n        self._dt = dt\n        self._num_sigmas = points.num_sigmas()\n        self.hx = hx\n        self.fx = fx\n        self.x_mean = x_mean_fn\n        self.z_mean = z_mean_fn\n\n        # Only computed only if requested via property\n        self._log_likelihood = log(sys.float_info.min)\n        self._likelihood = sys.float_info.min\n        self._mahalanobis = None\n\n        if sqrt_fn is None:\n            self.msqrt = cholesky\n        else:\n            self.msqrt = sqrt_fn\n\n        # weights for the means and covariances.\n        self.Wm, self.Wc = points.Wm, points.Wc\n\n        if residual_x is None:\n            self.residual_x = np.subtract\n        else:\n            self.residual_x = residual_x\n\n        if residual_z is None:\n            self.residual_z = np.subtract\n        else:\n            self.residual_z = residual_z\n\n        if state_add is None:\n            self.state_add = np.add\n        else:\n            self.state_add = state_add\n\n        # sigma points transformed through f(x) and h(x)\n        # variables for efficiency so we don't recreate every update\n\n        self.sigmas_f = zeros((self._num_sigmas, self._dim_x))\n        self.sigmas_h = zeros((self._num_sigmas, self._dim_z))\n\n        self.K = np.zeros((dim_x, dim_z))  # Kalman gain\n        self.y = np.zeros((dim_z))  # residual\n        self.z = np.array([[None] * dim_z]).T  # measurement\n        self.S = np.zeros((dim_z, dim_z))  # system uncertainty\n        self.SI = np.zeros((dim_z, dim_z))  # inverse system uncertainty\n\n        self.inv = np.linalg.inv\n\n        # these will always be a copy of x,P after predict() is called\n        self.x_prior = self.x.copy()\n        self.P_prior = self.P.copy()\n\n        # these will always be a copy of x,P after update() is called\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n\n    def predict(self, dt=None, UT=None, fx=None, **fx_args):\n        r\"\"\"\n        Performs the predict step of the UKF. On return, self.x and\n        self.P contain the predicted state (x) and covariance (P). '\n\n        Important: this MUST be called before update() is called for the first\n        time.\n\n        Parameters\n        ----------\n\n        dt : double, optional\n            If specified, the time step to be used for this prediction.\n            self._dt is used if this is not provided.\n\n        fx : callable f(x, dt, **fx_args), optional\n            State transition function. If not provided, the default\n            function passed in during construction will be used.\n\n        UT : function(sigmas, Wm, Wc, noise_cov), optional\n            Optional function to compute the unscented transform for the sigma\n            points passed through hx. Typically the default function will\n            work - you can use x_mean_fn and z_mean_fn to alter the behavior\n            of the unscented transform.\n\n        **fx_args : keyword arguments\n            optional keyword arguments to be passed into f(x).\n        \"\"\"\n\n        if dt is None:\n            dt = self._dt\n\n        if UT is None:\n            UT = unscented_transform\n\n        # calculate sigma points for given mean and covariance\n        self.compute_process_sigmas(dt, fx, **fx_args)\n\n        # and pass sigmas through the unscented transform to compute prior\n        self.x, self.P = UT(self.sigmas_f, self.Wm, self.Wc, self.Q, self.x_mean, self.residual_x)\n\n        # update sigma points to reflect the new variance of the points\n        self.sigmas_f = self.points_fn.sigma_points(self.x, self.P)\n\n        # save prior\n        self.x_prior = np.copy(self.x)\n        self.P_prior = np.copy(self.P)\n\n    def update(self, z, R=None, UT=None, hx=None, **hx_args):\n        \"\"\"\n        Update the UKF with the given measurements. On return,\n        self.x and self.P contain the new mean and covariance of the filter.\n\n        Parameters\n        ----------\n\n        z : numpy.array of shape (dim_z)\n            measurement vector\n\n        R : numpy.array((dim_z, dim_z)), optional\n            Measurement noise. If provided, overrides self.R for\n            this function call.\n\n        UT : function(sigmas, Wm, Wc, noise_cov), optional\n            Optional function to compute the unscented transform for the sigma\n            points passed through hx. Typically the default function will\n            work - you can use x_mean_fn and z_mean_fn to alter the behavior\n            of the unscented transform.\n\n        hx : callable h(x, **hx_args), optional\n            Measurement function. If not provided, the default\n            function passed in during construction will be used.\n\n        **hx_args : keyword argument\n            arguments to be passed into h(x) after x -&gt; h(x, **hx_args)\n        \"\"\"\n\n        if z is None:\n            self.z = np.array([[None] * self._dim_z]).T\n            self.x_post = self.x.copy()\n            self.P_post = self.P.copy()\n            return\n\n        if hx is None:\n            hx = self.hx\n\n        if UT is None:\n            UT = unscented_transform\n\n        if R is None:\n            R = self.R\n        elif isscalar(R):\n            R = eye(self._dim_z) * R\n\n        # pass prior sigmas through h(x) to get measurement sigmas\n        # the shape of sigmas_h will vary if the shape of z varies, so\n        # recreate each time\n        sigmas_h = []\n        for s in self.sigmas_f:\n            sigmas_h.append(hx(s, **hx_args))\n\n        self.sigmas_h = np.atleast_2d(sigmas_h)\n\n        # mean and covariance of prediction passed through unscented transform\n        zp, self.S = UT(self.sigmas_h, self.Wm, self.Wc, R, self.z_mean, self.residual_z)\n        self.SI = self.inv(self.S)\n\n        # compute cross variance of the state and the measurements\n        Pxz = self.cross_variance(self.x, zp, self.sigmas_f, self.sigmas_h)\n\n        self.K = dot(Pxz, self.SI)  # Kalman gain\n        self.y = self.residual_z(z, zp)  # residual\n\n        # update Gaussian state estimate (x, P)\n        self.x = self.state_add(self.x, dot(self.K, self.y))\n        self.P = self.P - dot(self.K, dot(self.S, self.K.T))\n\n        # save measurement and posterior state\n        self.z = deepcopy(z)\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n\n        # set to None to force recompute\n        self._log_likelihood = None\n        self._likelihood = None\n        self._mahalanobis = None\n\n    def cross_variance(self, x, z, sigmas_f, sigmas_h):\n        \"\"\"\n        Compute cross variance of the state `x` and measurement `z`.\n        \"\"\"\n\n        Pxz = zeros((sigmas_f.shape[1], sigmas_h.shape[1]))\n        N = sigmas_f.shape[0]\n        for i in range(N):\n            dx = self.residual_x(sigmas_f[i], x)\n            dz = self.residual_z(sigmas_h[i], z)\n            Pxz += self.Wc[i] * outer(dx, dz)\n        return Pxz\n\n    def compute_process_sigmas(self, dt, fx=None, **fx_args):\n        \"\"\"\n        computes the values of sigmas_f. Normally a user would not call\n        this, but it is useful if you need to call update more than once\n        between calls to predict (to update for multiple simultaneous\n        measurements), so the sigmas correctly reflect the updated state\n        x, P.\n        \"\"\"\n\n        if fx is None:\n            fx = self.fx\n\n        # calculate sigma points for given mean and covariance\n        sigmas = self.points_fn.sigma_points(self.x, self.P)\n\n        for i, s in enumerate(sigmas):\n            self.sigmas_f[i] = fx(s, dt, **fx_args)\n\n    def batch_filter(self, zs, Rs=None, dts=None, UT=None, saver=None):\n        \"\"\"\n        Performs the UKF filter over the list of measurement in `zs`.\n\n        Parameters\n        ----------\n\n        zs : list-like\n            list of measurements at each time step `self._dt` Missing\n            measurements must be represented by 'None'.\n\n        Rs : None, np.array or list-like, default=None\n            optional list of values to use for the measurement error\n            covariance R.\n\n            If Rs is None then self.R is used for all epochs.\n\n            If it is a list of matrices or a 3D array where\n            len(Rs) == len(zs), then it is treated as a list of R values, one\n            per epoch. This allows you to have varying R per epoch.\n\n        dts : None, scalar or list-like, default=None\n            optional value or list of delta time to be passed into predict.\n\n            If dtss is None then self.dt is used for all epochs.\n\n            If it is a list where len(dts) == len(zs), then it is treated as a\n            list of dt values, one per epoch. This allows you to have varying\n            epoch durations.\n\n        UT : function(sigmas, Wm, Wc, noise_cov), optional\n            Optional function to compute the unscented transform for the sigma\n            points passed through hx. Typically the default function will\n            work - you can use x_mean_fn and z_mean_fn to alter the behavior\n            of the unscented transform.\n\n        saver : filterpy.common.Saver, optional\n            filterpy.common.Saver object. If provided, saver.save() will be\n            called after every epoch\n\n        Returns\n        -------\n\n        means: ndarray((n,dim_x,1))\n            array of the state for each time step after the update. Each entry\n            is an np.array. In other words `means[k,:]` is the state at step\n            `k`.\n\n        covariance: ndarray((n,dim_x,dim_x))\n            array of the covariances for each time step after the update.\n            In other words `covariance[k,:,:]` is the covariance at step `k`.\n\n        Examples\n        --------\n\n        .. code-block:: Python\n\n            # this example demonstrates tracking a measurement where the time\n            # between measurement varies, as stored in dts The output is then smoothed\n            # with an RTS smoother.\n\n            zs = [t + random.randn()*4 for t in range (40)]\n\n            (mu, cov, _, _) = ukf.batch_filter(zs, dts=dts)\n            (xs, Ps, Ks) = ukf.rts_smoother(mu, cov)\n\n        \"\"\"\n        # pylint: disable=too-many-arguments\n\n        try:\n            z = zs[0]\n        except TypeError:\n            raise TypeError(\"zs must be list-like\")\n\n        if self._dim_z == 1:\n            if not (isscalar(z) or (z.ndim == 1 and len(z) == 1)):\n                raise TypeError(\"zs must be a list of scalars or 1D, 1 element arrays\")\n        else:\n            if len(z) != self._dim_z:\n                raise TypeError(\"each element in zs must be a 1D array of length {}\".format(self._dim_z))\n\n        z_n = len(zs)\n\n        if Rs is None:\n            Rs = [self.R] * z_n\n\n        if dts is None:\n            dts = [self._dt] * z_n\n\n        # mean estimates from Kalman Filter\n        if self.x.ndim == 1:\n            means = zeros((z_n, self._dim_x))\n        else:\n            means = zeros((z_n, self._dim_x, 1))\n\n        # state covariances from Kalman Filter\n        covariances = zeros((z_n, self._dim_x, self._dim_x))\n\n        for i, (z, r, dt) in enumerate(zip(zs, Rs, dts)):\n            self.predict(dt=dt, UT=UT)\n            self.update(z, r, UT=UT)\n            means[i, :] = self.x\n            covariances[i, :, :] = self.P\n\n            if saver is not None:\n                saver.save()\n\n        return (means, covariances)\n\n    def rts_smoother(self, Xs, Ps, Qs=None, dts=None, UT=None):\n        \"\"\"\n        Runs the Rauch-Tung-Striebel Kalman smoother on a set of\n        means and covariances computed by the UKF. The usual input\n        would come from the output of `batch_filter()`.\n\n        Parameters\n        ----------\n\n        Xs : numpy.array\n            array of the means (state variable x) of the output of a Kalman\n            filter.\n\n        Ps : numpy.array\n            array of the covariances of the output of a kalman filter.\n\n        Qs: list-like collection of numpy.array, optional\n            Process noise of the Kalman filter at each time step. Optional,\n            if not provided the filter's self.Q will be used\n\n        dt : optional, float or array-like of float\n            If provided, specifies the time step of each step of the filter.\n            If float, then the same time step is used for all steps. If\n            an array, then each element k contains the time  at step k.\n            Units are seconds.\n\n        UT : function(sigmas, Wm, Wc, noise_cov), optional\n            Optional function to compute the unscented transform for the sigma\n            points passed through hx. Typically the default function will\n            work - you can use x_mean_fn and z_mean_fn to alter the behavior\n            of the unscented transform.\n\n        Returns\n        -------\n\n        x : numpy.ndarray\n            smoothed means\n\n        P : numpy.ndarray\n            smoothed state covariances\n\n        K : numpy.ndarray\n            smoother gain at each step\n\n        Examples\n        --------\n\n        .. code-block:: Python\n\n            zs = [t + random.randn()*4 for t in range (40)]\n\n            (mu, cov, _, _) = kalman.batch_filter(zs)\n            (x, P, K) = rts_smoother(mu, cov, fk.F, fk.Q)\n        \"\"\"\n        # pylint: disable=too-many-locals, too-many-arguments\n\n        if len(Xs) != len(Ps):\n            raise ValueError(\"Xs and Ps must have the same length\")\n\n        n, dim_x = Xs.shape\n\n        if dts is None:\n            dts = [self._dt] * n\n        elif isscalar(dts):\n            dts = [dts] * n\n\n        if Qs is None:\n            Qs = [self.Q] * n\n\n        if UT is None:\n            UT = unscented_transform\n\n        # smoother gain\n        Ks = zeros((n, dim_x, dim_x))\n\n        num_sigmas = self._num_sigmas\n\n        xs, ps = Xs.copy(), Ps.copy()\n        sigmas_f = zeros((num_sigmas, dim_x))\n\n        for k in reversed(range(n - 1)):\n            # create sigma points from state estimate, pass through state func\n            sigmas = self.points_fn.sigma_points(xs[k], ps[k])\n            for i in range(num_sigmas):\n                sigmas_f[i] = self.fx(sigmas[i], dts[k])\n\n            xb, Pb = UT(sigmas_f, self.Wm, self.Wc, self.Q, self.x_mean, self.residual_x)\n\n            # compute cross variance\n            Pxb = 0\n            for i in range(num_sigmas):\n                y = self.residual_x(sigmas_f[i], xb)\n                z = self.residual_x(sigmas[i], Xs[k])\n                Pxb += self.Wc[i] * outer(z, y)\n\n            # compute gain\n            K = dot(Pxb, self.inv(Pb))\n\n            # update the smoothed estimates\n            xs[k] += dot(K, self.residual_x(xs[k + 1], xb))\n            ps[k] += dot(K, ps[k + 1] - Pb).dot(K.T)\n            Ks[k] = K\n\n        return (xs, ps, Ks)\n\n    @property\n    def log_likelihood(self):\n        \"\"\"\n        log-likelihood of the last measurement.\n        \"\"\"\n        if self._log_likelihood is None:\n            self._log_likelihood = logpdf(x=self.y, cov=self.S)\n        return self._log_likelihood\n\n    @property\n    def likelihood(self):\n        \"\"\"\n        Computed from the log-likelihood. The log-likelihood can be very\n        small,  meaning a large negative value such as -28000. Taking the\n        exp() of that results in 0.0, which can break typical algorithms\n        which multiply by this value, so by default we always return a\n        number &gt;= sys.float_info.min.\n        \"\"\"\n        if self._likelihood is None:\n            self._likelihood = exp(self.log_likelihood)\n            if self._likelihood == 0:\n                self._likelihood = sys.float_info.min\n        return self._likelihood\n\n    @property\n    def mahalanobis(self):\n        \"\"\" \"\n        Mahalanobis distance of measurement. E.g. 3 means measurement\n        was 3 standard deviations away from the predicted value.\n\n        Returns\n        -------\n        mahalanobis : float\n        \"\"\"\n        if self._mahalanobis is None:\n            self._mahalanobis = sqrt(float(dot(dot(self.y.T, self.SI), self.y)))\n        return self._mahalanobis\n\n    def __repr__(self):\n        return \"\\n\".join(\n            [\n                \"UnscentedKalmanFilter object\",\n                pretty_str(\"x\", self.x),\n                pretty_str(\"P\", self.P),\n                pretty_str(\"x_prior\", self.x_prior),\n                pretty_str(\"P_prior\", self.P_prior),\n                pretty_str(\"Q\", self.Q),\n                pretty_str(\"R\", self.R),\n                pretty_str(\"S\", self.S),\n                pretty_str(\"K\", self.K),\n                pretty_str(\"y\", self.y),\n                pretty_str(\"log-likelihood\", self.log_likelihood),\n                pretty_str(\"likelihood\", self.likelihood),\n                pretty_str(\"mahalanobis\", self.mahalanobis),\n                pretty_str(\"sigmas_f\", self.sigmas_f),\n                pretty_str(\"h\", self.sigmas_h),\n                pretty_str(\"Wm\", self.Wm),\n                pretty_str(\"Wc\", self.Wc),\n                pretty_str(\"residual_x\", self.residual_x),\n                pretty_str(\"residual_z\", self.residual_z),\n                pretty_str(\"msqrt\", self.msqrt),\n                pretty_str(\"hx\", self.hx),\n                pretty_str(\"fx\", self.fx),\n                pretty_str(\"x_mean\", self.x_mean),\n                pretty_str(\"z_mean\", self.z_mean),\n            ]\n        )\n</code></pre>"},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.UKF.UnscentedKalmanFilter.likelihood","title":"<code>likelihood</code>  <code>property</code>","text":"<p>Computed from the log-likelihood. The log-likelihood can be very small,  meaning a large negative value such as -28000. Taking the exp() of that results in 0.0, which can break typical algorithms which multiply by this value, so by default we always return a number &gt;= sys.float_info.min.</p>"},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.UKF.UnscentedKalmanFilter.log_likelihood","title":"<code>log_likelihood</code>  <code>property</code>","text":"<p>log-likelihood of the last measurement.</p>"},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.UKF.UnscentedKalmanFilter.mahalanobis","title":"<code>mahalanobis</code>  <code>property</code>","text":"<p>\" Mahalanobis distance of measurement. E.g. 3 means measurement was 3 standard deviations away from the predicted value.</p> <p>Returns:</p> Name Type Description <code>mahalanobis</code> <code>float</code>"},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.UKF.UnscentedKalmanFilter.__init__","title":"<code>__init__(dim_x, dim_z, dt, hx, fx, points, sqrt_fn=None, x_mean_fn=None, z_mean_fn=None, residual_x=None, residual_z=None, state_add=None)</code>","text":"<p>Create a Kalman filter. You are responsible for setting the various state variables to reasonable values; the defaults below will not give you a functional filter.</p> Source code in <code>bayesian_filters/kalman/UKF.py</code> <pre><code>def __init__(\n    self,\n    dim_x,\n    dim_z,\n    dt,\n    hx,\n    fx,\n    points,\n    sqrt_fn=None,\n    x_mean_fn=None,\n    z_mean_fn=None,\n    residual_x=None,\n    residual_z=None,\n    state_add=None,\n):\n    \"\"\"\n    Create a Kalman filter. You are responsible for setting the\n    various state variables to reasonable values; the defaults below will\n    not give you a functional filter.\n\n    \"\"\"\n\n    # pylint: disable=too-many-arguments\n\n    self.x = zeros(dim_x)\n    self.P = eye(dim_x)\n    self.x_prior = np.copy(self.x)\n    self.P_prior = np.copy(self.P)\n    self.Q = eye(dim_x)\n    self.R = eye(dim_z)\n    self._dim_x = dim_x\n    self._dim_z = dim_z\n    self.points_fn = points\n    self._dt = dt\n    self._num_sigmas = points.num_sigmas()\n    self.hx = hx\n    self.fx = fx\n    self.x_mean = x_mean_fn\n    self.z_mean = z_mean_fn\n\n    # Only computed only if requested via property\n    self._log_likelihood = log(sys.float_info.min)\n    self._likelihood = sys.float_info.min\n    self._mahalanobis = None\n\n    if sqrt_fn is None:\n        self.msqrt = cholesky\n    else:\n        self.msqrt = sqrt_fn\n\n    # weights for the means and covariances.\n    self.Wm, self.Wc = points.Wm, points.Wc\n\n    if residual_x is None:\n        self.residual_x = np.subtract\n    else:\n        self.residual_x = residual_x\n\n    if residual_z is None:\n        self.residual_z = np.subtract\n    else:\n        self.residual_z = residual_z\n\n    if state_add is None:\n        self.state_add = np.add\n    else:\n        self.state_add = state_add\n\n    # sigma points transformed through f(x) and h(x)\n    # variables for efficiency so we don't recreate every update\n\n    self.sigmas_f = zeros((self._num_sigmas, self._dim_x))\n    self.sigmas_h = zeros((self._num_sigmas, self._dim_z))\n\n    self.K = np.zeros((dim_x, dim_z))  # Kalman gain\n    self.y = np.zeros((dim_z))  # residual\n    self.z = np.array([[None] * dim_z]).T  # measurement\n    self.S = np.zeros((dim_z, dim_z))  # system uncertainty\n    self.SI = np.zeros((dim_z, dim_z))  # inverse system uncertainty\n\n    self.inv = np.linalg.inv\n\n    # these will always be a copy of x,P after predict() is called\n    self.x_prior = self.x.copy()\n    self.P_prior = self.P.copy()\n\n    # these will always be a copy of x,P after update() is called\n    self.x_post = self.x.copy()\n    self.P_post = self.P.copy()\n</code></pre>"},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.UKF.UnscentedKalmanFilter.batch_filter","title":"<code>batch_filter(zs, Rs=None, dts=None, UT=None, saver=None)</code>","text":"<p>Performs the UKF filter over the list of measurement in <code>zs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>zs</code> <code>list - like</code> <p>list of measurements at each time step <code>self._dt</code> Missing measurements must be represented by 'None'.</p> required <code>Rs</code> <code>(None, array or list - like)</code> <p>optional list of values to use for the measurement error covariance R.</p> <p>If Rs is None then self.R is used for all epochs.</p> <p>If it is a list of matrices or a 3D array where len(Rs) == len(zs), then it is treated as a list of R values, one per epoch. This allows you to have varying R per epoch.</p> <code>None</code> <code>dts</code> <code>(None, scalar or list - like)</code> <p>optional value or list of delta time to be passed into predict.</p> <p>If dtss is None then self.dt is used for all epochs.</p> <p>If it is a list where len(dts) == len(zs), then it is treated as a list of dt values, one per epoch. This allows you to have varying epoch durations.</p> <code>None</code> <code>UT</code> <code>function(sigmas, Wm, Wc, noise_cov)</code> <p>Optional function to compute the unscented transform for the sigma points passed through hx. Typically the default function will work - you can use x_mean_fn and z_mean_fn to alter the behavior of the unscented transform.</p> <code>None</code> <code>saver</code> <code>Saver</code> <p>filterpy.common.Saver object. If provided, saver.save() will be called after every epoch</p> <code>None</code> <p>Returns:</p> Name Type Description <code>means</code> <code>ndarray((n, dim_x, 1))</code> <p>array of the state for each time step after the update. Each entry is an np.array. In other words <code>means[k,:]</code> is the state at step <code>k</code>.</p> <code>covariance</code> <code>ndarray((n, dim_x, dim_x))</code> <p>array of the covariances for each time step after the update. In other words <code>covariance[k,:,:]</code> is the covariance at step <code>k</code>.</p> <p>Examples:</p> <p>.. code-block:: Python</p> <pre><code># this example demonstrates tracking a measurement where the time\n# between measurement varies, as stored in dts The output is then smoothed\n# with an RTS smoother.\n\nzs = [t + random.randn()*4 for t in range (40)]\n\n(mu, cov, _, _) = ukf.batch_filter(zs, dts=dts)\n(xs, Ps, Ks) = ukf.rts_smoother(mu, cov)\n</code></pre> Source code in <code>bayesian_filters/kalman/UKF.py</code> <pre><code>def batch_filter(self, zs, Rs=None, dts=None, UT=None, saver=None):\n    \"\"\"\n    Performs the UKF filter over the list of measurement in `zs`.\n\n    Parameters\n    ----------\n\n    zs : list-like\n        list of measurements at each time step `self._dt` Missing\n        measurements must be represented by 'None'.\n\n    Rs : None, np.array or list-like, default=None\n        optional list of values to use for the measurement error\n        covariance R.\n\n        If Rs is None then self.R is used for all epochs.\n\n        If it is a list of matrices or a 3D array where\n        len(Rs) == len(zs), then it is treated as a list of R values, one\n        per epoch. This allows you to have varying R per epoch.\n\n    dts : None, scalar or list-like, default=None\n        optional value or list of delta time to be passed into predict.\n\n        If dtss is None then self.dt is used for all epochs.\n\n        If it is a list where len(dts) == len(zs), then it is treated as a\n        list of dt values, one per epoch. This allows you to have varying\n        epoch durations.\n\n    UT : function(sigmas, Wm, Wc, noise_cov), optional\n        Optional function to compute the unscented transform for the sigma\n        points passed through hx. Typically the default function will\n        work - you can use x_mean_fn and z_mean_fn to alter the behavior\n        of the unscented transform.\n\n    saver : filterpy.common.Saver, optional\n        filterpy.common.Saver object. If provided, saver.save() will be\n        called after every epoch\n\n    Returns\n    -------\n\n    means: ndarray((n,dim_x,1))\n        array of the state for each time step after the update. Each entry\n        is an np.array. In other words `means[k,:]` is the state at step\n        `k`.\n\n    covariance: ndarray((n,dim_x,dim_x))\n        array of the covariances for each time step after the update.\n        In other words `covariance[k,:,:]` is the covariance at step `k`.\n\n    Examples\n    --------\n\n    .. code-block:: Python\n\n        # this example demonstrates tracking a measurement where the time\n        # between measurement varies, as stored in dts The output is then smoothed\n        # with an RTS smoother.\n\n        zs = [t + random.randn()*4 for t in range (40)]\n\n        (mu, cov, _, _) = ukf.batch_filter(zs, dts=dts)\n        (xs, Ps, Ks) = ukf.rts_smoother(mu, cov)\n\n    \"\"\"\n    # pylint: disable=too-many-arguments\n\n    try:\n        z = zs[0]\n    except TypeError:\n        raise TypeError(\"zs must be list-like\")\n\n    if self._dim_z == 1:\n        if not (isscalar(z) or (z.ndim == 1 and len(z) == 1)):\n            raise TypeError(\"zs must be a list of scalars or 1D, 1 element arrays\")\n    else:\n        if len(z) != self._dim_z:\n            raise TypeError(\"each element in zs must be a 1D array of length {}\".format(self._dim_z))\n\n    z_n = len(zs)\n\n    if Rs is None:\n        Rs = [self.R] * z_n\n\n    if dts is None:\n        dts = [self._dt] * z_n\n\n    # mean estimates from Kalman Filter\n    if self.x.ndim == 1:\n        means = zeros((z_n, self._dim_x))\n    else:\n        means = zeros((z_n, self._dim_x, 1))\n\n    # state covariances from Kalman Filter\n    covariances = zeros((z_n, self._dim_x, self._dim_x))\n\n    for i, (z, r, dt) in enumerate(zip(zs, Rs, dts)):\n        self.predict(dt=dt, UT=UT)\n        self.update(z, r, UT=UT)\n        means[i, :] = self.x\n        covariances[i, :, :] = self.P\n\n        if saver is not None:\n            saver.save()\n\n    return (means, covariances)\n</code></pre>"},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.UKF.UnscentedKalmanFilter.compute_process_sigmas","title":"<code>compute_process_sigmas(dt, fx=None, **fx_args)</code>","text":"<p>computes the values of sigmas_f. Normally a user would not call this, but it is useful if you need to call update more than once between calls to predict (to update for multiple simultaneous measurements), so the sigmas correctly reflect the updated state x, P.</p> Source code in <code>bayesian_filters/kalman/UKF.py</code> <pre><code>def compute_process_sigmas(self, dt, fx=None, **fx_args):\n    \"\"\"\n    computes the values of sigmas_f. Normally a user would not call\n    this, but it is useful if you need to call update more than once\n    between calls to predict (to update for multiple simultaneous\n    measurements), so the sigmas correctly reflect the updated state\n    x, P.\n    \"\"\"\n\n    if fx is None:\n        fx = self.fx\n\n    # calculate sigma points for given mean and covariance\n    sigmas = self.points_fn.sigma_points(self.x, self.P)\n\n    for i, s in enumerate(sigmas):\n        self.sigmas_f[i] = fx(s, dt, **fx_args)\n</code></pre>"},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.UKF.UnscentedKalmanFilter.cross_variance","title":"<code>cross_variance(x, z, sigmas_f, sigmas_h)</code>","text":"<p>Compute cross variance of the state <code>x</code> and measurement <code>z</code>.</p> Source code in <code>bayesian_filters/kalman/UKF.py</code> <pre><code>def cross_variance(self, x, z, sigmas_f, sigmas_h):\n    \"\"\"\n    Compute cross variance of the state `x` and measurement `z`.\n    \"\"\"\n\n    Pxz = zeros((sigmas_f.shape[1], sigmas_h.shape[1]))\n    N = sigmas_f.shape[0]\n    for i in range(N):\n        dx = self.residual_x(sigmas_f[i], x)\n        dz = self.residual_z(sigmas_h[i], z)\n        Pxz += self.Wc[i] * outer(dx, dz)\n    return Pxz\n</code></pre>"},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.UKF.UnscentedKalmanFilter.predict","title":"<code>predict(dt=None, UT=None, fx=None, **fx_args)</code>","text":"<p>Performs the predict step of the UKF. On return, self.x and self.P contain the predicted state (x) and covariance (P). '</p> <p>Important: this MUST be called before update() is called for the first time.</p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <code>double</code> <p>If specified, the time step to be used for this prediction. self._dt is used if this is not provided.</p> <code>None</code> <code>fx</code> <code>callable f(x, dt, **fx_args)</code> <p>State transition function. If not provided, the default function passed in during construction will be used.</p> <code>None</code> <code>UT</code> <code>function(sigmas, Wm, Wc, noise_cov)</code> <p>Optional function to compute the unscented transform for the sigma points passed through hx. Typically the default function will work - you can use x_mean_fn and z_mean_fn to alter the behavior of the unscented transform.</p> <code>None</code> <code>**fx_args</code> <code>keyword arguments</code> <p>optional keyword arguments to be passed into f(x).</p> <code>{}</code> Source code in <code>bayesian_filters/kalman/UKF.py</code> <pre><code>def predict(self, dt=None, UT=None, fx=None, **fx_args):\n    r\"\"\"\n    Performs the predict step of the UKF. On return, self.x and\n    self.P contain the predicted state (x) and covariance (P). '\n\n    Important: this MUST be called before update() is called for the first\n    time.\n\n    Parameters\n    ----------\n\n    dt : double, optional\n        If specified, the time step to be used for this prediction.\n        self._dt is used if this is not provided.\n\n    fx : callable f(x, dt, **fx_args), optional\n        State transition function. If not provided, the default\n        function passed in during construction will be used.\n\n    UT : function(sigmas, Wm, Wc, noise_cov), optional\n        Optional function to compute the unscented transform for the sigma\n        points passed through hx. Typically the default function will\n        work - you can use x_mean_fn and z_mean_fn to alter the behavior\n        of the unscented transform.\n\n    **fx_args : keyword arguments\n        optional keyword arguments to be passed into f(x).\n    \"\"\"\n\n    if dt is None:\n        dt = self._dt\n\n    if UT is None:\n        UT = unscented_transform\n\n    # calculate sigma points for given mean and covariance\n    self.compute_process_sigmas(dt, fx, **fx_args)\n\n    # and pass sigmas through the unscented transform to compute prior\n    self.x, self.P = UT(self.sigmas_f, self.Wm, self.Wc, self.Q, self.x_mean, self.residual_x)\n\n    # update sigma points to reflect the new variance of the points\n    self.sigmas_f = self.points_fn.sigma_points(self.x, self.P)\n\n    # save prior\n    self.x_prior = np.copy(self.x)\n    self.P_prior = np.copy(self.P)\n</code></pre>"},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.UKF.UnscentedKalmanFilter.rts_smoother","title":"<code>rts_smoother(Xs, Ps, Qs=None, dts=None, UT=None)</code>","text":"<p>Runs the Rauch-Tung-Striebel Kalman smoother on a set of means and covariances computed by the UKF. The usual input would come from the output of <code>batch_filter()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>Xs</code> <code>array</code> <p>array of the means (state variable x) of the output of a Kalman filter.</p> required <code>Ps</code> <code>array</code> <p>array of the covariances of the output of a kalman filter.</p> required <code>Qs</code> <p>Process noise of the Kalman filter at each time step. Optional, if not provided the filter's self.Q will be used</p> <code>None</code> <code>dt</code> <code>optional, float or array-like of float</code> <p>If provided, specifies the time step of each step of the filter. If float, then the same time step is used for all steps. If an array, then each element k contains the time  at step k. Units are seconds.</p> required <code>UT</code> <code>function(sigmas, Wm, Wc, noise_cov)</code> <p>Optional function to compute the unscented transform for the sigma points passed through hx. Typically the default function will work - you can use x_mean_fn and z_mean_fn to alter the behavior of the unscented transform.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>smoothed means</p> <code>P</code> <code>ndarray</code> <p>smoothed state covariances</p> <code>K</code> <code>ndarray</code> <p>smoother gain at each step</p> <p>Examples:</p> <p>.. code-block:: Python</p> <pre><code>zs = [t + random.randn()*4 for t in range (40)]\n\n(mu, cov, _, _) = kalman.batch_filter(zs)\n(x, P, K) = rts_smoother(mu, cov, fk.F, fk.Q)\n</code></pre> Source code in <code>bayesian_filters/kalman/UKF.py</code> <pre><code>def rts_smoother(self, Xs, Ps, Qs=None, dts=None, UT=None):\n    \"\"\"\n    Runs the Rauch-Tung-Striebel Kalman smoother on a set of\n    means and covariances computed by the UKF. The usual input\n    would come from the output of `batch_filter()`.\n\n    Parameters\n    ----------\n\n    Xs : numpy.array\n        array of the means (state variable x) of the output of a Kalman\n        filter.\n\n    Ps : numpy.array\n        array of the covariances of the output of a kalman filter.\n\n    Qs: list-like collection of numpy.array, optional\n        Process noise of the Kalman filter at each time step. Optional,\n        if not provided the filter's self.Q will be used\n\n    dt : optional, float or array-like of float\n        If provided, specifies the time step of each step of the filter.\n        If float, then the same time step is used for all steps. If\n        an array, then each element k contains the time  at step k.\n        Units are seconds.\n\n    UT : function(sigmas, Wm, Wc, noise_cov), optional\n        Optional function to compute the unscented transform for the sigma\n        points passed through hx. Typically the default function will\n        work - you can use x_mean_fn and z_mean_fn to alter the behavior\n        of the unscented transform.\n\n    Returns\n    -------\n\n    x : numpy.ndarray\n        smoothed means\n\n    P : numpy.ndarray\n        smoothed state covariances\n\n    K : numpy.ndarray\n        smoother gain at each step\n\n    Examples\n    --------\n\n    .. code-block:: Python\n\n        zs = [t + random.randn()*4 for t in range (40)]\n\n        (mu, cov, _, _) = kalman.batch_filter(zs)\n        (x, P, K) = rts_smoother(mu, cov, fk.F, fk.Q)\n    \"\"\"\n    # pylint: disable=too-many-locals, too-many-arguments\n\n    if len(Xs) != len(Ps):\n        raise ValueError(\"Xs and Ps must have the same length\")\n\n    n, dim_x = Xs.shape\n\n    if dts is None:\n        dts = [self._dt] * n\n    elif isscalar(dts):\n        dts = [dts] * n\n\n    if Qs is None:\n        Qs = [self.Q] * n\n\n    if UT is None:\n        UT = unscented_transform\n\n    # smoother gain\n    Ks = zeros((n, dim_x, dim_x))\n\n    num_sigmas = self._num_sigmas\n\n    xs, ps = Xs.copy(), Ps.copy()\n    sigmas_f = zeros((num_sigmas, dim_x))\n\n    for k in reversed(range(n - 1)):\n        # create sigma points from state estimate, pass through state func\n        sigmas = self.points_fn.sigma_points(xs[k], ps[k])\n        for i in range(num_sigmas):\n            sigmas_f[i] = self.fx(sigmas[i], dts[k])\n\n        xb, Pb = UT(sigmas_f, self.Wm, self.Wc, self.Q, self.x_mean, self.residual_x)\n\n        # compute cross variance\n        Pxb = 0\n        for i in range(num_sigmas):\n            y = self.residual_x(sigmas_f[i], xb)\n            z = self.residual_x(sigmas[i], Xs[k])\n            Pxb += self.Wc[i] * outer(z, y)\n\n        # compute gain\n        K = dot(Pxb, self.inv(Pb))\n\n        # update the smoothed estimates\n        xs[k] += dot(K, self.residual_x(xs[k + 1], xb))\n        ps[k] += dot(K, ps[k + 1] - Pb).dot(K.T)\n        Ks[k] = K\n\n    return (xs, ps, Ks)\n</code></pre>"},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.UKF.UnscentedKalmanFilter.update","title":"<code>update(z, R=None, UT=None, hx=None, **hx_args)</code>","text":"<p>Update the UKF with the given measurements. On return, self.x and self.P contain the new mean and covariance of the filter.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>numpy.array of shape (dim_z)</code> <p>measurement vector</p> required <code>R</code> <code>array((dim_z, dim_z))</code> <p>Measurement noise. If provided, overrides self.R for this function call.</p> <code>None</code> <code>UT</code> <code>function(sigmas, Wm, Wc, noise_cov)</code> <p>Optional function to compute the unscented transform for the sigma points passed through hx. Typically the default function will work - you can use x_mean_fn and z_mean_fn to alter the behavior of the unscented transform.</p> <code>None</code> <code>hx</code> <code>callable h(x, **hx_args)</code> <p>Measurement function. If not provided, the default function passed in during construction will be used.</p> <code>None</code> <code>**hx_args</code> <code>keyword argument</code> <p>arguments to be passed into h(x) after x -&gt; h(x, **hx_args)</p> <code>{}</code> Source code in <code>bayesian_filters/kalman/UKF.py</code> <pre><code>def update(self, z, R=None, UT=None, hx=None, **hx_args):\n    \"\"\"\n    Update the UKF with the given measurements. On return,\n    self.x and self.P contain the new mean and covariance of the filter.\n\n    Parameters\n    ----------\n\n    z : numpy.array of shape (dim_z)\n        measurement vector\n\n    R : numpy.array((dim_z, dim_z)), optional\n        Measurement noise. If provided, overrides self.R for\n        this function call.\n\n    UT : function(sigmas, Wm, Wc, noise_cov), optional\n        Optional function to compute the unscented transform for the sigma\n        points passed through hx. Typically the default function will\n        work - you can use x_mean_fn and z_mean_fn to alter the behavior\n        of the unscented transform.\n\n    hx : callable h(x, **hx_args), optional\n        Measurement function. If not provided, the default\n        function passed in during construction will be used.\n\n    **hx_args : keyword argument\n        arguments to be passed into h(x) after x -&gt; h(x, **hx_args)\n    \"\"\"\n\n    if z is None:\n        self.z = np.array([[None] * self._dim_z]).T\n        self.x_post = self.x.copy()\n        self.P_post = self.P.copy()\n        return\n\n    if hx is None:\n        hx = self.hx\n\n    if UT is None:\n        UT = unscented_transform\n\n    if R is None:\n        R = self.R\n    elif isscalar(R):\n        R = eye(self._dim_z) * R\n\n    # pass prior sigmas through h(x) to get measurement sigmas\n    # the shape of sigmas_h will vary if the shape of z varies, so\n    # recreate each time\n    sigmas_h = []\n    for s in self.sigmas_f:\n        sigmas_h.append(hx(s, **hx_args))\n\n    self.sigmas_h = np.atleast_2d(sigmas_h)\n\n    # mean and covariance of prediction passed through unscented transform\n    zp, self.S = UT(self.sigmas_h, self.Wm, self.Wc, R, self.z_mean, self.residual_z)\n    self.SI = self.inv(self.S)\n\n    # compute cross variance of the state and the measurements\n    Pxz = self.cross_variance(self.x, zp, self.sigmas_f, self.sigmas_h)\n\n    self.K = dot(Pxz, self.SI)  # Kalman gain\n    self.y = self.residual_z(z, zp)  # residual\n\n    # update Gaussian state estimate (x, P)\n    self.x = self.state_add(self.x, dot(self.K, self.y))\n    self.P = self.P - dot(self.K, dot(self.S, self.K.T))\n\n    # save measurement and posterior state\n    self.z = deepcopy(z)\n    self.x_post = self.x.copy()\n    self.P_post = self.P.copy()\n\n    # set to None to force recompute\n    self._log_likelihood = None\n    self._likelihood = None\n    self._mahalanobis = None\n</code></pre>"},{"location":"filters/unscented-kalman-filter/#merwe-scaled-sigma-points","title":"Merwe Scaled Sigma Points","text":""},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.sigma_points.MerweScaledSigmaPoints","title":"<code>MerweScaledSigmaPoints</code>","text":"<p>               Bases: <code>object</code></p> <p>Generates sigma points and weights according to Van der Merwe's 2004 dissertation[1] for the UnscentedKalmanFilter class.. It parametizes the sigma points using alpha, beta, kappa terms, and is the version seen in most publications.</p> <p>Unless you know better, this should be your default choice.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Dimensionality of the state. 2n+1 weights will be generated.</p> required <code>alpha</code> <code>float</code> <p>Determins the spread of the sigma points around the mean. Usually a small positive value (1e-3) according to [3].</p> required <code>beta</code> <code>float</code> <p>Incorporates prior knowledge of the distribution of the mean. For Gaussian x beta=2 is optimal, according to [3].</p> required <code>sqrt_method</code> <code>function(ndarray)</code> <p>Defines how we compute the square root of a matrix, which has no unique answer. Cholesky is the default choice due to its speed. Typically your alternative choice will be scipy.linalg.sqrtm. Different choices affect how the sigma points are arranged relative to the eigenvectors of the covariance matrix. Usually this will not matter to you; if so the default cholesky() yields maximal performance. As of van der Merwe's dissertation of 2004 [6] this was not a well reseached area so I have no advice to give you.</p> <p>If your method returns a triangular matrix it must be upper triangular. Do not use numpy.linalg.cholesky - for historical reasons it returns a lower triangular matrix. The SciPy version does the right thing.</p> <code>scipy.linalg.cholesky</code> <code>subtract</code> <code>callable(x, y)</code> <p>Function that computes the difference between x and y. You will have to supply this if your state variable cannot support subtraction, such as angles (359-1 degreees is 2, not 358). x and y are state vectors, not scalars.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>Wm</code> <code>array</code> <p>weight for each sigma point for the mean</p> <code>Wc</code> <code>array</code> <p>weight for each sigma point for the covariance</p> <p>Examples:</p> <p>See my book Kalman and Bayesian Filters in Python https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</p> References <p>.. [1] R. Van der Merwe \"Sigma-Point Kalman Filters for Probabilitic        Inference in Dynamic State-Space Models\" (Doctoral dissertation)</p> Source code in <code>bayesian_filters/kalman/sigma_points.py</code> <pre><code>class MerweScaledSigmaPoints(object):\n    \"\"\"\n    Generates sigma points and weights according to Van der Merwe's\n    2004 dissertation[1] for the UnscentedKalmanFilter class.. It\n    parametizes the sigma points using alpha, beta, kappa terms, and\n    is the version seen in most publications.\n\n    Unless you know better, this should be your default choice.\n\n    Parameters\n    ----------\n\n    n : int\n        Dimensionality of the state. 2n+1 weights will be generated.\n\n    alpha : float\n        Determins the spread of the sigma points around the mean.\n        Usually a small positive value (1e-3) according to [3].\n\n    beta : float\n        Incorporates prior knowledge of the distribution of the mean. For\n        Gaussian x beta=2 is optimal, according to [3].\n\n\n    sqrt_method : function(ndarray), default=scipy.linalg.cholesky\n        Defines how we compute the square root of a matrix, which has\n        no unique answer. Cholesky is the default choice due to its\n        speed. Typically your alternative choice will be\n        scipy.linalg.sqrtm. Different choices affect how the sigma points\n        are arranged relative to the eigenvectors of the covariance matrix.\n        Usually this will not matter to you; if so the default cholesky()\n        yields maximal performance. As of van der Merwe's dissertation of\n        2004 [6] this was not a well reseached area so I have no advice\n        to give you.\n\n        If your method returns a triangular matrix it must be upper\n        triangular. Do not use numpy.linalg.cholesky - for historical\n        reasons it returns a lower triangular matrix. The SciPy version\n        does the right thing.\n\n    subtract : callable (x, y), optional\n        Function that computes the difference between x and y.\n        You will have to supply this if your state variable cannot support\n        subtraction, such as angles (359-1 degreees is 2, not 358). x and y\n        are state vectors, not scalars.\n\n    Attributes\n    ----------\n\n    Wm : np.array\n        weight for each sigma point for the mean\n\n    Wc : np.array\n        weight for each sigma point for the covariance\n\n    Examples\n    --------\n\n    See my book Kalman and Bayesian Filters in Python\n    https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n\n\n    References\n    ----------\n\n    .. [1] R. Van der Merwe \"Sigma-Point Kalman Filters for Probabilitic\n           Inference in Dynamic State-Space Models\" (Doctoral dissertation)\n\n    \"\"\"\n\n    def __init__(self, n, alpha, beta, kappa, sqrt_method=None, subtract=None):\n        # pylint: disable=too-many-arguments\n\n        self.n = n\n        self.alpha = alpha\n        self.beta = beta\n        self.kappa = kappa\n        if sqrt_method is None:\n            self.sqrt = cholesky\n        else:\n            self.sqrt = sqrt_method\n\n        if subtract is None:\n            self.subtract = np.subtract\n        else:\n            self.subtract = subtract\n\n        self._compute_weights()\n\n    def num_sigmas(self):\n        \"\"\"Number of sigma points for each variable in the state x\"\"\"\n        return 2 * self.n + 1\n\n    def sigma_points(self, x, P):\n        \"\"\"Computes the sigma points for an unscented Kalman filter\n        given the mean (x) and covariance(P) of the filter.\n        Returns tuple of the sigma points and weights.\n\n        Works with both scalar and array inputs:\n        sigma_points (5, 9, 2) # mean 5, covariance 9\n        sigma_points ([5, 2], 9*eye(2), 2) # means 5 and 2, covariance 9I\n\n        Parameters\n        ----------\n\n        x : An array-like object of the means of length n\n            Can be a scalar if 1D.\n            examples: 1, [1,2], np.array([1,2])\n\n        P : scalar, or np.array\n           Covariance of the filter. If scalar, is treated as eye(n)*P.\n\n        Returns\n        -------\n\n        sigmas : np.array, of size (2n+1, n)\n            Two dimensional array of sigma points. Each column contains all of\n            the sigmas for one dimension in the problem space.\n\n            Ordered by Xi_0, Xi_{1..n}, Xi_{n+1..2n}\n        \"\"\"\n\n        if self.n != np.size(x):\n            raise ValueError(\"expected size(x) {}, but size is {}\".format(self.n, np.size(x)))\n\n        n = self.n\n\n        if np.isscalar(x):\n            x = np.asarray([x])\n\n        if np.isscalar(P):\n            P = np.eye(n) * P\n        else:\n            P = np.atleast_2d(P)\n\n        lambda_ = self.alpha**2 * (n + self.kappa) - n\n        U = self.sqrt((lambda_ + n) * P)\n\n        sigmas = np.zeros((2 * n + 1, n))\n        sigmas[0] = x\n        for k in range(n):\n            # pylint: disable=bad-whitespace\n            sigmas[k + 1] = self.subtract(x, -U[k])\n            sigmas[n + k + 1] = self.subtract(x, U[k])\n\n        return sigmas\n\n    def _compute_weights(self):\n        \"\"\"Computes the weights for the scaled unscented Kalman filter.\"\"\"\n\n        n = self.n\n        lambda_ = self.alpha**2 * (n + self.kappa) - n\n\n        c = 0.5 / (n + lambda_)\n        self.Wc = np.full(2 * n + 1, c)\n        self.Wm = np.full(2 * n + 1, c)\n        self.Wc[0] = lambda_ / (n + lambda_) + (1 - self.alpha**2 + self.beta)\n        self.Wm[0] = lambda_ / (n + lambda_)\n\n    def __repr__(self):\n        return \"\\n\".join(\n            [\n                \"MerweScaledSigmaPoints object\",\n                pretty_str(\"n\", self.n),\n                pretty_str(\"alpha\", self.alpha),\n                pretty_str(\"beta\", self.beta),\n                pretty_str(\"kappa\", self.kappa),\n                pretty_str(\"Wm\", self.Wm),\n                pretty_str(\"Wc\", self.Wc),\n                pretty_str(\"subtract\", self.subtract),\n                pretty_str(\"sqrt\", self.sqrt),\n            ]\n        )\n</code></pre>"},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.sigma_points.MerweScaledSigmaPoints.num_sigmas","title":"<code>num_sigmas()</code>","text":"<p>Number of sigma points for each variable in the state x</p> Source code in <code>bayesian_filters/kalman/sigma_points.py</code> <pre><code>def num_sigmas(self):\n    \"\"\"Number of sigma points for each variable in the state x\"\"\"\n    return 2 * self.n + 1\n</code></pre>"},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.sigma_points.MerweScaledSigmaPoints.sigma_points","title":"<code>sigma_points(x, P)</code>","text":"<p>Computes the sigma points for an unscented Kalman filter given the mean (x) and covariance(P) of the filter. Returns tuple of the sigma points and weights.</p> <p>Works with both scalar and array inputs: sigma_points (5, 9, 2) # mean 5, covariance 9 sigma_points ([5, 2], 9*eye(2), 2) # means 5 and 2, covariance 9I</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>An array-like object of the means of length n</code> <p>Can be a scalar if 1D. examples: 1, [1,2], np.array([1,2])</p> required <code>P</code> <code>scalar, or np.array</code> <p>Covariance of the filter. If scalar, is treated as eye(n)*P.</p> required <p>Returns:</p> Name Type Description <code>sigmas</code> <code>np.array, of size (2n+1, n)</code> <p>Two dimensional array of sigma points. Each column contains all of the sigmas for one dimension in the problem space.</p> <p>Ordered by Xi_0, Xi_{1..n}, Xi_{n+1..2n}</p> Source code in <code>bayesian_filters/kalman/sigma_points.py</code> <pre><code>def sigma_points(self, x, P):\n    \"\"\"Computes the sigma points for an unscented Kalman filter\n    given the mean (x) and covariance(P) of the filter.\n    Returns tuple of the sigma points and weights.\n\n    Works with both scalar and array inputs:\n    sigma_points (5, 9, 2) # mean 5, covariance 9\n    sigma_points ([5, 2], 9*eye(2), 2) # means 5 and 2, covariance 9I\n\n    Parameters\n    ----------\n\n    x : An array-like object of the means of length n\n        Can be a scalar if 1D.\n        examples: 1, [1,2], np.array([1,2])\n\n    P : scalar, or np.array\n       Covariance of the filter. If scalar, is treated as eye(n)*P.\n\n    Returns\n    -------\n\n    sigmas : np.array, of size (2n+1, n)\n        Two dimensional array of sigma points. Each column contains all of\n        the sigmas for one dimension in the problem space.\n\n        Ordered by Xi_0, Xi_{1..n}, Xi_{n+1..2n}\n    \"\"\"\n\n    if self.n != np.size(x):\n        raise ValueError(\"expected size(x) {}, but size is {}\".format(self.n, np.size(x)))\n\n    n = self.n\n\n    if np.isscalar(x):\n        x = np.asarray([x])\n\n    if np.isscalar(P):\n        P = np.eye(n) * P\n    else:\n        P = np.atleast_2d(P)\n\n    lambda_ = self.alpha**2 * (n + self.kappa) - n\n    U = self.sqrt((lambda_ + n) * P)\n\n    sigmas = np.zeros((2 * n + 1, n))\n    sigmas[0] = x\n    for k in range(n):\n        # pylint: disable=bad-whitespace\n        sigmas[k + 1] = self.subtract(x, -U[k])\n        sigmas[n + k + 1] = self.subtract(x, U[k])\n\n    return sigmas\n</code></pre>"},{"location":"filters/unscented-kalman-filter/#julier-sigma-points","title":"Julier Sigma Points","text":""},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.sigma_points.JulierSigmaPoints","title":"<code>JulierSigmaPoints</code>","text":"<p>               Bases: <code>object</code></p> <p>Generates sigma points and weights according to Simon J. Julier and Jeffery K. Uhlmann's original paper[1]. It parametizes the sigma points using kappa.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Dimensionality of the state. 2n+1 weights will be generated.</p> required <code>sqrt_method</code> <code>function(ndarray)</code> <p>Defines how we compute the square root of a matrix, which has no unique answer. Cholesky is the default choice due to its speed. Typically your alternative choice will be scipy.linalg.sqrtm. Different choices affect how the sigma points are arranged relative to the eigenvectors of the covariance matrix. Usually this will not matter to you; if so the default cholesky() yields maximal performance. As of van der Merwe's dissertation of 2004 [6] this was not a well reseached area so I have no advice to give you.</p> <p>If your method returns a triangular matrix it must be upper triangular. Do not use numpy.linalg.cholesky - for historical reasons it returns a lower triangular matrix. The SciPy version does the right thing.</p> <code>scipy.linalg.cholesky</code> <code>subtract</code> <code>callable(x, y)</code> <p>Function that computes the difference between x and y. You will have to supply this if your state variable cannot support subtraction, such as angles (359-1 degreees is 2, not 358). x and y</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>Wm</code> <code>array</code> <p>weight for each sigma point for the mean</p> <code>Wc</code> <code>array</code> <p>weight for each sigma point for the covariance</p> References <p>.. [1] Julier, Simon J.; Uhlmann, Jeffrey \"A New Extension of the Kalman     Filter to Nonlinear Systems\". Proc. SPIE 3068, Signal Processing,     Sensor Fusion, and Target Recognition VI, 182 (July 28, 1997)</p> Source code in <code>bayesian_filters/kalman/sigma_points.py</code> <pre><code>class JulierSigmaPoints(object):\n    \"\"\"\n    Generates sigma points and weights according to Simon J. Julier\n    and Jeffery K. Uhlmann's original paper[1]. It parametizes the sigma\n    points using kappa.\n\n    Parameters\n    ----------\n\n    n : int\n        Dimensionality of the state. 2n+1 weights will be generated.\n\n\n    sqrt_method : function(ndarray), default=scipy.linalg.cholesky\n        Defines how we compute the square root of a matrix, which has\n        no unique answer. Cholesky is the default choice due to its\n        speed. Typically your alternative choice will be\n        scipy.linalg.sqrtm. Different choices affect how the sigma points\n        are arranged relative to the eigenvectors of the covariance matrix.\n        Usually this will not matter to you; if so the default cholesky()\n        yields maximal performance. As of van der Merwe's dissertation of\n        2004 [6] this was not a well reseached area so I have no advice\n        to give you.\n\n        If your method returns a triangular matrix it must be upper\n        triangular. Do not use numpy.linalg.cholesky - for historical\n        reasons it returns a lower triangular matrix. The SciPy version\n        does the right thing.\n\n    subtract : callable (x, y), optional\n        Function that computes the difference between x and y.\n        You will have to supply this if your state variable cannot support\n        subtraction, such as angles (359-1 degreees is 2, not 358). x and y\n\n    Attributes\n    ----------\n\n    Wm : np.array\n        weight for each sigma point for the mean\n\n    Wc : np.array\n        weight for each sigma point for the covariance\n\n    References\n    ----------\n\n    .. [1] Julier, Simon J.; Uhlmann, Jeffrey \"A New Extension of the Kalman\n        Filter to Nonlinear Systems\". Proc. SPIE 3068, Signal Processing,\n        Sensor Fusion, and Target Recognition VI, 182 (July 28, 1997)\n    \"\"\"\n\n    def __init__(self, n, kappa=0.0, sqrt_method=None, subtract=None):\n        self.n = n\n        self.kappa = kappa\n        if sqrt_method is None:\n            self.sqrt = cholesky\n        else:\n            self.sqrt = sqrt_method\n\n        if subtract is None:\n            self.subtract = np.subtract\n        else:\n            self.subtract = subtract\n\n        self._compute_weights()\n\n    def num_sigmas(self):\n        \"\"\"Number of sigma points for each variable in the state x\"\"\"\n        return 2 * self.n + 1\n\n    def sigma_points(self, x, P):\n        r\"\"\" Computes the sigma points for an unscented Kalman filter\n        given the mean (x) and covariance(P) of the filter.\n        kappa is an arbitrary constant. Returns sigma points.\n\n        Works with both scalar and array inputs:\n        sigma_points (5, 9, 2) # mean 5, covariance 9\n        sigma_points ([5, 2], 9*eye(2), 2) # means 5 and 2, covariance 9I\n\n        Parameters\n        ----------\n\n        x : array-like object of the means of length n\n            Can be a scalar if 1D.\n            examples: 1, [1,2], np.array([1,2])\n\n        P : scalar, or np.array\n           Covariance of the filter. If scalar, is treated as eye(n)*P.\n\n        kappa : float\n            Scaling factor.\n\n        Returns\n        -------\n\n        sigmas : np.array, of size (2n+1, n)\n            2D array of sigma points :math:`\\chi`. Each column contains all of\n            the sigmas for one dimension in the problem space. They\n            are ordered as:\n\n            .. math::\n                :nowrap:\n\n                \\begin{eqnarray}\n                  \\chi[0]    = &amp;x \\\\\n                  \\chi[1..n] = &amp;x + [\\sqrt{(n+\\kappa)P}]_k \\\\\n                  \\chi[n+1..2n] = &amp;x - [\\sqrt{(n+\\kappa)P}]_k\n                \\end{eqnarray}\n\n        \"\"\"\n\n        if self.n != np.size(x):\n            raise ValueError(\"expected size(x) {}, but size is {}\".format(self.n, np.size(x)))\n\n        n = self.n\n\n        if np.isscalar(x):\n            x = np.asarray([x])\n\n        n = np.size(x)  # dimension of problem\n\n        if np.isscalar(P):\n            P = np.eye(n) * P\n        else:\n            P = np.atleast_2d(P)\n\n        sigmas = np.zeros((2 * n + 1, n))\n\n        # implements U'*U = (n+kappa)*P. Returns lower triangular matrix.\n        # Take transpose so we can access with U[i]\n        U = self.sqrt((n + self.kappa) * P)\n\n        sigmas[0] = x\n        for k in range(n):\n            # pylint: disable=bad-whitespace\n            sigmas[k + 1] = self.subtract(x, -U[k])\n            sigmas[n + k + 1] = self.subtract(x, U[k])\n        return sigmas\n\n    def _compute_weights(self):\n        \"\"\"Computes the weights for the unscented Kalman filter. In this\n        formulation the weights for the mean and covariance are the same.\n        \"\"\"\n\n        n = self.n\n        k = self.kappa\n\n        self.Wm = np.full(2 * n + 1, 0.5 / (n + k))\n        self.Wm[0] = k / (n + k)\n        self.Wc = self.Wm\n\n    def __repr__(self):\n        return \"\\n\".join(\n            [\n                \"JulierSigmaPoints object\",\n                pretty_str(\"n\", self.n),\n                pretty_str(\"kappa\", self.kappa),\n                pretty_str(\"Wm\", self.Wm),\n                pretty_str(\"Wc\", self.Wc),\n                pretty_str(\"subtract\", self.subtract),\n                pretty_str(\"sqrt\", self.sqrt),\n            ]\n        )\n</code></pre>"},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.sigma_points.JulierSigmaPoints.num_sigmas","title":"<code>num_sigmas()</code>","text":"<p>Number of sigma points for each variable in the state x</p> Source code in <code>bayesian_filters/kalman/sigma_points.py</code> <pre><code>def num_sigmas(self):\n    \"\"\"Number of sigma points for each variable in the state x\"\"\"\n    return 2 * self.n + 1\n</code></pre>"},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.sigma_points.JulierSigmaPoints.sigma_points","title":"<code>sigma_points(x, P)</code>","text":"<p>Computes the sigma points for an unscented Kalman filter given the mean (x) and covariance(P) of the filter. kappa is an arbitrary constant. Returns sigma points.</p> <p>Works with both scalar and array inputs: sigma_points (5, 9, 2) # mean 5, covariance 9 sigma_points ([5, 2], 9*eye(2), 2) # means 5 and 2, covariance 9I</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array-like object of the means of length n</code> <p>Can be a scalar if 1D. examples: 1, [1,2], np.array([1,2])</p> required <code>P</code> <code>scalar, or np.array</code> <p>Covariance of the filter. If scalar, is treated as eye(n)*P.</p> required <code>kappa</code> <code>float</code> <p>Scaling factor.</p> required <p>Returns:</p> Name Type Description <code>sigmas</code> <code>np.array, of size (2n+1, n)</code> <p>2D array of sigma points :math:<code>\\chi</code>. Each column contains all of the sigmas for one dimension in the problem space. They are ordered as:</p> <p>.. math::     :nowrap:</p> <pre><code>\\begin{eqnarray}\n  \\chi[0]    = &amp;x \\\\\n  \\chi[1..n] = &amp;x + [\\sqrt{(n+\\kappa)P}]_k \\\\\n  \\chi[n+1..2n] = &amp;x - [\\sqrt{(n+\\kappa)P}]_k\n\\end{eqnarray}\n</code></pre> Source code in <code>bayesian_filters/kalman/sigma_points.py</code> <pre><code>def sigma_points(self, x, P):\n    r\"\"\" Computes the sigma points for an unscented Kalman filter\n    given the mean (x) and covariance(P) of the filter.\n    kappa is an arbitrary constant. Returns sigma points.\n\n    Works with both scalar and array inputs:\n    sigma_points (5, 9, 2) # mean 5, covariance 9\n    sigma_points ([5, 2], 9*eye(2), 2) # means 5 and 2, covariance 9I\n\n    Parameters\n    ----------\n\n    x : array-like object of the means of length n\n        Can be a scalar if 1D.\n        examples: 1, [1,2], np.array([1,2])\n\n    P : scalar, or np.array\n       Covariance of the filter. If scalar, is treated as eye(n)*P.\n\n    kappa : float\n        Scaling factor.\n\n    Returns\n    -------\n\n    sigmas : np.array, of size (2n+1, n)\n        2D array of sigma points :math:`\\chi`. Each column contains all of\n        the sigmas for one dimension in the problem space. They\n        are ordered as:\n\n        .. math::\n            :nowrap:\n\n            \\begin{eqnarray}\n              \\chi[0]    = &amp;x \\\\\n              \\chi[1..n] = &amp;x + [\\sqrt{(n+\\kappa)P}]_k \\\\\n              \\chi[n+1..2n] = &amp;x - [\\sqrt{(n+\\kappa)P}]_k\n            \\end{eqnarray}\n\n    \"\"\"\n\n    if self.n != np.size(x):\n        raise ValueError(\"expected size(x) {}, but size is {}\".format(self.n, np.size(x)))\n\n    n = self.n\n\n    if np.isscalar(x):\n        x = np.asarray([x])\n\n    n = np.size(x)  # dimension of problem\n\n    if np.isscalar(P):\n        P = np.eye(n) * P\n    else:\n        P = np.atleast_2d(P)\n\n    sigmas = np.zeros((2 * n + 1, n))\n\n    # implements U'*U = (n+kappa)*P. Returns lower triangular matrix.\n    # Take transpose so we can access with U[i]\n    U = self.sqrt((n + self.kappa) * P)\n\n    sigmas[0] = x\n    for k in range(n):\n        # pylint: disable=bad-whitespace\n        sigmas[k + 1] = self.subtract(x, -U[k])\n        sigmas[n + k + 1] = self.subtract(x, U[k])\n    return sigmas\n</code></pre>"},{"location":"filters/unscented-kalman-filter/#simplex-sigma-points","title":"Simplex Sigma Points","text":""},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.sigma_points.SimplexSigmaPoints","title":"<code>SimplexSigmaPoints</code>","text":"<p>               Bases: <code>object</code></p> <p>Generates sigma points and weights according to the simplex method presented in [1].</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Dimensionality of the state. n+1 weights will be generated.</p> required <code>sqrt_method</code> <code>function(ndarray)</code> <p>Defines how we compute the square root of a matrix, which has no unique answer. Cholesky is the default choice due to its speed. Typically your alternative choice will be scipy.linalg.sqrtm</p> <p>If your method returns a triangular matrix it must be upper triangular. Do not use numpy.linalg.cholesky - for historical reasons it returns a lower triangular matrix. The SciPy version does the right thing.</p> <code>scipy.linalg.cholesky</code> <code>subtract</code> <code>callable(x, y)</code> <p>Function that computes the difference between x and y. You will have to supply this if your state variable cannot support subtraction, such as angles (359-1 degreees is 2, not 358). x and y are state vectors, not scalars.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>Wm</code> <code>array</code> <p>weight for each sigma point for the mean</p> <code>Wc</code> <code>array</code> <p>weight for each sigma point for the covariance</p> References <p>.. [1] Phillippe Moireau and Dominique Chapelle \"Reduced-Order        Unscented Kalman Filtering with Application to Parameter        Identification in Large-Dimensional Systems\"        DOI: 10.1051/cocv/2010006</p> Source code in <code>bayesian_filters/kalman/sigma_points.py</code> <pre><code>class SimplexSigmaPoints(object):\n    \"\"\"\n    Generates sigma points and weights according to the simplex\n    method presented in [1].\n\n    Parameters\n    ----------\n\n    n : int\n        Dimensionality of the state. n+1 weights will be generated.\n\n    sqrt_method : function(ndarray), default=scipy.linalg.cholesky\n        Defines how we compute the square root of a matrix, which has\n        no unique answer. Cholesky is the default choice due to its\n        speed. Typically your alternative choice will be\n        scipy.linalg.sqrtm\n\n        If your method returns a triangular matrix it must be upper\n        triangular. Do not use numpy.linalg.cholesky - for historical\n        reasons it returns a lower triangular matrix. The SciPy version\n        does the right thing.\n\n    subtract : callable (x, y), optional\n        Function that computes the difference between x and y.\n        You will have to supply this if your state variable cannot support\n        subtraction, such as angles (359-1 degreees is 2, not 358). x and y\n        are state vectors, not scalars.\n\n    Attributes\n    ----------\n\n    Wm : np.array\n        weight for each sigma point for the mean\n\n    Wc : np.array\n        weight for each sigma point for the covariance\n\n    References\n    ----------\n\n    .. [1] Phillippe Moireau and Dominique Chapelle \"Reduced-Order\n           Unscented Kalman Filtering with Application to Parameter\n           Identification in Large-Dimensional Systems\"\n           DOI: 10.1051/cocv/2010006\n    \"\"\"\n\n    def __init__(self, n, alpha=1, sqrt_method=None, subtract=None):\n        self.n = n\n        self.alpha = alpha\n        if sqrt_method is None:\n            self.sqrt = cholesky\n        else:\n            self.sqrt = sqrt_method\n\n        if subtract is None:\n            self.subtract = np.subtract\n        else:\n            self.subtract = subtract\n\n        self._compute_weights()\n\n    def num_sigmas(self):\n        \"\"\"Number of sigma points for each variable in the state x\"\"\"\n        return self.n + 1\n\n    def sigma_points(self, x, P):\n        \"\"\"\n        Computes the implex sigma points for an unscented Kalman filter\n        given the mean (x) and covariance(P) of the filter.\n        Returns tuple of the sigma points and weights.\n\n        Works with both scalar and array inputs:\n        sigma_points (5, 9, 2) # mean 5, covariance 9\n        sigma_points ([5, 2], 9*eye(2), 2) # means 5 and 2, covariance 9I\n\n        Parameters\n        ----------\n\n        x : An array-like object of the means of length n\n            Can be a scalar if 1D.\n            examples: 1, [1,2], np.array([1,2])\n\n        P : scalar, or np.array\n            Covariance of the filter. If scalar, is treated as eye(n)*P.\n\n        Returns\n        -------\n\n        sigmas : np.array, of size (n+1, n)\n            Two dimensional array of sigma points. Each column contains all of\n            the sigmas for one dimension in the problem space.\n\n            Ordered by Xi_0, Xi_{1..n}\n        \"\"\"\n\n        if self.n != np.size(x):\n            raise ValueError(\"expected size(x) {}, but size is {}\".format(self.n, np.size(x)))\n\n        n = self.n\n\n        if np.isscalar(x):\n            x = np.asarray([x])\n        x = x.reshape(-1, 1)\n\n        if np.isscalar(P):\n            P = np.eye(n) * P\n        else:\n            P = np.atleast_2d(P)\n\n        U = self.sqrt(P)\n\n        lambda_ = n / (n + 1)\n        Istar = np.array([[-1 / np.sqrt(2 * lambda_), 1 / np.sqrt(2 * lambda_)]])\n\n        for d in range(2, n + 1):\n            row = np.ones((1, Istar.shape[1] + 1)) * 1.0 / np.sqrt(lambda_ * d * (d + 1))  # pylint: disable=unsubscriptable-object\n            row[0, -1] = -d / np.sqrt(lambda_ * d * (d + 1))\n            Istar = np.r_[np.c_[Istar, np.zeros((Istar.shape[0]))], row]  # pylint: disable=unsubscriptable-object\n\n        I = np.sqrt(n) * Istar\n        scaled_unitary = (U.T).dot(I)\n\n        sigmas = self.subtract(x, -scaled_unitary)\n        return sigmas.T\n\n    def _compute_weights(self):\n        \"\"\"Computes the weights for the scaled unscented Kalman filter.\"\"\"\n\n        n = self.n\n        c = 1.0 / (n + 1)\n        self.Wm = np.full(n + 1, c)\n        self.Wc = self.Wm\n\n    def __repr__(self):\n        return \"\\n\".join(\n            [\n                \"SimplexSigmaPoints object\",\n                pretty_str(\"n\", self.n),\n                pretty_str(\"alpha\", self.alpha),\n                pretty_str(\"Wm\", self.Wm),\n                pretty_str(\"Wc\", self.Wc),\n                pretty_str(\"subtract\", self.subtract),\n                pretty_str(\"sqrt\", self.sqrt),\n            ]\n        )\n</code></pre>"},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.sigma_points.SimplexSigmaPoints.num_sigmas","title":"<code>num_sigmas()</code>","text":"<p>Number of sigma points for each variable in the state x</p> Source code in <code>bayesian_filters/kalman/sigma_points.py</code> <pre><code>def num_sigmas(self):\n    \"\"\"Number of sigma points for each variable in the state x\"\"\"\n    return self.n + 1\n</code></pre>"},{"location":"filters/unscented-kalman-filter/#bayesian_filters.kalman.sigma_points.SimplexSigmaPoints.sigma_points","title":"<code>sigma_points(x, P)</code>","text":"<p>Computes the implex sigma points for an unscented Kalman filter given the mean (x) and covariance(P) of the filter. Returns tuple of the sigma points and weights.</p> <p>Works with both scalar and array inputs: sigma_points (5, 9, 2) # mean 5, covariance 9 sigma_points ([5, 2], 9*eye(2), 2) # means 5 and 2, covariance 9I</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>An array-like object of the means of length n</code> <p>Can be a scalar if 1D. examples: 1, [1,2], np.array([1,2])</p> required <code>P</code> <code>scalar, or np.array</code> <p>Covariance of the filter. If scalar, is treated as eye(n)*P.</p> required <p>Returns:</p> Name Type Description <code>sigmas</code> <code>np.array, of size (n+1, n)</code> <p>Two dimensional array of sigma points. Each column contains all of the sigmas for one dimension in the problem space.</p> <p>Ordered by Xi_0, Xi_{1..n}</p> Source code in <code>bayesian_filters/kalman/sigma_points.py</code> <pre><code>def sigma_points(self, x, P):\n    \"\"\"\n    Computes the implex sigma points for an unscented Kalman filter\n    given the mean (x) and covariance(P) of the filter.\n    Returns tuple of the sigma points and weights.\n\n    Works with both scalar and array inputs:\n    sigma_points (5, 9, 2) # mean 5, covariance 9\n    sigma_points ([5, 2], 9*eye(2), 2) # means 5 and 2, covariance 9I\n\n    Parameters\n    ----------\n\n    x : An array-like object of the means of length n\n        Can be a scalar if 1D.\n        examples: 1, [1,2], np.array([1,2])\n\n    P : scalar, or np.array\n        Covariance of the filter. If scalar, is treated as eye(n)*P.\n\n    Returns\n    -------\n\n    sigmas : np.array, of size (n+1, n)\n        Two dimensional array of sigma points. Each column contains all of\n        the sigmas for one dimension in the problem space.\n\n        Ordered by Xi_0, Xi_{1..n}\n    \"\"\"\n\n    if self.n != np.size(x):\n        raise ValueError(\"expected size(x) {}, but size is {}\".format(self.n, np.size(x)))\n\n    n = self.n\n\n    if np.isscalar(x):\n        x = np.asarray([x])\n    x = x.reshape(-1, 1)\n\n    if np.isscalar(P):\n        P = np.eye(n) * P\n    else:\n        P = np.atleast_2d(P)\n\n    U = self.sqrt(P)\n\n    lambda_ = n / (n + 1)\n    Istar = np.array([[-1 / np.sqrt(2 * lambda_), 1 / np.sqrt(2 * lambda_)]])\n\n    for d in range(2, n + 1):\n        row = np.ones((1, Istar.shape[1] + 1)) * 1.0 / np.sqrt(lambda_ * d * (d + 1))  # pylint: disable=unsubscriptable-object\n        row[0, -1] = -d / np.sqrt(lambda_ * d * (d + 1))\n        Istar = np.r_[np.c_[Istar, np.zeros((Istar.shape[0]))], row]  # pylint: disable=unsubscriptable-object\n\n    I = np.sqrt(n) * Istar\n    scaled_unitary = (U.T).dot(I)\n\n    sigmas = self.subtract(x, -scaled_unitary)\n    return sigmas.T\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#using-pip-recommended","title":"Using pip (Recommended)","text":"<p>The easiest way to install Bayesian Filters is using pip:</p> <pre><code>pip install bayesian-filters\n</code></pre>"},{"location":"getting-started/installation/#using-uv","title":"Using uv","text":"<p>If you use uv for Python package management:</p> <pre><code>uv pip install bayesian-filters\n</code></pre>"},{"location":"getting-started/installation/#using-anaconda","title":"Using Anaconda","text":"<p>If you use Anaconda, you can install from the conda-forge channel:</p> <pre><code># Add conda-forge channel if you haven't already\nconda config --add channels conda-forge\n\n# Install the package\nconda install filterpy\n</code></pre> <p>Note: The conda package is still named <code>filterpy</code> for backward compatibility.</p>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>To install the latest development version from GitHub:</p> <pre><code>pip install git+https://github.com/GeorgePearse/filterpy.git\n</code></pre> <p>Or clone and install locally:</p> <pre><code>git clone https://github.com/GeorgePearse/filterpy.git\ncd filterpy\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.6</li> <li>NumPy</li> <li>SciPy</li> <li>Matplotlib</li> </ul> <p>These dependencies will be installed automatically when you install the package.</p>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>To verify the installation, try importing the library:</p> <pre><code>import bayesian_filters\nfrom bayesian_filters.kalman import KalmanFilter\n\nprint(f\"Bayesian Filters version: {bayesian_filters.__version__}\")\n</code></pre> <p>If this runs without errors, you're ready to go!</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Continue to the Quick Start guide to learn how to use the library.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start Guide","text":"<p>This guide will walk you through creating your first Kalman filter using Bayesian Filters.</p>"},{"location":"getting-started/quick-start/#basic-kalman-filter-example","title":"Basic Kalman Filter Example","text":"<p>Let's create a simple Kalman filter to track a 1D position with constant velocity.</p>"},{"location":"getting-started/quick-start/#import-required-modules","title":"Import Required Modules","text":"<pre><code>import numpy as np\nfrom bayesian_filters.kalman import KalmanFilter\nfrom bayesian_filters.common import Q_discrete_white_noise\n</code></pre>"},{"location":"getting-started/quick-start/#create-the-filter","title":"Create the Filter","text":"<pre><code># Create a Kalman filter with 2 state variables (position, velocity)\n# and 1 measurement variable (position)\nkf = KalmanFilter(dim_x=2, dim_z=1)\n</code></pre>"},{"location":"getting-started/quick-start/#initialize-state-vector","title":"Initialize State Vector","text":"<pre><code># Initial state: [position, velocity]\nkf.x = np.array([[2.],   # initial position\n                 [0.]])  # initial velocity\n</code></pre>"},{"location":"getting-started/quick-start/#define-state-transition-matrix","title":"Define State Transition Matrix","text":"<pre><code># State transition matrix (constant velocity model)\ndt = 0.1  # time step\nkf.F = np.array([[1., dt],\n                 [0., 1.]])\n</code></pre>"},{"location":"getting-started/quick-start/#define-measurement-function","title":"Define Measurement Function","text":"<pre><code># Measurement function (we only measure position)\nkf.H = np.array([[1., 0.]])\n</code></pre>"},{"location":"getting-started/quick-start/#set-covariance-matrices","title":"Set Covariance Matrices","text":"<pre><code># Initial covariance matrix (uncertainty in initial state)\nkf.P *= 1000.\n\n# Measurement uncertainty\nkf.R = 5.\n\n# Process noise\nkf.Q = Q_discrete_white_noise(dim=2, dt=dt, var=0.1)\n</code></pre>"},{"location":"getting-started/quick-start/#run-the-filter","title":"Run the Filter","text":"<pre><code># Simulate measurements (in practice, these come from sensors)\nmeasurements = [2.1, 2.3, 2.5, 2.7, 2.9, 3.1, 3.3]\n\nfor z in measurements:\n    # Predict step\n    kf.predict()\n\n    # Update step\n    kf.update(z)\n\n    # Get the current state estimate\n    print(f\"Position: {kf.x[0, 0]:.2f}, Velocity: {kf.x[1, 0]:.2f}\")\n</code></pre>"},{"location":"getting-started/quick-start/#understanding-the-filter-cycle","title":"Understanding the Filter Cycle","text":"<p>The Kalman filter operates in a two-step cycle:</p>"},{"location":"getting-started/quick-start/#1-predict-step","title":"1. Predict Step","text":"<pre><code>kf.predict()\n</code></pre> <p>This step uses the state transition model to predict the next state:</p> <ul> <li>Projects the state forward: <code>x = F @ x</code></li> <li>Projects the covariance forward: <code>P = F @ P @ F.T + Q</code></li> </ul>"},{"location":"getting-started/quick-start/#2-update-step","title":"2. Update Step","text":"<pre><code>kf.update(measurement)\n</code></pre> <p>This step incorporates a new measurement:</p> <ul> <li>Computes the Kalman gain</li> <li>Updates the state estimate</li> <li>Updates the covariance estimate</li> </ul>"},{"location":"getting-started/quick-start/#complete-working-example","title":"Complete Working Example","text":"<p>Here's a complete example with plotting:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom bayesian_filters.kalman import KalmanFilter\nfrom bayesian_filters.common import Q_discrete_white_noise\n\n# Create filter\nkf = KalmanFilter(dim_x=2, dim_z=1)\n\n# Initialize\nkf.x = np.array([[0.], [1.]])  # start at position 0, velocity 1\nkf.F = np.array([[1., 1.], [0., 1.]])  # dt = 1\nkf.H = np.array([[1., 0.]])\nkf.P *= 1000.\nkf.R = 5.\nkf.Q = Q_discrete_white_noise(dim=2, dt=1., var=0.1)\n\n# Generate noisy measurements\ntrue_positions = np.arange(0, 50, 1)\nmeasurements = true_positions + np.random.normal(0, 2, len(true_positions))\n\n# Run filter\nestimates = []\nfor z in measurements:\n    kf.predict()\n    kf.update(z)\n    estimates.append(kf.x[0, 0])\n\n# Plot results\nplt.figure(figsize=(10, 6))\nplt.plot(true_positions, label='True Position', linewidth=2)\nplt.scatter(range(len(measurements)), measurements,\n            label='Measurements', alpha=0.5, s=30)\nplt.plot(estimates, label='Kalman Filter Estimate', linewidth=2)\nplt.legend()\nplt.xlabel('Time Step')\nplt.ylabel('Position')\nplt.title('Kalman Filter Example')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you understand the basics, explore:</p> <ul> <li>Kalman Filter: Detailed documentation on the standard Kalman filter</li> <li>Extended Kalman Filter: For nonlinear systems</li> <li>Unscented Kalman Filter: Alternative approach for nonlinear systems</li> <li>Examples: More complex examples and use cases</li> </ul>"},{"location":"getting-started/quick-start/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quick-start/#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple measurements at once\nfor z in measurements:\n    kf.predict()\n    kf.update(z)\n</code></pre>"},{"location":"getting-started/quick-start/#accessing-filter-state","title":"Accessing Filter State","text":"<pre><code># Current state estimate\nposition = kf.x[0, 0]\nvelocity = kf.x[1, 0]\n\n# Current covariance (uncertainty)\nuncertainty = kf.P\n\n# Innovation (measurement residual)\nresidual = kf.y\n</code></pre>"},{"location":"getting-started/quick-start/#saving-filter-history","title":"Saving Filter History","text":"<pre><code>from bayesian_filters.kalman import Saver\n\n# Create a saver to log filter history\nsaver = Saver(kf)\n\nfor z in measurements:\n    kf.predict()\n    kf.update(z)\n    saver.save()\n\n# Access saved data\nsaver.x  # All state estimates\nsaver.P  # All covariances\n</code></pre>"}]}